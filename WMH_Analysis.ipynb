{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fbc67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flowchart generated. Files saved in Flow_Chart folder:\n",
      " - Flow_Chart\\flowchart_neurology.png\n",
      " - Flow_Chart\\flowchart_neurology.pdf\n",
      " - Flow_Chart\\flowchart_neurology.svg\n",
      " - Flow_Chart\\flowchart_neurology.tiff\n",
      " - Flow_Chart\\flowchart_neurology.dot\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 13.1.2 (20250808.2320)\n",
       " -->\n",
       "<!-- Title: Study_Flowchart Pages: 1 -->\n",
       "<svg width=\"3285pt\" height=\"4267pt\"\n",
       " viewBox=\"0.00 0.00 3285.00 4267.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(8.33333 8.33333) rotate(0) translate(4 508)\">\n",
       "<title>Study_Flowchart</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-508 390.25,-508 390.25,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.12,-504C259.12,-504 161.38,-504 161.38,-504 155.38,-504 149.38,-498 149.38,-492 149.38,-492 149.38,-480 149.38,-480 149.38,-474 155.38,-468 161.38,-468 161.38,-468 259.12,-468 259.12,-468 265.12,-468 271.12,-474 271.12,-480 271.12,-480 271.12,-492 271.12,-492 271.12,-498 265.12,-504 259.12,-504\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"210.25\" y=\"-488.5\" font-family=\"Arial\" font-size=\"10.00\">UK Biobank participants</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"210.25\" y=\"-476.5\" font-family=\"Arial\" font-size=\"10.00\"> ≈ 502,000</text>\n",
       "</g>\n",
       "<!-- BC -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>BC</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.25,-432C269.25,-432 151.25,-432 151.25,-432 145.25,-432 139.25,-426 139.25,-420 139.25,-420 139.25,-408 139.25,-408 139.25,-402 145.25,-396 151.25,-396 151.25,-396 269.25,-396 269.25,-396 275.25,-396 281.25,-402 281.25,-408 281.25,-408 281.25,-420 281.25,-420 281.25,-426 275.25,-432 269.25,-432\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"210.25\" y=\"-416.5\" font-family=\"Arial\" font-size=\"10.00\">Brain MRI available ≈ 46,000</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"210.25\" y=\"-404.5\" font-family=\"Arial\" font-size=\"10.00\">SA identified ≈ 13,000</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;BC -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;BC</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.25,-467.7C210.25,-460.41 210.25,-451.73 210.25,-443.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213.75,-443.62 210.25,-433.62 206.75,-443.62 213.75,-443.62\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.88,-360C277.88,-360 142.62,-360 142.62,-360 136.62,-360 130.62,-354 130.62,-348 130.62,-348 130.62,-336 130.62,-336 130.62,-330 136.62,-324 142.62,-324 142.62,-324 277.88,-324 277.88,-324 283.88,-324 289.88,-330 289.88,-336 289.88,-336 289.88,-348 289.88,-348 289.88,-354 283.88,-360 277.88,-360\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"210.25\" y=\"-344.5\" font-family=\"Arial\" font-size=\"10.00\">SA diagnosed before the V1 MRI</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"210.25\" y=\"-332.5\" font-family=\"Arial\" font-size=\"10.00\">N = 598</text>\n",
       "</g>\n",
       "<!-- BC&#45;&gt;D -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>BC&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.25,-395.7C210.25,-388.41 210.25,-379.73 210.25,-371.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213.75,-371.62 210.25,-361.62 206.75,-371.62 213.75,-371.62\"/>\n",
       "</g>\n",
       "<!-- E1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>E1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M218.5,-288C218.5,-288 12,-288 12,-288 6,-288 0,-282 0,-276 0,-276 0,-264 0,-264 0,-258 6,-252 12,-252 12,-252 218.5,-252 218.5,-252 224.5,-252 230.5,-258 230.5,-264 230.5,-264 230.5,-276 230.5,-276 230.5,-282 224.5,-288 218.5,-288\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-272.5\" font-family=\"Arial\" font-size=\"10.00\">After excluding SA cases with pre&#45;index neuro dx</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-260.5\" font-family=\"Arial\" font-size=\"10.00\">N = 578</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>D&#45;&gt;E1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.77,-323.7C175.01,-315.03 160.58,-304.4 147.77,-294.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.14,-292.36 140.01,-289.24 145.99,-297.99 150.14,-292.36\"/>\n",
       "</g>\n",
       "<!-- E2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M352.12,-288C352.12,-288 260.38,-288 260.38,-288 254.38,-288 248.38,-282 248.38,-276 248.38,-276 248.38,-264 248.38,-264 248.38,-258 254.38,-252 260.38,-252 260.38,-252 352.12,-252 352.12,-252 358.12,-252 364.12,-258 364.12,-264 364.12,-264 364.12,-276 364.12,-276 364.12,-282 358.12,-288 352.12,-288\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"306.25\" y=\"-266.5\" font-family=\"Arial\" font-size=\"10.00\">No pre&#45;index exclusion</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>D&#45;&gt;E2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.98,-323.7C245.87,-315.03 260.44,-304.4 273.39,-294.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"275.22,-297.96 281.24,-289.24 271.09,-292.3 275.22,-297.96\"/>\n",
       "</g>\n",
       "<!-- F1 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>F1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.12,-216C191.12,-216 39.38,-216 39.38,-216 33.38,-216 27.38,-210 27.38,-204 27.38,-204 27.38,-184 27.38,-184 27.38,-178 33.38,-172 39.38,-172 39.38,-172 191.12,-172 191.12,-172 197.12,-172 203.12,-178 203.12,-184 203.12,-184 203.12,-204 203.12,-204 203.12,-210 197.12,-216 191.12,-216\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-202.5\" font-family=\"Arial\" font-size=\"10.00\">Propensity score matching (1:10)</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-190.5\" font-family=\"Arial\" font-size=\"10.00\">Control index assigned</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-178.5\" font-family=\"Arial\" font-size=\"10.00\">Pre&#45;index neuro dx exclusion applied</text>\n",
       "</g>\n",
       "<!-- E1&#45;&gt;F1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>E1&#45;&gt;F1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.25,-251.84C115.25,-244.64 115.25,-236.02 115.25,-227.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"118.75,-227.82 115.25,-217.82 111.75,-227.82 118.75,-227.82\"/>\n",
       "</g>\n",
       "<!-- F2 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>F2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M374.25,-212C374.25,-212 238.25,-212 238.25,-212 232.25,-212 226.25,-206 226.25,-200 226.25,-200 226.25,-188 226.25,-188 226.25,-182 232.25,-176 238.25,-176 238.25,-176 374.25,-176 374.25,-176 380.25,-176 386.25,-182 386.25,-188 386.25,-188 386.25,-200 386.25,-200 386.25,-206 380.25,-212 374.25,-212\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"306.25\" y=\"-190.5\" font-family=\"Arial\" font-size=\"10.00\">Propensity score matching (1:10)</text>\n",
       "</g>\n",
       "<!-- E2&#45;&gt;F2 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>E2&#45;&gt;F2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M306.25,-251.84C306.25,-243.49 306.25,-233.23 306.25,-223.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.75,-223.84 306.25,-213.84 302.75,-223.84 309.75,-223.84\"/>\n",
       "</g>\n",
       "<!-- G1 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>G1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.38,-136C154.38,-136 76.12,-136 76.12,-136 70.12,-136 64.12,-130 64.12,-124 64.12,-124 64.12,-104 64.12,-104 64.12,-98 70.12,-92 76.12,-92 76.12,-92 154.38,-92 154.38,-92 160.38,-92 166.38,-98 166.38,-104 166.38,-104 166.38,-124 166.38,-124 166.38,-130 160.38,-136 154.38,-136\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-122.5\" font-family=\"Arial\" font-size=\"10.00\">Primary cohort</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-110.5\" font-family=\"Arial\" font-size=\"10.00\">SA: N = 578</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-98.5\" font-family=\"Arial\" font-size=\"10.00\">Controls: N = 5,672</text>\n",
       "</g>\n",
       "<!-- F1&#45;&gt;G1 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>F1&#45;&gt;G1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.25,-171.69C115.25,-164.29 115.25,-155.82 115.25,-147.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"118.75,-147.8 115.25,-137.8 111.75,-147.8 118.75,-147.8\"/>\n",
       "</g>\n",
       "<!-- G2 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>G2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.38,-136C351.38,-136 261.12,-136 261.12,-136 255.12,-136 249.12,-130 249.12,-124 249.12,-124 249.12,-104 249.12,-104 249.12,-98 255.12,-92 261.12,-92 261.12,-92 351.38,-92 351.38,-92 357.38,-92 363.38,-98 363.38,-104 363.38,-104 363.38,-124 363.38,-124 363.38,-130 357.38,-136 351.38,-136\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"306.25\" y=\"-122.5\" font-family=\"Arial\" font-size=\"10.00\">Main sensitivity cohort</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"306.25\" y=\"-110.5\" font-family=\"Arial\" font-size=\"10.00\">SA: N = 598</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"306.25\" y=\"-98.5\" font-family=\"Arial\" font-size=\"10.00\">Controls: N = 5,980</text>\n",
       "</g>\n",
       "<!-- F2&#45;&gt;G2 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>F2&#45;&gt;G2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M306.25,-175.69C306.25,-167.46 306.25,-157.33 306.25,-147.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.75,-147.95 306.25,-137.95 302.75,-147.95 309.75,-147.95\"/>\n",
       "</g>\n",
       "<!-- H1 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>H1</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M206.12,-56C206.12,-56 24.38,-56 24.38,-56 18.38,-56 12.38,-50 12.38,-44 12.38,-44 12.38,-12 12.38,-12 12.38,-6 18.38,0 24.38,0 24.38,0 206.12,0 206.12,0 212.12,0 218.12,-6 218.12,-12 218.12,-12 218.12,-44 218.12,-44 218.12,-50 212.12,-56 206.12,-56\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-42.5\" font-family=\"Arial\" font-size=\"10.00\">Exploratory longitudinal subcohort (V1→V2)</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-30.5\" font-family=\"Arial\" font-size=\"10.00\">From Primary cohort</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-18.5\" font-family=\"Arial\" font-size=\"10.00\">SA: N = 47</text>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"115.25\" y=\"-6.5\" font-family=\"Arial\" font-size=\"10.00\">Controls: N = 528</text>\n",
       "</g>\n",
       "<!-- G1&#45;&gt;H1 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>G1&#45;&gt;H1</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M115.25,-91.8C115.25,-84.47 115.25,-76 115.25,-67.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"118.75,-67.75 115.25,-57.75 111.75,-67.75 118.75,-67.75\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x19ca84516a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graphviz Flowchart for submission (Option 1: inline display + file export)\n",
    "# If not installed, run:  !pip install graphviz\n",
    "# (Optional for LZW compression of TIFF:  !pip install pillow)\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Optional: Pillow for LZW-compressed TIFF post-processing\n",
    "try:\n",
    "    from PIL import Image\n",
    "except Exception:\n",
    "    Image = None  # If Pillow is not available, TIFF is still exported (without LZW)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Output directory\n",
    "# ------------------------------------------------------\n",
    "outdir = \"Flow_Chart\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Auto-generate filenames with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "basename = \"flowchart_neurology\"\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Initialize graph\n",
    "# ------------------------------------------------------\n",
    "# Note: 'dpi' controls raster resolution for PNG/TIFF outputs.\n",
    "dot = Digraph(\"Study_Flowchart\", format=\"png\")\n",
    "dot.attr(rankdir=\"TB\", size=\"8,10\", dpi=\"600\")   # TB = top-to-bottom layout; 600 dpi for high-res raster\n",
    "\n",
    "# Global font settings (sans-serif fonts, e.g., Arial)\n",
    "dot.attr(\"graph\", fontname=\"Arial\")\n",
    "dot.attr(\"node\", shape=\"rectangle\", style=\"rounded\",\n",
    "         fontsize=\"10\", fontname=\"Arial\", color=\"black\", fillcolor=\"white\")\n",
    "dot.attr(\"edge\", fontname=\"Arial\", fontsize=\"9\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Define nodes\n",
    "# ------------------------------------------------------\n",
    "dot.node(\"A\", \"UK Biobank participants\\n ≈ 502,000\")\n",
    "# dot.node(\"B\", \"I2 brain MRI (WMH IDPs) available\\n≈ 61,000\")\n",
    "# dot.node(\"C\", \"SA identified (Self-report / Hospital)\\n≈ 13,000\")\n",
    "\n",
    "dot.node(\"BC\", \"Brain MRI available ≈ 46,000\\nSA identified ≈ 13,000\")\n",
    "\n",
    "dot.node(\"D\", \"SA diagnosed before the V1 MRI\\nN = 598\")\n",
    "\n",
    "# Branches\n",
    "dot.node(\"E1\", \"After excluding SA cases with pre-index neuro dx\\nN = 578\")\n",
    "dot.node(\"E2\", \"No pre-index exclusion\")\n",
    "\n",
    "# Matching steps\n",
    "dot.node(\"F1\", \"Propensity score matching (1:10)\\nControl index assigned\\nPre-index neuro dx exclusion applied\")\n",
    "dot.node(\"F2\", \"Propensity score matching (1:10)\")\n",
    "\n",
    "# Final cohorts\n",
    "dot.node(\"G1\", \"Primary cohort\\nSA: N = 578\\nControls: N = 5,672\")\n",
    "dot.node(\"G2\", \"Main sensitivity cohort\\nSA: N = 598\\nControls: N = 5,980\")\n",
    "\n",
    "# Align F1 and F2 horizontally\n",
    "with dot.subgraph() as s:\n",
    "    s.attr(rank='same')\n",
    "    s.node(\"F1\")\n",
    "    s.node(\"F2\")\n",
    "\n",
    "# Align G1 and G2 horizontally\n",
    "with dot.subgraph() as s:\n",
    "    s.attr(rank='same')\n",
    "    s.node(\"G1\")\n",
    "    s.node(\"G2\")\n",
    "\n",
    "# Exploratory longitudinal subcohort (derived from Primary cohort only)\n",
    "dot.node(\"H1\", \"Exploratory longitudinal subcohort (V1→V2)\\nFrom Primary cohort\\nSA: N = 47\\nControls: N = 528\",\n",
    "         style=\"rounded,dashed\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Define edges\n",
    "# ------------------------------------------------------\n",
    "# Branches from A\n",
    "dot.edge(\"A\", \"BC\")\n",
    "# dot.edge(\"A\", \"C\")\n",
    "\n",
    "# Intersection: D = B ∩ C\n",
    "dot.edge(\"BC\", \"D\")\n",
    "# dot.edge(\"C\", \"D\")\n",
    "\n",
    "# Continue paths\n",
    "# dot.edge(\"D\", \"E1\", xlabel =\"Primary path\", labeldistance = \"2.5\")\n",
    "# dot.edge(\"D\", \"E2\", label =\"Main sensitivity path\", labeldistance=\"2.5\")\n",
    "dot.edge(\"D\", \"E1\")\n",
    "dot.edge(\"D\", \"E2\")\n",
    "\n",
    "dot.edge(\"E1\", \"F1\")\n",
    "dot.edge(\"F1\", \"G1\")\n",
    "\n",
    "dot.edge(\"E2\", \"F2\")\n",
    "dot.edge(\"F2\", \"G2\")\n",
    "\n",
    "# Dashed edge to show derivation\n",
    "dot.edge(\"G1\", \"H1\", style=\"dashed\")\n",
    "\n",
    "\n",
    "# #Invisible node for alignment (optional)\n",
    "# dot.node(\"AlignLeft\", \"\", style=\"invis\", width=\"0\", height=\"0\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Save outputs (PNG, PDF, SVG, DOT, and high-res TIFF)\n",
    "# ------------------------------------------------------\n",
    "# Base paths\n",
    "base_path = os.path.join(outdir, basename)\n",
    "png_path = base_path + \".png\"\n",
    "pdf_path = base_path + \".pdf\"\n",
    "svg_path = base_path + \".svg\"\n",
    "tif_path = base_path + \".tiff\"\n",
    "dot_path = base_path + \".dot\"\n",
    "\n",
    "# Render outputs\n",
    "dot.render(filename=base_path, format=\"png\", cleanup=True)\n",
    "dot.render(filename=base_path, format=\"pdf\", cleanup=True)\n",
    "dot.render(filename=base_path, format=\"svg\", cleanup=True)\n",
    "dot.render(filename=base_path, format=\"tiff\", cleanup=True)\n",
    "\n",
    "# Save DOT source once\n",
    "with open(dot_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dot.source)\n",
    "\n",
    "# Optional TIFF LZW compression\n",
    "if Image is not None and os.path.exists(tif_path):\n",
    "    try:\n",
    "        im = Image.open(tif_path)\n",
    "        if im.mode in (\"P\", \"L\"):\n",
    "            im = im.convert(\"RGB\")\n",
    "        im.save(tif_path, format=\"TIFF\", compression=\"tiff_lzw\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Pillow LZW compression failed: {e}\")\n",
    "\n",
    "print(\"✅ Flowchart generated. Files saved in Flow_Chart folder:\")\n",
    "print(\" -\", png_path)\n",
    "print(\" -\", pdf_path)\n",
    "print(\" -\", svg_path)\n",
    "print(\" -\", tif_path)\n",
    "print(\" -\", dot_path)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6. Display inline in Jupyter\n",
    "# ------------------------------------------------------\n",
    "dot  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab4c518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_11008\\3628346309.py:112: DtypeWarning: Columns (34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_IN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded withdrawal list: 590 unique IDs.\n",
      "\n",
      "=== Pre-removal sample size by treatment_var ===\n",
      "Total sample size: 46173\n",
      "  treatment_var = 0: n = 45559\n",
      "  treatment_var = 1: n = 614\n",
      "\n",
      "=== Post-removal sample size by treatment_var ===\n",
      "Total sample size: 46173\n",
      "  treatment_var = 0: n = 45559\n",
      "  treatment_var = 1: n = 614\n",
      "\n",
      "--- Publication-Grade Summary ---\n",
      "Participants removed due to withdrawn consent: n = 0 (from N = 46173 to N = 46173).\n",
      "Post-removal counts are reported above by 'treatment_var'.\n",
      "\n",
      "✅ Cleaned dataset saved to: data.csv\n"
     ]
    }
   ],
   "source": [
    "# Removal of Participants Who Withdrew Consent\n",
    "\"\"\"\n",
    "Publication-Ready Description\n",
    "-----------------------------\n",
    "Objective\n",
    "    Remove UK Biobank participants who have withdrawn consent from a single\n",
    "    analysis dataset and report post-removal sample sizes by treatment arm.\n",
    "\n",
    "Inputs\n",
    "    1) data20250415.csv\n",
    "       - Contains at minimum:\n",
    "           • A participant identifier column (auto-detected; see below)\n",
    "           • A treatment assignment column named exactly: treatment_var\n",
    "             (e.g., 'Study' vs 'Control' or 1 vs 0)\n",
    "    2) w48286_20250818.csv\n",
    "       - Headerless CSV; the FIRST column lists participant IDs who withdrew consent.\n",
    "\n",
    "Outputs\n",
    "    • data.csv                             (cleaned analysis dataset)\n",
    "    • Console report (publication-grade English):\n",
    "        - N removed in total\n",
    "        - Post-removal sample sizes by treatment_var\n",
    "\n",
    "ID Handling\n",
    "    • The participant ID column is detected heuristically from common names:\n",
    "      ['Participant_ID', 'participant_id', 'eid', 'EID', 'ID', 'id'].\n",
    "      If none are present, the first column is used.\n",
    "    • All IDs are compared as strings to avoid leading-zero mismatches.\n",
    "\n",
    "Notes\n",
    "    • This script performs a *strict* row-wise removal: any row whose ID appears\n",
    "      in the withdrawal list is dropped. No cluster or matched-set logic is applied.\n",
    "    • The script prints both pre- and post-removal counts by treatment_var.\n",
    "\n",
    "Usage\n",
    "    python remove_withdrawals_simple.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Set\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "DATA_IN = \"data20250415.csv\"\n",
    "WITHDRAWN_FN = \"w48286_20250818.csv\"   # headerless; first column are IDs\n",
    "DATA_OUT = \"data.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def detect_id_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Detect the participant ID column from common names.\n",
    "    Fallback: use the first column if none of the common names are present.\n",
    "    \"\"\"\n",
    "    candidates = [\"Participant_ID\", \"participant_id\", \"eid\", \"EID\", \"ID\", \"id\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return df.columns[0]\n",
    "\n",
    "def load_withdrawn_ids(path: str | Path) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load withdrawn IDs from a headerless CSV where the FIRST column contains IDs.\n",
    "    Return as a set of strings (NaN and empty values discarded).\n",
    "    \"\"\"\n",
    "    wd = pd.read_csv(path, header=None)\n",
    "    if wd.empty:\n",
    "        return set()\n",
    "    col0 = wd.columns[0]\n",
    "    return set(\n",
    "        wd[col0]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace({\"nan\": \"\"})\n",
    "        .dropna()\n",
    "        .loc[lambda s: s.ne(\"\")]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "def summarize_by_treatment(df: pd.DataFrame, title: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Print and return counts by treatment_var.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    if \"treatment_var\" not in df.columns:\n",
    "        raise KeyError(\"Required column 'treatment_var' not found in the dataset.\")\n",
    "    counts = df[\"treatment_var\"].value_counts(dropna=False).to_dict()\n",
    "    total = len(df)\n",
    "    # Pretty print\n",
    "    print(f\"Total sample size: {total}\")\n",
    "    for k, v in counts.items():\n",
    "        print(f\"  treatment_var = {k!r}: n = {v}\")\n",
    "    # Return a flattened dict for optional downstream use\n",
    "    out = {\"total\": total}\n",
    "    out.update({f\"treatment_{k}\": int(v) for k, v in counts.items()})\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Existence checks\n",
    "    for p in [DATA_IN, WITHDRAWN_FN]:\n",
    "        if not Path(p).exists():\n",
    "            raise FileNotFoundError(f\"Required input not found: {p}\")\n",
    "\n",
    "    # Load main dataset\n",
    "    df = pd.read_csv(DATA_IN)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Input dataset is empty: data20250415.csv\")\n",
    "\n",
    "    # Detect ID column and normalize to string for safe comparison\n",
    "    id_col = detect_id_col(df)\n",
    "    df[id_col] = df[id_col].astype(str).str.strip()\n",
    "\n",
    "    # Sanity: ensure treatment_var is present\n",
    "    if \"treatment_var\" not in df.columns:\n",
    "        raise KeyError(\n",
    "            \"The dataset must contain a 'treatment_var' column \"\n",
    "            \"(e.g., 'Study' vs 'Control' or 1 vs 0).\"\n",
    "        )\n",
    "\n",
    "    # Load withdrawal list\n",
    "    withdrawn_ids = load_withdrawn_ids(WITHDRAWN_FN)\n",
    "    print(f\"Loaded withdrawal list: {len(withdrawn_ids)} unique IDs.\")\n",
    "\n",
    "    # Pre-removal report\n",
    "    summarize_by_treatment(df, \"Pre-removal sample size by treatment_var\")\n",
    "\n",
    "    # Remove withdrawn IDs\n",
    "    before_n = len(df)\n",
    "    mask_keep = ~df[id_col].isin(withdrawn_ids)\n",
    "    removed_n = int((~mask_keep).sum())\n",
    "    df_clean = df.loc[mask_keep].copy()\n",
    "\n",
    "    # Post-removal report\n",
    "    summarize_by_treatment(df_clean, \"Post-removal sample size by treatment_var\")\n",
    "\n",
    "    # High-level removal statement (publication-ready)\n",
    "    print(\"\\n--- Publication-Grade Summary ---\")\n",
    "    print(f\"Participants removed due to withdrawn consent: n = {removed_n} \"\n",
    "          f\"(from N = {before_n} to N = {len(df_clean)}).\")\n",
    "    print(\"Post-removal counts are reported above by 'treatment_var'.\")\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    df_clean.to_csv(DATA_OUT, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n✅ Cleaned dataset saved to: {DATA_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bacade30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\1731210757.py:858: DtypeWarning: Columns (24,25,33,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(INPUT_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fix] Filled missing Genetic_ethnic_grouping as 0 (Non-White). Unique values now: [1, 0]\n",
      "APOE merge report\n",
      "- Source file: APOE_calls.csv\n",
      "- Join: Participant_ID (main) ← IID (APOE)\n",
      "- APOE rows: 487409; unique IID: 487409\n",
      "- Main rows: 501931\n",
      "- Matched in main: 486740\n",
      "- Unmatched in main (APOE NA): 15191\n",
      "- Kept columns: ['e4_count', 'e2_count', 'APOE_genotype', 'APOE_e4_carrier', 'APOE_e2_carrier']\n",
      "[APOE] Merge OK: matched=486740, unmatched_in_main=15191\n",
      "[SR detect] Found 102 long-name 20002 columns. Examples: ['Non_cancer_illness_code_self_reported_Instance_0_Array_0', 'Non_cancer_illness_code_self_reported_Instance_0_Array_1', 'Non_cancer_illness_code_self_reported_Instance_0_Array_2', 'Non_cancer_illness_code_self_reported_Instance_0_Array_3', 'Non_cancer_illness_code_self_reported_Instance_0_Array_4', 'Non_cancer_illness_code_self_reported_Instance_0_Array_5']\n",
      "[SR detect] Rows with self-reported 1123 = 377\n",
      "[SA ascertainment] Hospital_Primary=329, Hospital_Secondary=558, Self_Report_Only=150\n",
      "[SA ascertainment] Self_Report_Only count = 150 (0.145 of Study)\n",
      "[Counts | After SA flags (WMH-complete)] Study=1037, Control=60082, Total=61119\n",
      "[Counts | After SA-negative exclusion] Study=880, Control=60082, Total=60962\n",
      "[DONE] Final dataset saved: data_processed.csv\n",
      "       Documentation saved in: Process/\n"
     ]
    }
   ],
   "source": [
    "#Cleaning & Final Dataset Preparation\n",
    "\"\"\"\n",
    "UK Biobank Cleaning & Final Dataset Preparation (Submission-Ready)\n",
    "-----------------------------------------------------------------\n",
    "- Reads:  data.csv\n",
    "- Writes: data_processed.csv (final analysis dataset)\n",
    "          Process/Derived_Variables_UKB.docx / .csv\n",
    "          Process/eMethod_DataProcessing.docx\n",
    "          Process/Sample_Flow.txt\n",
    "\n",
    "Pipeline\n",
    "1) Normalize column names (spaces & all special characters -> underscores).\n",
    "2) Derive variables (overwrite if present):\n",
    "   - Age_at_Instance_2\n",
    "   - Smoking_Ever\n",
    "   - Alcohol_intake_frequency_ordinal\n",
    "3) Impute missing values:\n",
    "   - Continuous -> median (Age_at_Instance_2, Alcohol_intake_frequency_ordinal)\n",
    "   - Categorical/Binary -> mode (Smoking_Ever)\n",
    "4) Add Group labels from treatment_var (Study/Control, overwrite if present).\n",
    "5) Exclude participants with missing WMH outcome\n",
    "   ('Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2').\n",
    "6) Save final dataset and generate Word documentation via python-docx.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Word generation (python-docx) ---\n",
    "# pip install python-docx\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.shared import Inches, Pt\n",
    "    from docx.oxml.ns import qn\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docx.enum.table import WD_TABLE_ALIGNMENT\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        \"python-docx is required for Word outputs. \"\n",
    "        \"Install with: pip install python-docx\\n\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "INPUT_FILE   = \"data.csv\"\n",
    "OUTPUT_FINAL = \"data_processed.csv\"\n",
    "PROCESS_DIR  = Path(\"Process\")\n",
    "PROCESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Raw WMH outcome column name (as it appears in the original CSV, BEFORE cleaning).\n",
    "WMH_COL_RAW = \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"\n",
    "\n",
    "# NEW: APOE calls file (after column-name cleaning we will merge it)\n",
    "APOE_FILE = \"APOE_calls.csv\"\n",
    "APOE_KEEP_COLS = [\"IID\", \"e4_count\", \"e2_count\", \"APOE_genotype\", \"APOE_e4_carrier\", \"APOE_e2_carrier\"]\n",
    "\n",
    "# SA diagnosis date source column (as it appears in the original CSV, BEFORE cleaning).\n",
    "SA_DATE_RAW = \"Date_G47_first_reported_sleep_disorders\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers: data processing\n",
    "# =========================\n",
    "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize all column names to snake_case with underscores.\"\"\"\n",
    "    cols = (\n",
    "        df.columns\n",
    "          .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "          .str.replace(r\"_{2,}\", \"_\", regex=True)\n",
    "          .str.strip(\"_\")\n",
    "    )\n",
    "    out = df.copy()\n",
    "    out.columns = cols\n",
    "    return out\n",
    "\n",
    "def derive_variables(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create/overwrite Age_at_Instance_2, Smoking_Ever, Alcohol_intake_frequency_ordinal.\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    col_i2_date  = \"Date_of_attending_assessment_centre_Instance_2\"  # UKB Field 53 (I2)\n",
    "    col_yob      = \"Year_of_birth\"                                   # UKB Field 34\n",
    "    col_smoke_i2 = \"Smoking_status_Instance_2\"                        # UKB Field 20160 (I2)\n",
    "    col_alc_i2   = \"Alcohol_intake_frequency_Instance_2\"             # UKB Field 1558  (I2)\n",
    "\n",
    "    # Age_at_Instance_2 = year(Date I2) - Year of birth\n",
    "    if col_i2_date in out.columns and col_yob in out.columns:\n",
    "        out[col_i2_date] = pd.to_datetime(out[col_i2_date], errors=\"coerce\")\n",
    "        out[\"Age_at_Instance_2\"] = out[col_i2_date].dt.year - out[col_yob]\n",
    "    else:\n",
    "        out[\"Age_at_Instance_2\"] = np.nan\n",
    "\n",
    "    # Smoking_Ever from coded Smoking_status_Instance_2\n",
    "    # Codes: -3=Prefer not to answer (missing), 0=Never, 1=Previous, 2=Current\n",
    "    if col_smoke_i2 in out.columns:\n",
    "        s = out[col_smoke_i2].replace(-3, np.nan)\n",
    "        out[\"Smoking_Ever\"] = s.apply(\n",
    "            lambda v: 1 if pd.notna(v) and v in [1, 2]\n",
    "            else (0 if pd.notna(v) and v == 0 else np.nan)\n",
    "        )\n",
    "    else:\n",
    "        out[\"Smoking_Ever\"] = np.nan\n",
    "\n",
    "    # Alcohol_intake_frequency_ordinal (ordinal mapping; -3 -> missing)\n",
    "    freq_map = {6:0, 5:1, 4:2, 3:3, 2:4, 1:5}\n",
    "    if col_alc_i2 in out.columns:\n",
    "        a = out[col_alc_i2].replace(-3, np.nan)\n",
    "        out[\"Alcohol_intake_frequency_ordinal\"] = a.map(freq_map)\n",
    "    else:\n",
    "        out[\"Alcohol_intake_frequency_ordinal\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "def merge_apoe_calls(df_main: pd.DataFrame,\n",
    "                     apoe_path: str,\n",
    "                     keep_cols: list[str]) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Merge APOE_calls.csv into the main dataframe (LEFT JOIN).\n",
    "    - Main key: Participant_ID  (main file)\n",
    "    - APOE key: IID             (APOE file)\n",
    "    - Keep only the five requested APOE columns.\n",
    "    - All keys coerced to string for safe join.\n",
    "\n",
    "    Returns:\n",
    "        df_merged, stats_dict\n",
    "    \"\"\"\n",
    "    stats = {\"apoe_rows\": 0, \"apoe_unique_iid\": 0, \"merged_on\": \"Participant_ID ← IID\",\n",
    "             \"matched_n\": 0, \"unmatched_in_main\": 0}\n",
    "\n",
    "    apoe = pd.read_csv(apoe_path)\n",
    "    # Clean APOE column names to be consistent with main cleaning\n",
    "    apoe.columns = (\n",
    "        apoe.columns\n",
    "            .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "            .str.replace(r\"_{2,}\", \"_\", regex=True)\n",
    "            .str.strip(\"_\")\n",
    "    )\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    missing = [c for c in keep_cols if c not in apoe.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"APOE file missing columns: {missing}. Present={list(apoe.columns)}\")\n",
    "\n",
    "    apoe = apoe[keep_cols].copy()\n",
    "\n",
    "    # Key alignment: main uses Participant_ID; APOE uses IID\n",
    "    apoe[\"IID\"] = apoe[\"IID\"].astype(str)\n",
    "    out = df_main.copy()\n",
    "    if \"Participant_ID\" not in out.columns:\n",
    "        raise KeyError(\"Main dataframe lacks 'Participant_ID' needed for APOE merge.\")\n",
    "    out[\"Participant_ID\"] = out[\"Participant_ID\"].astype(str)\n",
    "\n",
    "    # Handle duplicated IID in APOE (keep first; log)\n",
    "    apoe_dedup = apoe.drop_duplicates(subset=[\"IID\"], keep=\"first\")\n",
    "    stats[\"apoe_rows\"] = len(apoe)\n",
    "    stats[\"apoe_unique_iid\"] = apoe_dedup[\"IID\"].nunique()\n",
    "    if len(apoe_dedup) < len(apoe):\n",
    "        print(f\"[APOE] Duplicated IID detected: {len(apoe) - len(apoe_dedup)} rows removed (keeping first).\")\n",
    "\n",
    "    # Left join\n",
    "    out = out.merge(apoe_dedup.rename(columns={\"IID\": \"Participant_ID\"}),\n",
    "                    on=\"Participant_ID\", how=\"left\")\n",
    "\n",
    "    # Merge stats\n",
    "    matched_mask = out[[\"e4_count\",\"e2_count\",\"APOE_genotype\",\"APOE_e4_carrier\",\"APOE_e2_carrier\"]].notna().any(axis=1)\n",
    "    stats[\"matched_n\"] = int(matched_mask.sum())\n",
    "    stats[\"unmatched_in_main\"] = int(len(out) - stats[\"matched_n\"])\n",
    "\n",
    "    # Write a small report\n",
    "    rep = (\n",
    "        f\"APOE merge report\\n\"\n",
    "        f\"- Source file: {apoe_path}\\n\"\n",
    "        f\"- Join: Participant_ID (main) ← IID (APOE)\\n\"\n",
    "        f\"- APOE rows: {stats['apoe_rows']}; unique IID: {stats['apoe_unique_iid']}\\n\"\n",
    "        f\"- Main rows: {len(df_main)}\\n\"\n",
    "        f\"- Matched in main: {stats['matched_n']}\\n\"\n",
    "        f\"- Unmatched in main (APOE NA): {stats['unmatched_in_main']}\\n\"\n",
    "        f\"- Kept columns: {keep_cols[1:]}\\n\"\n",
    "    )\n",
    "    (PROCESS_DIR / \"APOE_Merge_Report.txt\").write_text(rep, encoding=\"utf-8\")\n",
    "    print(rep.strip())\n",
    "    return out, stats\n",
    "\n",
    "\n",
    "def derive_has_degree(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Derive binary has_degree from UK Biobank Qualifications (Data-Coding 100305, multi-select).\n",
    "\n",
    "    Logic:\n",
    "        - The qualifications field is stored as a multi-select string\n",
    "          (e.g. \"1\", \"2|3|5\", \"1|2|3\", \"-7\", \"-3\").\n",
    "        - Coding 100305:\n",
    "              1  = College or University degree\n",
    "              2  = A levels/AS levels or equivalent\n",
    "              3  = O levels/GCSEs or equivalent\n",
    "              4  = CSEs or equivalent\n",
    "              5  = NVQ/HND/HNC or equivalent\n",
    "              6  = Other professional qualifications\n",
    "             -7  = None of the above\n",
    "             -3  = Prefer not to answer\n",
    "\n",
    "        - has_degree:\n",
    "              1 if ANY code == 1\n",
    "              0 if NO code == 1 AND at least one code in {2,3,4,5,6,-7}\n",
    "              NaN if only -3, empty, or uninformative\n",
    "\n",
    "    If no qualifications-like column is found, has_degree is set to NaN.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Try to locate the qualifications column (adjust if your export uses a fixed name)\n",
    "    cand_cols = [c for c in out.columns if c.lower().startswith(\"qualifications\")]\n",
    "    if not cand_cols:\n",
    "        out[\"has_degree\"] = np.nan\n",
    "        return out\n",
    "\n",
    "    col = cand_cols[0]\n",
    "    s = out[col]\n",
    "\n",
    "    def _parse_has_degree(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "\n",
    "        text = str(val).strip()\n",
    "        if text == \"\":\n",
    "            return np.nan\n",
    "\n",
    "        # Split on common delimiters for multi-select encoding\n",
    "        parts = re.split(r\"[|,; ]+\", text)\n",
    "        parts = [p for p in parts if p]\n",
    "\n",
    "        if not parts:\n",
    "            return np.nan\n",
    "\n",
    "        # Normalize tokens to integer-like strings (e.g. \"1.0\" -> \"1\")\n",
    "        norm = []\n",
    "        for p in parts:\n",
    "            try:\n",
    "                norm.append(str(int(float(p))))\n",
    "            except ValueError:\n",
    "                # Ignore non-numeric garbage\n",
    "                continue\n",
    "\n",
    "        if not norm:\n",
    "            return np.nan\n",
    "\n",
    "        # Any code 1 -> has degree\n",
    "        if \"1\" in norm:\n",
    "            return 1\n",
    "\n",
    "        # No code 1, but at least one valid non-degree code -> no degree\n",
    "        if any(x in {\"2\", \"3\", \"4\", \"5\", \"6\", \"-7\"} for x in norm):\n",
    "            return 0\n",
    "\n",
    "        # Only -3 (prefer not to answer) -> missing\n",
    "        if all(x == \"-3\" for x in norm):\n",
    "            return np.nan\n",
    "\n",
    "        # Fallback: treat remaining ambiguous patterns as missing\n",
    "        return np.nan\n",
    "\n",
    "    out[\"has_degree\"] = s.map(_parse_has_degree)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def derive_sa_flags_and_groups(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Derive SA-related flags:\n",
    "      - treatment_var: 1 if any ICD-10 contains 'G473' OR any self-report (20002) code == 1123; else 0\n",
    "      - group        : 'Study' if treatment_var==1, else 'Control'\n",
    "      - SA_ascertain_group:\n",
    "            Control                      -> 'No_SA'\n",
    "            Study & has_main             -> 'Hospital_Primary'\n",
    "            Study & ~has_main & has_sec  -> 'Hospital_Secondary'\n",
    "            Study & ~has_any             -> 'Self_Report_Only'\n",
    "      - hospitalization_exposure: 1 if Diagnoses_ICD10 non-empty, else 0\n",
    "\n",
    "    Returns:\n",
    "        df_out, self_report_only_count, self_report_only_prop\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- ICD-10 source columns (after name cleaning) ---\n",
    "    col_any_icd  = \"Diagnoses_ICD10\"            # pipe-separated ICD-10 codes (string)\n",
    "    col_main_icd = \"Diagnoses_main_ICD10\"       # primary diagnosis ICD-10 (string)\n",
    "    col_sec_icd  = \"Diagnoses_secondary_ICD10\"  # secondary diagnosis ICD-10 (string)\n",
    "\n",
    "    # --- 1) Identify self-report SA columns (UKB Field 20002, long-form names) ---\n",
    "    # Example: Non_cancer_illness_code_self_reported_Instance_0_Array_0 ... _Array_33 (Instance 0–2)\n",
    "    import re\n",
    "    pattern_long = re.compile(\n",
    "        r\"^Non_cancer_illness_code_self_reported_Instance_[0-2]_Array_([0-9]|[1-2][0-9]|3[0-3])$\"\n",
    "    )\n",
    "    self_report_cols = [c for c in out.columns if pattern_long.match(c)]\n",
    "    print(f\"[SR detect] Found {len(self_report_cols)} long-name 20002 columns. \"\n",
    "          f\"Examples: {self_report_cols[:6]}\")\n",
    "\n",
    "    # --- 2) Flag rows with self-reported SA (code 1123) robustly (numeric compare) ---\n",
    "    if self_report_cols:\n",
    "        block = out[self_report_cols]\n",
    "        block_num = block.apply(pd.to_numeric, errors=\"coerce\")  # \"1123\", \"1123.0\", 1123 -> 1123\n",
    "        sr_has_1123 = block_num.eq(1123).any(axis=1)\n",
    "        print(f\"[SR detect] Rows with self-reported 1123 = {int(sr_has_1123.sum())}\")\n",
    "    else:\n",
    "        sr_has_1123 = pd.Series(False, index=out.index)\n",
    "        print(\"[SR detect] No 20002 columns detected; self-reported SA cannot be identified.\")\n",
    "\n",
    "    # --- 3) Flags for ICD-10 SA (G473) from any/main/secondary lists ---\n",
    "    s_any  = out.get(col_any_icd,  pd.Series(\"\", index=out.index)).fillna(\"\").astype(str)\n",
    "    s_main = out.get(col_main_icd, pd.Series(\"\", index=out.index)).fillna(\"\").astype(str)\n",
    "    s_sec  = out.get(col_sec_icd,  pd.Series(\"\", index=out.index)).fillna(\"\").astype(str)\n",
    "\n",
    "    has_any  = s_any.str.contains(\"G473\", regex=False)\n",
    "    has_main = s_main.str.contains(\"G473\", regex=False)\n",
    "    has_sec  = s_sec.str.contains(\"G473\", regex=False)\n",
    "\n",
    "    # --- 4) treatment_var & group ---\n",
    "    out[\"treatment_var\"] = np.where(has_any | sr_has_1123, 1, 0)\n",
    "    out[\"group\"] = np.where(out[\"treatment_var\"].eq(1), \"Study\", \"Control\")\n",
    "\n",
    "    # --- 5) SA ascertainment group ---\n",
    "    out[\"SA_ascertain_group\"] = pd.NA\n",
    "    out.loc[out[\"group\"] == \"Control\", \"SA_ascertain_group\"] = \"No_SA\"\n",
    "    out.loc[(out[\"group\"] == \"Study\") & has_main,\n",
    "            \"SA_ascertain_group\"] = \"Hospital_Primary\"\n",
    "    out.loc[(out[\"group\"] == \"Study\") & has_any & (~has_main),\n",
    "            \"SA_ascertain_group\"] = \"Hospital_Secondary\"\n",
    "    out.loc[(out[\"group\"] == \"Study\") & (~has_any),\n",
    "            \"SA_ascertain_group\"] = \"Self_Report_Only\"\n",
    "\n",
    "    # --- 6) hospitalization_exposure: Diagnoses_ICD10 non-empty -> 1, else 0\n",
    "    non_empty_any_icd = s_any.str.strip().ne(\"\")\n",
    "    out[\"hospitalization_exposure\"] = np.where(non_empty_any_icd, 1, 0)\n",
    "\n",
    "    # --- 7) Console stats ---\n",
    "    study_mask = out[\"group\"].eq(\"Study\")\n",
    "    sro_mask   = out[\"SA_ascertain_group\"].eq(\"Self_Report_Only\")\n",
    "    self_report_only_count = int((study_mask & sro_mask).sum())\n",
    "    study_n = int(study_mask.sum())\n",
    "    self_report_only_prop  = (self_report_only_count / study_n) if study_n > 0 else float(\"nan\")\n",
    "\n",
    "    n_hp  = int((out[\"SA_ascertain_group\"] == \"Hospital_Primary\").sum())\n",
    "    n_hs  = int((out[\"SA_ascertain_group\"] == \"Hospital_Secondary\").sum())\n",
    "    n_sro = int((out[\"SA_ascertain_group\"] == \"Self_Report_Only\").sum())\n",
    "    print(f\"[SA ascertainment] Hospital_Primary={n_hp}, Hospital_Secondary={n_hs}, Self_Report_Only={n_sro}\")\n",
    "    print(f\"[SA ascertainment] Self_Report_Only count = {self_report_only_count} \"\n",
    "          f\"({self_report_only_prop:.3f} of Study)\")\n",
    "\n",
    "    return out, self_report_only_count, self_report_only_prop\n",
    "\n",
    "\n",
    "\n",
    "def compute_cmc_variables(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build CMC components (0/1), raw score (0–7), and categorical score (0/1/2),\n",
    "    using diagnosis dates prior to the MRI date at Instance 2.\n",
    "\n",
    "    Assumptions:\n",
    "      - df already contains the MRI reference date column:\n",
    "        \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "      - disease diagnosis columns follow the \"Date_XXX_first_reported_...\" naming\n",
    "      - column names have been cleaned to snake_case already\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # MRI date column\n",
    "    mri_col = \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "    if mri_col in out.columns:\n",
    "        out[mri_col] = pd.to_datetime(out[mri_col], errors=\"coerce\")\n",
    "\n",
    "    # List all diagnosis-date columns we may use (exists-or-ignore)\n",
    "    date_cols = [\n",
    "        # MRI date (included for uniform datetime handling)\n",
    "        mri_col,\n",
    "\n",
    "        # Hypertension (I10–I15)\n",
    "        \"Date_I10_first_reported_essential_primary_hypertension\",\n",
    "        \"Date_I11_first_reported_hypertensive_heart_disease\",\n",
    "        \"Date_I12_first_reported_hypertensive_renal_disease\",\n",
    "        \"Date_I13_first_reported_hypertensive_heart_and_renal_disease\",\n",
    "        \"Date_I15_first_reported_secondary_hypertension\",\n",
    "\n",
    "        # Hyperlipidemia (E78)\n",
    "        \"Date_E78_first_reported_disorders_of_lipoprotein_metabolism_and_other_lipidaemias\",\n",
    "\n",
    "        # Arrhythmia (I47 + I48 + I49)\n",
    "        \"Date_I47_first_reported_paroxysmal_tachycardia\",\n",
    "        \"Date_I48_first_reported_atrial_fibrillation_and_flutter\",\n",
    "        \"Date_I49_first_reported_other_cardiac_arrhythmias\",\n",
    "\n",
    "        # CAD (I20–I25)\n",
    "        \"Date_I20_first_reported_angina_pectoris\",\n",
    "        \"Date_I21_first_reported_acute_myocardial_infarction\",\n",
    "        \"Date_I22_first_reported_subsequent_myocardial_infarction\",\n",
    "        \"Date_I23_first_reported_complications_following_acute_myocardial_infarction\",\n",
    "        \"Date_I24_first_reported_other_acute_ischaemic_heart_diseases\",\n",
    "        \"Date_I25_first_reported_chronic_ischaemic_heart_disease\",\n",
    "\n",
    "        # Heart failure (I50)\n",
    "        \"Date_I50_first_reported_heart_failure\",\n",
    "\n",
    "        # Diabetes (E10–E14)\n",
    "        \"Date_E10_first_reported_type1_insulin_dependent_diabetes_mellitus\",\n",
    "        \"Date_E11_first_reported_type2_non_insulin_dependent_diabetes_mellitus\",\n",
    "        \"Date_E12_first_reported_malnutrition_related_diabetes_mellitus\",\n",
    "        \"Date_E13_first_reported_other_specified_diabetes_mellitus\",\n",
    "        \"Date_E14_first_reported_unspecified_diabetes_mellitus\",\n",
    "\n",
    "        # Stroke (I60–I64 only)\n",
    "        \"Date_I60_first_reported_subarachnoid_haemorrhage\",\n",
    "        \"Date_I61_first_reported_intracerebral_haemorrhage\",\n",
    "        \"Date_I62_first_reported_other_nontraumatic_intracranial_haemorrhage\",\n",
    "        \"Date_I63_first_reported_cerebral_infarction\",\n",
    "        \"Date_I64_first_reported_stroke_not_specified_as_haemorrhage_or_infarction\",\n",
    "    ]\n",
    "    for c in date_cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_datetime(out[c], errors=\"coerce\")\n",
    "\n",
    "    def diagnosed_before_mri(row, candidate_cols, mri_col_name):\n",
    "        \"\"\"\n",
    "        Return 1 if any candidate diagnosis date exists and is <= MRI date; else 0.\n",
    "        If MRI date is missing, return 0 (conservative).\n",
    "        \"\"\"\n",
    "        mri_date = row.get(mri_col_name, pd.NaT)\n",
    "        if pd.isna(mri_date):\n",
    "            return 0\n",
    "        for cc in candidate_cols:\n",
    "            if cc in row.index:\n",
    "                d = row[cc]\n",
    "                if (not pd.isna(d)) and (d <= mri_date):\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    # Component definitions\n",
    "    htn_cols = [\n",
    "        \"Date_I10_first_reported_essential_primary_hypertension\",\n",
    "        \"Date_I11_first_reported_hypertensive_heart_disease\",\n",
    "        \"Date_I12_first_reported_hypertensive_renal_disease\",\n",
    "        \"Date_I13_first_reported_hypertensive_heart_and_renal_disease\",\n",
    "        \"Date_I15_first_reported_secondary_hypertension\",\n",
    "    ]\n",
    "    lipid_cols = [\"Date_E78_first_reported_disorders_of_lipoprotein_metabolism_and_other_lipidaemias\"]\n",
    "    arrhythmia_cols = [\n",
    "        \"Date_I47_first_reported_paroxysmal_tachycardia\",\n",
    "        \"Date_I48_first_reported_atrial_fibrillation_and_flutter\",\n",
    "        \"Date_I49_first_reported_other_cardiac_arrhythmias\",\n",
    "    ]\n",
    "    cad_cols = [\n",
    "        \"Date_I20_first_reported_angina_pectoris\",\n",
    "        \"Date_I21_first_reported_acute_myocardial_infarction\",\n",
    "        \"Date_I22_first_reported_subsequent_myocardial_infarction\",\n",
    "        \"Date_I23_first_reported_complications_following_acute_myocardial_infarction\",\n",
    "        \"Date_I24_first_reported_other_acute_ischaemic_heart_diseases\",\n",
    "        \"Date_I25_first_reported_chronic_ischaemic_heart_disease\",\n",
    "    ]\n",
    "    hf_cols = [\"Date_I50_first_reported_heart_failure\"]\n",
    "    dm_cols = [\n",
    "        \"Date_E10_first_reported_type1_insulin_dependent_diabetes_mellitus\",\n",
    "        \"Date_E11_first_reported_type2_non_insulin_dependent_diabetes_mellitus\",\n",
    "        \"Date_E12_first_reported_malnutrition_related_diabetes_mellitus\",\n",
    "        \"Date_E13_first_reported_other_specified_diabetes_mellitus\",\n",
    "        \"Date_E14_first_reported_unspecified_diabetes_mellitus\",\n",
    "    ]\n",
    "    stroke_cols = [\n",
    "        \"Date_I60_first_reported_subarachnoid_haemorrhage\",\n",
    "        \"Date_I61_first_reported_intracerebral_haemorrhage\",\n",
    "        \"Date_I62_first_reported_other_nontraumatic_intracranial_haemorrhage\",\n",
    "        \"Date_I63_first_reported_cerebral_infarction\",\n",
    "        \"Date_I64_first_reported_stroke_not_specified_as_haemorrhage_or_infarction\",\n",
    "    ]\n",
    "\n",
    "    # Compute components (0/1)\n",
    "    out[\"CMC_hypertension\"]   = out.apply(lambda r: diagnosed_before_mri(r, htn_cols,       mri_col), axis=1)\n",
    "    out[\"CMC_hyperlipidemia\"] = out.apply(lambda r: diagnosed_before_mri(r, lipid_cols,     mri_col), axis=1)\n",
    "    out[\"CMC_arrhythmia\"]     = out.apply(lambda r: diagnosed_before_mri(r, arrhythmia_cols, mri_col), axis=1)\n",
    "    out[\"CMC_CAD\"]            = out.apply(lambda r: diagnosed_before_mri(r, cad_cols,       mri_col), axis=1)\n",
    "    out[\"CMC_heart_failure\"]  = out.apply(lambda r: diagnosed_before_mri(r, hf_cols,        mri_col), axis=1)\n",
    "    out[\"CMC_diabetes\"]       = out.apply(lambda r: diagnosed_before_mri(r, dm_cols,        mri_col), axis=1)\n",
    "    out[\"CMC_stroke\"]         = out.apply(lambda r: diagnosed_before_mri(r, stroke_cols,    mri_col), axis=1)\n",
    "\n",
    "    # Sum as raw score (0–7)\n",
    "    cmc_components = [\n",
    "        \"CMC_hypertension\",\"CMC_hyperlipidemia\",\"CMC_arrhythmia\",\n",
    "        \"CMC_CAD\",\"CMC_heart_failure\",\"CMC_diabetes\",\"CMC_stroke\",\n",
    "    ]\n",
    "    out[\"CMC_score_raw\"] = out[cmc_components].sum(axis=1)\n",
    "\n",
    "    # Categorical score: 0/1/2 (2 = ≥2 comorbidities)\n",
    "    def _categorize_cmc(score):\n",
    "        if pd.isna(score):\n",
    "            return np.nan\n",
    "        if score == 0:\n",
    "            return 0\n",
    "        elif score == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    out[\"CMC_score_cat\"] = out[\"CMC_score_raw\"].apply(_categorize_cmc)\n",
    "\n",
    "    return out\n",
    "\n",
    "def add_sa_years_and_exclude_negatives(df: pd.DataFrame, sa_col_name: str):\n",
    "    \"\"\"\n",
    "    Create 'Years_since_sleep_disorder' in years using a specific SA diagnosis date column:\n",
    "        Years = (MRI_date - SA_diagnosis_date) / 365.25\n",
    "      MRI_date: 'Date_of_attending_assessment_centre_Instance_2'\n",
    "      SA_date : sa_col_name  (e.g., 'Date_G47_first_reported_sleep_disorders')\n",
    "\n",
    "    Exclusion rule:\n",
    "      - EXCLUDE only Study (treatment_var == 1) if Years < 0\n",
    "      - Controls kept; if Years < 0 in controls, set Years = NaN.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    mri_col = \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "\n",
    "    # Ensure datetime dtype\n",
    "    if mri_col in out.columns:\n",
    "        out[mri_col] = pd.to_datetime(out[mri_col], errors=\"coerce\")\n",
    "    if sa_col_name in out.columns:\n",
    "        out[sa_col_name] = pd.to_datetime(out[sa_col_name], errors=\"coerce\")\n",
    "    else:\n",
    "        out[\"Years_since_sleep_disorder\"] = np.nan\n",
    "        print(f\"[SA years] SA date column '{sa_col_name}' not present; Years_since_sleep_disorder set to all-NaN.\")\n",
    "        return out, 0, 0\n",
    "\n",
    "    # Compute years only where both dates exist\n",
    "    both = (~out[mri_col].isna()) & (~out[sa_col_name].isna())\n",
    "    out[\"Years_since_sleep_disorder\"] = np.nan\n",
    "    if both.any():\n",
    "        delta_days = (out.loc[both, mri_col] - out.loc[both, sa_col_name]).dt.days\n",
    "        out.loc[both, \"Years_since_sleep_disorder\"] = delta_days / 365.25\n",
    "\n",
    "    # Negative-year handling\n",
    "    neg_mask     = out[\"Years_since_sleep_disorder\"] < 0\n",
    "    study_mask   = (out.get(\"treatment_var\", pd.Series(dtype=float)) == 1)\n",
    "    control_mask = (out.get(\"treatment_var\", pd.Series(dtype=float)) == 0)\n",
    "\n",
    "    exclude_mask = study_mask & neg_mask\n",
    "    excl_sa_negative_st = int(exclude_mask.sum())\n",
    "    neg_controls_kept   = int((control_mask & neg_mask).sum())\n",
    "\n",
    "    out.loc[control_mask & neg_mask, \"Years_since_sleep_disorder\"] = np.nan\n",
    "    out = out[~exclude_mask]\n",
    "\n",
    "    return out, excl_sa_negative_st, neg_controls_kept\n",
    "\n",
    "\n",
    "\n",
    "def impute_missing(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform imputation and generate a detailed before/after missingness report.\n",
    "\n",
    "    Returns:\n",
    "        df_imputed: DataFrame after imputation\n",
    "        imputation_df: DataFrame containing the imputation summary table\n",
    "                       (before/after missing counts, percentages, strategy, fill value, etc.)\n",
    "\n",
    "    Rules:\n",
    "        • Continuous variables → median imputation\n",
    "          (Age_at_Instance_2, Alcohol_intake_frequency_ordinal)\n",
    "        • Categorical/Binary/Count variables → mode imputation\n",
    "          (Smoking_Ever, has_degree, e4_count)\n",
    "          - e4_count is coerced to numeric (0/1/2), then imputed by mode, kept as integer (Int64)\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    # Define variables by strategy\n",
    "    CONTINUOUS_MEDIAN = [\"Age_at_Instance_2\", \"Alcohol_intake_frequency_ordinal\"]\n",
    "    CATEGORICAL_MODE  = [\"Smoking_Ever\", \"has_degree\", \"e4_count\"]  # << added e4_count\n",
    "\n",
    "    rows = []  # rows for the imputation report\n",
    "\n",
    "    def _miss_stat(series: pd.Series, total_n: int):\n",
    "        n = int(series.isna().sum())\n",
    "        pct = (n / total_n * 100.0) if total_n > 0 else 0.0\n",
    "        return n, round(pct, 2)\n",
    "\n",
    "    total_n = len(df_imputed)\n",
    "\n",
    "    # 1) Continuous variables → median imputation\n",
    "    for col in CONTINUOUS_MEDIAN:\n",
    "        if col in df_imputed.columns:\n",
    "            n_before, pct_before = _miss_stat(df_imputed[col], total_n)\n",
    "            if df_imputed[col].notna().any():\n",
    "                fill_value = float(df_imputed[col].median())\n",
    "                df_imputed[col] = df_imputed[col].fillna(fill_value)\n",
    "            else:\n",
    "                fill_value = 0.0\n",
    "                df_imputed[col] = df_imputed[col].fillna(fill_value)\n",
    "            n_after, pct_after = _miss_stat(df_imputed[col], total_n)\n",
    "\n",
    "            rows.append({\n",
    "                \"Variable\": col,\n",
    "                \"Type\": \"Continuous\",\n",
    "                \"Strategy\": \"Median\",\n",
    "                \"Fill_value\": fill_value,\n",
    "                \"Missing_n_before\": n_before,\n",
    "                \"Missing_%_before\": pct_before,\n",
    "                \"Imputed_n\": n_before - n_after,\n",
    "                \"Missing_n_after\": n_after,\n",
    "                \"Missing_%_after\": pct_after,\n",
    "            })\n",
    "\n",
    "    # 2) Categorical/Binary/Count variables → mode imputation\n",
    "    for col in CATEGORICAL_MODE:\n",
    "        if col in df_imputed.columns:\n",
    "            series = df_imputed[col]\n",
    "\n",
    "            # Special handling for e4_count: coerce to numeric and keep Int64\n",
    "            if col == \"e4_count\":\n",
    "                series = pd.to_numeric(series, errors=\"coerce\")  # coerce non-numeric → NaN\n",
    "                df_imputed[col] = series  # update coerced numeric back before stats\n",
    "\n",
    "            n_before, pct_before = _miss_stat(df_imputed[col], total_n)\n",
    "\n",
    "            if df_imputed[col].notna().any():\n",
    "                mode_series = df_imputed[col].mode(dropna=True)\n",
    "                fill_value = mode_series.iloc[0] if not mode_series.empty else 0\n",
    "            else:\n",
    "                fill_value = 0  # all missing → fallback to 0\n",
    "\n",
    "            df_imputed[col] = df_imputed[col].fillna(fill_value)\n",
    "\n",
    "            # Keep e4_count as integer dtype if possible\n",
    "            if col == \"e4_count\":\n",
    "                try:\n",
    "                    df_imputed[col] = df_imputed[col].astype(\"Int64\")\n",
    "                    # ensure the reported fill_value is int for consistency\n",
    "                    fill_value = int(fill_value)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            n_after, pct_after = _miss_stat(df_imputed[col], total_n)\n",
    "\n",
    "            rows.append({\n",
    "                \"Variable\": col,\n",
    "                \"Type\": \"Categorical/Binary/Count\" if col == \"e4_count\" else \"Categorical/Binary\",\n",
    "                \"Strategy\": \"Mode\",\n",
    "                \"Fill_value\": fill_value,\n",
    "                \"Missing_n_before\": n_before,\n",
    "                \"Missing_%_before\": pct_before,\n",
    "                \"Imputed_n\": n_before - n_after,\n",
    "                \"Missing_n_after\": n_after,\n",
    "                \"Missing_%_after\": pct_after,\n",
    "            })\n",
    "\n",
    "    imputation_df = pd.DataFrame(rows, columns=[\n",
    "        \"Variable\",\"Type\",\"Strategy\",\"Fill_value\",\n",
    "        \"Missing_n_before\",\"Missing_%_before\",\n",
    "        \"Imputed_n\",\"Missing_n_after\",\"Missing_%_after\"\n",
    "    ])\n",
    "    return df_imputed, imputation_df\n",
    "\n",
    "\n",
    "\n",
    "def compute_sample_flow(df: pd.DataFrame, wmh_col_clean: str) -> dict:\n",
    "    \"\"\"Return sample counts and filtered df after excluding WMH-missing.\"\"\"\n",
    "    orig_total   = len(df)\n",
    "    orig_study   = int((df.get(\"treatment_var\", pd.Series(dtype=int)) == 1).sum())\n",
    "    orig_control = int((df.get(\"treatment_var\", pd.Series(dtype=int)) == 0).sum())\n",
    "\n",
    "    filtered_df  = df[~df[wmh_col_clean].isna()]\n",
    "    final_total  = len(filtered_df)\n",
    "    final_study  = int((filtered_df.get(\"treatment_var\", pd.Series(dtype=int)) == 1).sum())\n",
    "    final_control= int((filtered_df.get(\"treatment_var\", pd.Series(dtype=int)) == 0).sum())\n",
    "    excl_total   = orig_total - final_total\n",
    "\n",
    "    return {\n",
    "        \"orig_total\": orig_total, \"orig_study\": orig_study, \"orig_control\": orig_control,\n",
    "        \"excl_total\": excl_total,\n",
    "        \"final_total\": final_total, \"final_study\": final_study, \"final_control\": final_control,\n",
    "        \"filtered_df\": filtered_df\n",
    "    }\n",
    "\n",
    "def _group_counts(df: pd.DataFrame):\n",
    "    \"\"\"Return total, Study, Control counts based on treatment_var.\"\"\"\n",
    "    total   = len(df)\n",
    "    study   = int((df.get(\"treatment_var\", pd.Series(dtype=int)) == 1).sum())\n",
    "    control = int((df.get(\"treatment_var\", pd.Series(dtype=int)) == 0).sum())\n",
    "    return total, study, control\n",
    "\n",
    "# =========================\n",
    "# Helpers: Word documents\n",
    "# =========================\n",
    "def _doc_apply_default_style(doc: Document):\n",
    "    \"\"\"Set a clean, journal-friendly default font/size.\"\"\"\n",
    "    style = doc.styles[\"Normal\"]\n",
    "    style.font.name = \"Times New Roman\"\n",
    "    style._element.rPr.rFonts.set(qn(\"w:eastAsia\"), \"Times New Roman\")\n",
    "    style.font.size = Pt(11)\n",
    "\n",
    "def export_variable_table_word_and_csv(process_dir: Path):\n",
    "    \"\"\"Create Derived_Variables_UKB.docx and CSV copy (now includes SA duration).\"\"\"\n",
    "    rows = [\n",
    "        [\n",
    "            \"Age_at_Instance_2\",\n",
    "            \"Date of attending assessment centre | 53 | Instance 2; Year of birth | 34 | —\",\n",
    "            \"Age at I2 = year(Date of attending assessment centre at Instance 2) - Year of birth.\",\n",
    "            \"Median imputation.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Smoking_Ever\",\n",
    "            \"Smoking status | 20160 | Instance 2\",\n",
    "            \"Codes: -3=Prefer not to answer (missing), 0=Never, 1=Previous, 2=Current. Smoking_Ever = 1 if code ∈ {1,2}, else 0.\",\n",
    "            \"Mode imputation.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Alcohol_intake_frequency_ordinal\",\n",
    "            \"Alcohol intake frequency | 1558 | Instance 2\",\n",
    "            \"Ordinal mapping: 6→0 (Never), 5→1, 4→2, 3→3, 2→4, 1→5; -3 treated as missing.\",\n",
    "            \"Median imputation.\"\n",
    "        ],\n",
    "        # --- If you already added CMC rows, keep them; otherwise you can add later ---\n",
    "        [\n",
    "            \"Years_since_sleep_disorder\",\n",
    "            \"Date_G47_first_reported_sleep_disorders and Date of attending assessment centre | 53 | Instance 2\",\n",
    "            (\"Years since sleep disorder diagnosis at I2 MRI: \"\n",
    "            \"Years = (Date_of_attending_assessment_centre_Instance_2 - Date_G47_first_reported_sleep_disorders) / 365.25. \"\n",
    "            \"Participants with negative values (<0) were excluded.\"),\n",
    "            \"No imputation (derived from dates; NaN when either date is missing).\"\n",
    "        ],\n",
    "\n",
    "        [\n",
    "            \"has_degree\",\n",
    "            \"Qualifications (coding 100305) | Instance 0\",\n",
    "            \"Binary indicator for tertiary education: 1 if code = 1 (College/University degree); \"\n",
    "            \"0 if code ∈ {2,3,4,5,6,-7}; -3 (prefer not to answer) treated as missing.\",\n",
    "            \"Mode imputation.\"\n",
    "        ],\n",
    "    ]\n",
    "    df_tbl = pd.DataFrame(rows, columns=[\n",
    "        \"Derived variable\", \"UKB source (name | Field ID | Instance)\",\n",
    "        \"Definition / Rule\", \"Missing-data handling\"\n",
    "    ])\n",
    "    df_tbl.to_csv(process_dir / \"Derived_Variables_UKB.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    doc = Document()\n",
    "    _doc_apply_default_style(doc)\n",
    "    h = doc.add_heading(\"Derived Variables (UK Biobank)\", level=1)\n",
    "    h.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    legend = (\"Legend. All derived variables were constructed from UK Biobank source fields. \"\n",
    "              \"Missing values were imputed using the median for continuous variables and the mode for categorical variables. \"\n",
    "              \"Date-derived variables (e.g., Years_since_sleep_disorder) were not imputed; \"\n",
    "              \"participants with negative duration were excluded.\")\n",
    "    p = doc.add_paragraph(legend)\n",
    "    p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    table = doc.add_table(rows=1, cols=len(df_tbl.columns))\n",
    "    table.alignment = WD_TABLE_ALIGNMENT.LEFT\n",
    "    hdr = table.rows[0].cells\n",
    "    for i, col in enumerate(df_tbl.columns):\n",
    "        hdr[i].text = col\n",
    "\n",
    "    for r in rows:\n",
    "        row = table.add_row().cells\n",
    "        for i, val in enumerate(r):\n",
    "            row[i].text = str(val)\n",
    "\n",
    "    out_path = process_dir / \"Derived_Variables_UKB.docx\"\n",
    "    doc.save(out_path)\n",
    "\n",
    "\n",
    "def export_emethod_word(process_dir: Path, flow_summary: str, wmh_raw_name: str, imputation_df: pd.DataFrame):\n",
    "    \"\"\"Create eMethod_DataProcessing.docx describing the data cleaning + imputation appendix + CMC definition.\"\"\"\n",
    "    doc = Document()\n",
    "    _doc_apply_default_style(doc)\n",
    "    doc.add_heading(\"eMethod: Data Processing and Cleaning\", level=1)\n",
    "\n",
    "    txt = dedent(f\"\"\"\n",
    "    We prepared the analysis dataset from a wide-format UK Biobank (UKB) extract as follows.\n",
    "\n",
    "    1) Column-name normalization and Outcome completeness screen\n",
    "       All column names were converted to snake_case by replacing spaces and special characters with underscores.\n",
    "        Before any derivations or exclusions, participants with missing WMH outcome (“Total volume of white matter hyperintensities from T1 and T2-FLAIR images, Instance 2”) were removed to ensure a consistent analytic cohort. The number excluded at this early step was recorded.  \n",
    "\n",
    "    2) Derived variables\n",
    "       • Age_at_Instance_2 was calculated as the year of the Instance 2 assessment date (UKB Field 53) minus Year of birth (Field 34).\n",
    "       • Smoking_Ever was defined from Smoking status at Instance 2 (Field 20160). Codes -3=Prefer not to answer were treated as missing; 0=Never; 1=Previous; 2=Current. Ever smokers (1 or 2) were coded as 1; never smokers as 0.\n",
    "       • Alcohol_intake_frequency_ordinal was derived from Alcohol intake frequency (Field 1558) at Instance 2 using an ordinal mapping (6→0 [Never], 5→1, 4→2, 3→3, 2→4, 1→5). Code -3 was treated as missing.\n",
    "       • CMC (comorbidity count) variables were constructed using first-reported diagnosis dates prior to the MRI date at Instance 2\n",
    "         (Date_of_attending_assessment_centre_Instance_2). Seven components (0/1 each) were included:\n",
    "           Hypertension (I10–I15), Hyperlipidemia (E78), Arrhythmia (I47/I48/I49), Coronary artery disease (I20–I25),\n",
    "           Heart failure (I50), Diabetes (E10–E14), and Stroke (I60–I64).\n",
    "         Raw CMC score (0–7) equals the sum of components; CMC category was 0 (none), 1 (single), or 2 (≥2).\n",
    "        • Years_since_sleep_disorder was computed as\n",
    "            (Date_of_attending_assessment_centre_Instance_2 - Sleep_Disorder_Diagnosis_Date) / 365.25 years.\n",
    "            Participants with negative values (<0) were excluded; the number excluded is reported in the sample flow.\n",
    "        • has_degree was derived from Qualifications (coding 100305): code 1 → 1; codes {2,3,4,5,6,-7} → 0; code -3 treated as missing.\n",
    "\n",
    "\n",
    "\n",
    "    3) Missing data\n",
    "       Continuous variables (Age_at_Instance_2, Alcohol_intake_frequency_ordinal) were imputed using the cohort median.\n",
    "       Categorical/Binary variables (Smoking_Ever, has_degree) and the count variable e4_count (0/1/2) were imputed using the mode.\n",
    "       For e4_count, values were coerced to numeric prior to imputation and kept as integer (Int64) after imputation.\n",
    "       When a variable was entirely missing, a fallback of 0 was used.\n",
    "\n",
    "    4) Group definition\n",
    "       A Group column was added as Study for participants with treatment_var=1 and Control for treatment_var=0.\n",
    "\n",
    "    5) Outcome completeness exclusion\n",
    "       Participants with missing WMH outcome were excluded. The WMH outcome field used for exclusion was:\n",
    "       “{wmh_raw_name}”. The same column was matched after column-name normalization.\n",
    "\n",
    "    6) Final dataset\n",
    "       The processed dataset (data_processed.csv) includes only participants with complete WMH outcomes and the derived covariates described above.\n",
    "\n",
    "    Sample flow summary\n",
    "    {flow_summary}\n",
    "    \"\"\").strip()\n",
    "\n",
    "    for para in txt.split(\"\\n\\n\"):\n",
    "        p = doc.add_paragraph(para)\n",
    "        p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # --- Appendix: Imputation summary (unchanged logic) ---\n",
    "    doc.add_heading(\"Appendix: Missingness and Imputation Summary\", level=2)\n",
    "    if imputation_df is not None and not imputation_df.empty:\n",
    "        imputation_csv = process_dir / \"Imputation_Report.csv\"\n",
    "        imputation_df.to_csv(imputation_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        cols = list(imputation_df.columns)\n",
    "        table = doc.add_table(rows=1, cols=len(cols))\n",
    "        table.alignment = WD_TABLE_ALIGNMENT.LEFT\n",
    "\n",
    "        hdr = table.rows[0].cells\n",
    "        for i, c in enumerate(cols):\n",
    "            hdr[i].text = str(c)\n",
    "\n",
    "        for _, r in imputation_df.iterrows():\n",
    "            row = table.add_row().cells\n",
    "            for i, c in enumerate(cols):\n",
    "                val = r[c]\n",
    "                row[i].text = str(val) if len(str(val)) <= 50 else str(val)[:47] + \"...\"\n",
    "    else:\n",
    "        doc.add_paragraph(\"No imputation summary available.\")\n",
    "\n",
    "    out_path = process_dir / \"eMethod_DataProcessing.docx\"\n",
    "    doc.save(out_path)\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    # 1) Load\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "    # 2) Normalize column names\n",
    "    df = clean_column_names(df)\n",
    "\n",
    "        # --- Fix: Fill missing Genetic_ethnic_grouping as Non-White (0) ---\n",
    "    # This field in UKB typically uses code 1 = White / Caucasian.\n",
    "    # According to project rule, any missing value should be treated as 0 (Non-White).\n",
    "    if \"Genetic_ethnic_grouping\" in df.columns:\n",
    "        df[\"Genetic_ethnic_grouping\"] = (\n",
    "            df[\"Genetic_ethnic_grouping\"]\n",
    "            .replace(\"\", np.nan)\n",
    "            .fillna(0)\n",
    "            .astype(int)\n",
    "        )\n",
    "        print(\"[Fix] Filled missing Genetic_ethnic_grouping as 0 (Non-White). \"\n",
    "              f\"Unique values now: {df['Genetic_ethnic_grouping'].unique().tolist()}\")\n",
    "    else:\n",
    "        print(\"[Warning] Column 'Genetic_ethnic_grouping' not found; skip ethnicity fix.\")\n",
    "\n",
    "    # 2.0) Merge APOE calls (after column-name cleaning; before any filtering)\n",
    "    try:\n",
    "        df, apoe_stats = merge_apoe_calls(df, APOE_FILE, APOE_KEEP_COLS)\n",
    "        print(f\"[APOE] Merge OK: matched={apoe_stats['matched_n']}, \"\n",
    "              f\"unmatched_in_main={apoe_stats['unmatched_in_main']}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[APOE] WARNING: File not found: {APOE_FILE}. Skipping APOE merge.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[APOE] WARNING: Merge failed: {e}. Skipping APOE merge.\")\n",
    "\n",
    "\n",
    "    # 2.1) Early WMH completeness filter (APPLY FIRST)\n",
    "    wmh_col_clean = (\n",
    "        pd.Index([WMH_COL_RAW])\n",
    "          .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "          .str.replace(r\"_{2,}\", \"_\", regex=True)\n",
    "          .str.strip(\"_\")\n",
    "          .tolist()[0]\n",
    "    )\n",
    "    if wmh_col_clean not in df.columns:\n",
    "        raise KeyError(\n",
    "            f\"WMH column '{WMH_COL_RAW}' not found after cleaning (expected '{wmh_col_clean}').\"\n",
    "        )\n",
    "    n0_total = len(df)\n",
    "    df = df[~df[wmh_col_clean].isna()].copy()\n",
    "    excl_wmh_early = n0_total - len(df)\n",
    "\n",
    "    # 2.2) SA flags and groups (now on WMH-complete cohort)\n",
    "    df, sro_count, sro_prop = derive_sa_flags_and_groups(df)\n",
    "    pre_total, pre_study, pre_control = _group_counts(df)\n",
    "    print(f\"[Counts | After SA flags (WMH-complete)] Study={pre_study}, Control={pre_control}, Total={pre_total}\")\n",
    "\n",
    "    # Compute the cleaned SA date column name from raw\n",
    "    sa_col_clean = (\n",
    "        pd.Index([SA_DATE_RAW])\n",
    "        .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "        .str.replace(r\"_{2,}\", \"_\", regex=True)\n",
    "        .str.strip(\"_\")\n",
    "        .tolist()[0]\n",
    "    )\n",
    "\n",
    "    if sa_col_clean not in df.columns:\n",
    "        print(f\"[SA date] WARNING: '{SA_DATE_RAW}' not found after cleaning (expected '{sa_col_clean}'). \"\n",
    "            \"Years_since_sleep_disorder will be NaN unless an alternative is supplied.\")\n",
    "\n",
    "\n",
    "    # 3) Derived variables\n",
    "    df = derive_variables(df)\n",
    "    df = derive_has_degree(df)\n",
    "    df = compute_cmc_variables(df)\n",
    "\n",
    "    # 3.3) SA years + exclude negatives (Study only)\n",
    "    orig_total_pre   = len(df)\n",
    "    orig_study_pre   = int((df.get(\"treatment_var\", pd.Series(dtype=int)) == 1).sum())\n",
    "    orig_control_pre = int((df.get(\"treatment_var\", pd.Series(dtype=int)) == 0).sum())\n",
    "    df, excl_sa_negative_st, neg_controls_kept = add_sa_years_and_exclude_negatives(df, sa_col_clean)\n",
    "    post_sa_total, post_sa_study, post_sa_control = _group_counts(df)\n",
    "    print(f\"[Counts | After SA-negative exclusion] Study={post_sa_study}, Control={post_sa_control}, Total={post_sa_total}\")\n",
    "\n",
    "    # 4) Impute missing values\n",
    "    df, imputation_df = impute_missing(df)\n",
    "\n",
    "    # 5) Save final dataset (already WMH-filtered at step 2.1)\n",
    "    df.to_csv(OUTPUT_FINAL, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 6) Sample flow log\n",
    "    flow_summary = dedent(f\"\"\"\n",
    "    Early exclusion for WMH missing (applied before any SA/derivations): {excl_wmh_early}\n",
    "    Counts after SA flags (on WMH-complete cohort): total={pre_total}, Study={pre_study}, Control={pre_control}\n",
    "    Self-Report-Only within Study: {sro_count} ({sro_prop:.3f})\n",
    "    Excluded negative SA duration in Study (Years_since_sleep_disorder < 0): {excl_sa_negative_st}\n",
    "    Controls with negative SA duration kept (Years set to NaN): {neg_controls_kept}\n",
    "    Final (WMH-complete + SA-negative exclusion): total={post_sa_total}, Study={post_sa_study}, Control={post_sa_control}\n",
    "    \"\"\").strip()\n",
    "    (PROCESS_DIR / \"Sample_Flow.txt\").write_text(flow_summary, encoding=\"utf-8\")\n",
    "\n",
    "    # 7) Documentation (Word + CSV)\n",
    "    export_variable_table_word_and_csv(PROCESS_DIR)\n",
    "    export_emethod_word(PROCESS_DIR, flow_summary, WMH_COL_RAW, imputation_df)\n",
    "\n",
    "    # 8) Done\n",
    "    print(f\"[DONE] Final dataset saved: {OUTPUT_FINAL}\")\n",
    "    print(\"       Documentation saved in: Process/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9415a1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\965059016.py:277: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Neuro Exclusion (Study only; ANY neuro date < SA index_date) =====\n",
      "Input : data_processed.csv\n",
      "Output: data_processed_exclude_neuro.csv\n",
      "\n",
      "Original: total=60962, Study=880, Control=60082\n",
      "Excluded: total=25, Study=25, Control=0\n",
      "Final   : total=60937, Study=855, Control=60082\n",
      "\n",
      "Note: Column aliasing enabled (with/without trailing underscores).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\965059016.py:664: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_primary_base = pd.read_csv(PRIMARY_INPUT_AFTER_NEURO)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Primary Matching (1:10, exact-match on hospitalization_exposure, no caliper, no replacement) =====\n",
      "Unmatched SA (no controls available): 0\n",
      "Unmatched SA (no same-exposure controls): 0\n",
      "Risk-set pruning:\n",
      "  Controls dropped (ANY neuro < index_date) : 174\n",
      "  Studies dropped (no controls left)        : 0\n",
      "\n",
      "Final matched sizes:\n",
      "  Study (after)   : 855\n",
      "  Control (after) : 8376\n",
      "  Avg controls per Study: 9.80 (target=10)\n",
      "\n",
      "Note: Column aliasing enabled; exposure exact-matching enforced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\965059016.py:670: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_sens_base = pd.read_csv(INPUT_FILE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Sensitivity Matching (1:10, exact-match on hospitalization_exposure, no caliper, no replacement, no pruning) =====\n",
      "SA with no same-exposure controls: 0\n",
      "Final matched sizes:\n",
      "  Study   : 880\n",
      "  Control : 8800\n",
      "[OK] Neurology-style eMethod saved: Matching\\eMethod_Matching.docx\n",
      "\n",
      "[DONE] Outputs:\n",
      "  • data_processed_exclude_neuro.csv\n",
      "  • primary_cohort.csv\n",
      "  • sensitivity_cohort.csv\n",
      "  • Matching diagnostics under: Matching/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\965059016.py:820: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ATO Sensitivity (MRI-anchored symmetric neuro exclusion at I2) =====\n",
      "Input : data_processed.csv\n",
      "Rows after exclusion: 59162\n",
      "\n",
      "Propensity engine: sklearn-LogisticRegression (lbfgs)\n",
      "Covariates used  : Sex, Age_at_Instance_2, Townsend_deprivation_index_at_recruitment, Body_mass_index_BMI_Instance_0, Genetic_ethnic_grouping, Smoking_Ever, Alcohol_intake_frequency_ordinal\n",
      "\n",
      "Effective Sample Size (normalized weights):\n",
      "  ESS (all)   : 3180.5\n",
      "  ESS (Study) : 828.5\n",
      "  ESS (Control): 19735.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\965059016.py:870: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ATO Sensitivity (No neuro exclusion) =====\n",
      "Input : data_processed.csv\n",
      "Rows after exclusion: 60962 (no exclusion applied)\n",
      "\n",
      "Propensity engine: sklearn-LogisticRegression (lbfgs)\n",
      "Covariates used  : Sex, Age_at_Instance_2, Townsend_deprivation_index_at_recruitment, Body_mass_index_BMI_Instance_0, Genetic_ethnic_grouping, Smoking_Ever, Alcohol_intake_frequency_ordinal\n",
      "\n",
      "Effective Sample Size (normalized weights):\n",
      "  ESS (all)   : 3360.5\n",
      "  ESS (Study) : 876.2\n",
      "  ESS (Control): 20365.2\n",
      "\n",
      "[DONE] ATO outputs (sensitivity cohorts):\n",
      "  • ato_sensitivity_sym.csv    (diagnostics in ATO\\SymmetricExclusion/)\n",
      "  • ato_sensitivity_noexclusion.csv   (diagnostics in ATO\\NoExclusion/)\n"
     ]
    }
   ],
   "source": [
    "# Matching and ATO Weighting Pipeline \n",
    "\"\"\"\n",
    "UK Biobank Propensity Score Matching and ATO Weighting Pipeline\n",
    "=======================================================================================\n",
    "This script implements a reproducible pipeline for propensity score (PS)–based cohort\n",
    "construction and overlap weighting, designed for publication-quality epidemiologic\n",
    "analyses of UK Biobank neuroimaging data.\n",
    "\n",
    "Workflow\n",
    "--------\n",
    "1. Neuro Exclusion (Primary Cohort)\n",
    "   • Exclude Study participants with any recorded neurologic disease prior to the\n",
    "     sleep apnea (SA) index date.\n",
    "   • Control participants are not excluded at this stage.\n",
    "\n",
    "2. Propensity Score Estimation\n",
    "   • Logistic regression using prespecified baseline covariates:\n",
    "     age at MRI, sex, baseline BMI, Townsend deprivation index, genetic ethnic\n",
    "     grouping, smoking status, and alcohol intake frequency.\n",
    "   • Continuous covariates are median-imputed; categorical covariates are mode-imputed\n",
    "     and label-encoded.\n",
    "   • Primary engine: scikit-learn (lbfgs); fallback: NumPy IRLS.\n",
    "\n",
    "3. 1:10 Nearest-Neighbor Matching (PS logit scale)\n",
    "   • Matching performed without replacement and without caliper.\n",
    "   • Primary cohort: post-matching risk-set pruning removes controls with any\n",
    "     neurologic disease preceding the assigned SA index date.\n",
    "   • Sensitivity cohort: same matching procedure but without pruning.\n",
    "\n",
    "4. Balance Diagnostics\n",
    "   • Standardized mean differences (SMDs) calculated for each covariate before and\n",
    "     after matching.\n",
    "   • Results exported as CSV for transparency and reproducibility.\n",
    "\n",
    "5. Supplementary eMethod (Word Document)\n",
    "   • Generates a publication-ready description of the study population, covariates,\n",
    "     PS estimation, matching algorithm, and balance assessment.\n",
    "\n",
    "6. ATO (Average Treatment effect in the Overlap population) Weighting\n",
    "   • Primary ATO: MRI-anchored symmetric neuro exclusion at Instance 2, followed by\n",
    "     PS estimation and overlap weights.\n",
    "   • Sensitivity ATO: pseudo-index construction stratified by PS quantiles, symmetric\n",
    "     neuro exclusion, PS re-estimation, and overlap weights.\n",
    "   • Outputs include weighted balance diagnostics and effective sample size.\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "• Primary and sensitivity matched cohorts (CSV)\n",
    "• Balance diagnostics before and after matching\n",
    "• Neurology-style eMethod (Word, publication-ready)\n",
    "• ATO cohorts (primary and sensitivity) with weights and diagnostics\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "• pandas, numpy, python-docx\n",
    "• scikit-learn (preferred; automatic fallback to NumPy IRLS if unavailable)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn logistic regression (preferred)\n",
    "USE_SKLEARN = True\n",
    "try:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "except Exception:\n",
    "    USE_SKLEARN = False\n",
    "\n",
    "# Word export\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "INPUT_FILE = \"data_processed.csv\"\n",
    "\n",
    "PRIMARY_INPUT_AFTER_NEURO = \"data_processed_exclude_neuro.csv\"\n",
    "PRIMARY_OUT     = \"primary_cohort.csv\"\n",
    "SENSITIVITY_OUT = \"sensitivity_cohort.csv\"\n",
    "\n",
    "MATCH_DIR         = Path(\"Matching\")\n",
    "MATCH_DIR.mkdir(exist_ok=True)\n",
    "MATCH_DIR_PRIMARY = MATCH_DIR / \"Primary\"\n",
    "MATCH_DIR_SENS    = MATCH_DIR / \"Sensitivity\"\n",
    "for p in [MATCH_DIR_PRIMARY, MATCH_DIR_SENS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ID & core fields\n",
    "COL_ID_CANDIDATES = [\"Participant_ID\", \"eid\"]  # fallback: first column\n",
    "COL_TREAT         = \"treatment_var\"            # 1=Study, 0=Control\n",
    "\n",
    "# ---- Canonical names used INSIDE code ----\n",
    "# We'll automatically map real CSV columns (with or without trailing underscore) to these canonical names.\n",
    "COL_SA_DATE_CANON = \"Date_G47_first_reported_sleep_disorders\"  # canonical\n",
    "NEURO_DATE_COLS_CANON = [\n",
    "    \"Date_I63_first_reported_cerebral_infarction\",\n",
    "    \"Date_I64_first_reported_stroke_not_specified_as_haemorrhage_or_infarction\",\n",
    "    \"Date_G45_first_reported_transient_cerebral_ischaemic_attacks_and_related_syndromes\",\n",
    "    \"Date_G35_first_reported_multiple_sclerosis\",\n",
    "    \"Date_I67_first_reported_other_cerebrovascular_diseases\",\n",
    "]\n",
    "\n",
    "# Prespecified covariates (use-if-present; others skipped with a warning)\n",
    "BASE_COVARIATES = [\n",
    "    \"Sex\",\n",
    "    \"Age_at_Instance_2\",\n",
    "    \"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\n",
    "    \"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\n",
    "    \"Alcohol_intake_frequency_ordinal\",\n",
    "]\n",
    "\n",
    "# Matching parameters\n",
    "RATIO = 10\n",
    "WITH_REPLACEMENT = False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Column aliasing (handles trailing underscores removed by your CSV)\n",
    "# =========================\n",
    "# Map from our canonical name -> list of acceptable variants in the CSV (priority order)\n",
    "COLUMN_ALIASES = {\n",
    "    # SA index date\n",
    "    COL_SA_DATE_CANON: [\n",
    "        \"Date_G47_first_reported_sleep_disorders\",          # preferred\n",
    "        \"Date_G47_first_reported_sleep_disorders\"           # without trailing underscore\n",
    "    ],\n",
    "    # Neuro dates\n",
    "    \"Date_I63_first_reported_cerebral_infarction\": [\n",
    "        \"Date_I63_first_reported_cerebral_infarction\",\n",
    "        \"Date_I63_first_reported_cerebral_infarction\"\n",
    "    ],\n",
    "    \"Date_I64_first_reported_stroke_not_specified_as_haemorrhage_or_infarction\": [\n",
    "        \"Date_I64_first_reported_stroke_not_specified_as_haemorrhage_or_infarction\",\n",
    "        \"Date_I64_first_reported_stroke_not_specified_as_haemorrhage_or_infarction\"\n",
    "    ],\n",
    "    \"Date_G45_first_reported_transient_cerebral_ischaemic_attacks_and_related_syndromes\": [\n",
    "        \"Date_G45_first_reported_transient_cerebral_ischaemic_attacks_and_related_syndromes\",\n",
    "        \"Date_G45_first_reported_transient_cerebral_ischaemic_attacks_and_related_syndromes\"\n",
    "    ],\n",
    "    \"Date_G35_first_reported_multiple_sclerosis\": [\n",
    "        \"Date_G35_first_reported_multiple_sclerosis\",\n",
    "        \"Date_G35_first_reported_multiple_sclerosis\"\n",
    "    ],\n",
    "    \"Date_I67_first_reported_other_cerebrovascular_diseases\": [\n",
    "        \"Date_I67_first_reported_other_cerebrovascular_diseases\",\n",
    "        \"Date_I67_first_reported_other_cerebrovascular_diseases\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "def apply_column_aliases(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure that the DataFrame has our canonical column names by renaming\n",
    "    any existing variant (with/without trailing underscore) to the canonical.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    current = set(df.columns)\n",
    "    rename_map = {}\n",
    "    for canon, variants in COLUMN_ALIASES.items():\n",
    "        for v in variants:\n",
    "            if v in current:\n",
    "                rename_map[v] = canon\n",
    "                break\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def pick_id_column(df: pd.DataFrame) -> str:\n",
    "    for c in COL_ID_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return df.columns[0]\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_datetime(out[c], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "def ps_logit(p: np.ndarray) -> np.ndarray:\n",
    "    p = np.clip(p, 1e-6, 1 - 1e-6)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def fill_and_encode_inplace(df: pd.DataFrame, cols: list) -> tuple[pd.DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Prepare covariates for logistic regression:\n",
    "      - numeric: median imputation\n",
    "      - non-numeric: mode imputation -> categorical codes\n",
    "    Returns (prepared_df, actually_used_covs)\n",
    "    \"\"\"\n",
    "    used = []\n",
    "    out = df.copy()\n",
    "    for col in cols:\n",
    "        if col not in out.columns:\n",
    "            print(f\"[WARN] Missing covariate dropped: {col}\")\n",
    "            continue\n",
    "        s = out[col]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            out[col] = s.fillna(s.median()) if s.notna().any() else s.fillna(0)\n",
    "        else:\n",
    "            if s.isna().all():\n",
    "                out[col] = \"Unknown\"\n",
    "            else:\n",
    "                mode_val = s.mode(dropna=True)\n",
    "                mode_val = mode_val.iloc[0] if not mode_val.empty else \"Unknown\"\n",
    "                out[col] = s.fillna(mode_val)\n",
    "            out[col] = out[col].astype(\"category\").cat.codes\n",
    "        used.append(col)\n",
    "    return out, used\n",
    "\n",
    "def smd_numeric(x: pd.Series, g: pd.Series) -> float:\n",
    "    \"\"\"Standardized mean difference for numeric variable by binary group g (1=Study, 0=Control).\"\"\"\n",
    "    x1 = x[g == 1].astype(float)\n",
    "    x0 = x[g == 0].astype(float)\n",
    "    if len(x1) < 2 or len(x0) < 2:\n",
    "        return np.nan\n",
    "    m1, m0 = x1.mean(), x0.mean()\n",
    "    s1, s0 = x1.std(ddof=1), x0.std(ddof=1)\n",
    "    sp = np.sqrt(((len(x1)-1)*s1**2 + (len(x0)-1)*s0**2) / max(len(x1)+len(x0)-2, 1))\n",
    "    return float((m1 - m0) / sp) if sp > 0 else np.nan\n",
    "\n",
    "def balance_table(df: pd.DataFrame, covs: list, treat_col: str, label: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in covs:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if not pd.api.types.is_numeric_dtype(s):\n",
    "            s = s.astype(\"category\").cat.codes\n",
    "        smd = smd_numeric(s, df[treat_col])\n",
    "        rows.append({\"label\": label, \"covariate\": c, \"SMD\": smd})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def write_text(path: Path, text: str):\n",
    "    path.write_text(text.strip() + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# Fallback: tiny NumPy IRLS logistic regression (only used if sklearn unavailable)\n",
    "def _irls_logistic_predict_proba(X: np.ndarray, y: np.ndarray, max_iter: int = 1000, tol: float = 1e-6) -> np.ndarray:\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]  # add intercept\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    for _ in range(max_iter):\n",
    "        z = X @ beta\n",
    "        p = 1.0 / (1.0 + np.exp(-np.clip(z, -50, 50)))\n",
    "        W = p * (1 - p) + 1e-8\n",
    "        z_tilde = z + (y - p) / W\n",
    "        XT_W = X.T * W\n",
    "        H = XT_W @ X\n",
    "        g = XT_W @ z_tilde\n",
    "        try:\n",
    "            beta_new = np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            beta_new = np.linalg.pinv(H) @ g\n",
    "        if np.max(np.abs(beta_new - beta)) < tol:\n",
    "            beta = beta_new\n",
    "            break\n",
    "        beta = beta_new\n",
    "    p_final = 1.0 / (1.0 + np.exp(-np.clip(X @ beta, -50, 50)))\n",
    "    return np.clip(p_final, 1e-9, 1-1e-9)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 1: Neuro exclusion (Study only -> primary input)\n",
    "# =========================\n",
    "def neuro_exclusion_study_only(input_file: str, output_file: str):\n",
    "    df = pd.read_csv(input_file)\n",
    "    # --- apply alias mapping so we can use canonical names below ---\n",
    "    df = apply_column_aliases(df)\n",
    "\n",
    "    need_cols = [COL_TREAT, COL_SA_DATE_CANON, *NEURO_DATE_COLS_CANON]\n",
    "    miss = [c for c in need_cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Missing required columns for neuro exclusion (after aliasing): {miss}\")\n",
    "\n",
    "    df = ensure_datetime(df, [COL_SA_DATE_CANON] + NEURO_DATE_COLS_CANON)\n",
    "    study_mask = df[COL_TREAT] == 1\n",
    "    lt_matrix = pd.DataFrame({c: df[c].lt(df[COL_SA_DATE_CANON]) for c in NEURO_DATE_COLS_CANON})\n",
    "    exclude_mask = study_mask & lt_matrix.any(axis=1)\n",
    "\n",
    "    kept = df[~exclude_mask].copy()\n",
    "    excluded = df[exclude_mask].copy()\n",
    "    kept.to_csv(output_file, index=False)\n",
    "\n",
    "    summ = dedent(f\"\"\"\n",
    "    ===== Neuro Exclusion (Study only; ANY neuro date < SA index_date) =====\n",
    "    Input : {input_file}\n",
    "    Output: {output_file}\n",
    "\n",
    "    Original: total={len(df)}, Study={(df[COL_TREAT]==1).sum()}, Control={(df[COL_TREAT]==0).sum()}\n",
    "    Excluded: total={len(excluded)}, Study={(excluded[COL_TREAT]==1).sum()}, Control={(excluded[COL_TREAT]==0).sum()}\n",
    "    Final   : total={len(kept)}, Study={(kept[COL_TREAT]==1).sum()}, Control={(kept[COL_TREAT]==0).sum()}\n",
    "\n",
    "    Note: Column aliasing enabled (with/without trailing underscores).\n",
    "    \"\"\").strip()\n",
    "    write_text(MATCH_DIR / \"Neuro_Exclusion_Summary.txt\", summ)\n",
    "    print(summ)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 2A: Propensity score estimation\n",
    "# =========================\n",
    "def estimate_propensity(df_in: pd.DataFrame, covariates: list) -> tuple[pd.DataFrame, list, str]:\n",
    "    \"\"\"\n",
    "    Preferred: sklearn LogisticRegression(lbfgs, max_iter=1000).\n",
    "    Fallback : pure-NumPy IRLS with identical input/output interface.\n",
    "    Returns: (df_with_ps, used_covariates, engine_str)\n",
    "    \"\"\"\n",
    "    if COL_TREAT not in df_in.columns:\n",
    "        raise KeyError(f\"'{COL_TREAT}' not found.\")\n",
    "\n",
    "    df = df_in.copy()\n",
    "    # also ensure aliasing here in case user modified columns between steps\n",
    "    df = apply_column_aliases(df)\n",
    "\n",
    "    df[COL_TREAT] = pd.to_numeric(df[COL_TREAT], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[COL_TREAT])\n",
    "\n",
    "    df, used_covs = fill_and_encode_inplace(df, covariates)\n",
    "    if not used_covs:\n",
    "        raise ValueError(\"No usable covariates for propensity estimation.\")\n",
    "\n",
    "    X = df[used_covs].to_numpy()\n",
    "    y = df[COL_TREAT].astype(int).to_numpy()\n",
    "\n",
    "    if USE_SKLEARN:\n",
    "        model = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "        model.fit(X, y)\n",
    "        df[\"propensity_score\"] = np.clip(model.predict_proba(X)[:, 1], 1e-9, 1-1e-9)\n",
    "        engine = \"sklearn-LogisticRegression (lbfgs)\"\n",
    "    else:\n",
    "        df[\"propensity_score\"] = _irls_logistic_predict_proba(X, y, max_iter=1000, tol=1e-6)\n",
    "        engine = \"NumPy-IRLS (fallback)\"\n",
    "\n",
    "    return df, used_covs, engine\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Step 2B: Matching (Primary with pruning; Sensitivity without)\n",
    "# =========================\n",
    "def match_primary(df_ps: pd.DataFrame, ratio: int, out_csv: str, diag_dir: Path):\n",
    "    # Ensure aliasing (for dates) before referencing canonical column names\n",
    "    df_ps = apply_column_aliases(df_ps)\n",
    "    id_col = pick_id_column(df_ps)\n",
    "\n",
    "    need = [id_col, COL_TREAT, \"propensity_score\", COL_SA_DATE_CANON, \"hospitalization_exposure\"] + NEURO_DATE_COLS_CANON\n",
    "    miss = [c for c in need if c not in df_ps.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Missing required columns for primary matching (after aliasing): {miss}\")\n",
    "\n",
    "    df = ensure_datetime(df_ps, [COL_SA_DATE_CANON] + NEURO_DATE_COLS_CANON).copy()\n",
    "    df[\"_orig_index_\"] = np.arange(len(df))\n",
    "\n",
    "    # ensure exposure is numeric/binary\n",
    "    df[\"hospitalization_exposure\"] = pd.to_numeric(df[\"hospitalization_exposure\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    df[\"_logit_ps\"] = ps_logit(df[\"propensity_score\"].to_numpy())\n",
    "    treated  = df[df[COL_TREAT] == 1].copy().sort_values(\"_logit_ps\").reset_index(drop=True)\n",
    "    controls = df[df[COL_TREAT] == 0].copy().sort_values(\"_logit_ps\").reset_index(drop=True)\n",
    "    controls[\"_available\"] = True\n",
    "\n",
    "    matched_rows, treated_used = [], []\n",
    "    drop_no_avail = 0\n",
    "    drop_no_same_exp = 0\n",
    "\n",
    "    def pick_k_neighbors_same_exposure(trow, controls_df, k):\n",
    "        # exact-match pool: same hospitalization_exposure AND still available\n",
    "        pool = controls_df[(controls_df[\"_available\"]) &\n",
    "                           (controls_df[\"hospitalization_exposure\"] == trow[\"hospitalization_exposure\"])]\n",
    "        if pool.empty:\n",
    "            return [], True  # no same-exposure controls\n",
    "        diffs = np.abs(pool[\"_logit_ps\"].values - trow[\"_logit_ps\"])\n",
    "        order = np.argsort(diffs)\n",
    "        ordered = pool.iloc[order]\n",
    "        return ordered.index[:min(k, len(ordered))].tolist(), False\n",
    "\n",
    "    for _, t in treated.iterrows():\n",
    "        if not controls[\"_available\"].any():\n",
    "            drop_no_avail += 1\n",
    "            continue\n",
    "        idxs, no_same = pick_k_neighbors_same_exposure(t, controls, ratio)\n",
    "        if len(idxs) == 0:\n",
    "            if no_same:\n",
    "                drop_no_same_exp += 1\n",
    "            else:\n",
    "                drop_no_avail += 1\n",
    "            continue\n",
    "        treated_used.append(t[id_col])\n",
    "        for ci in idxs:\n",
    "            crow = controls.loc[ci].copy()\n",
    "            if not WITH_REPLACEMENT:\n",
    "                controls.at[ci, \"_available\"] = False\n",
    "            row = crow.to_dict()\n",
    "            row[\"match_id_orig\"] = t[id_col]\n",
    "            matched_rows.append(row)\n",
    "\n",
    "    matched_treated_df = df[(df[COL_TREAT]==1) & (df[id_col].isin(treated_used))].copy()\n",
    "    matched_treated_df[\"group\"] = \"Study\"\n",
    "    matched_treated_df[\"match_id_orig\"] = matched_treated_df[id_col].values\n",
    "    matched_treated_df[\"index_date\"] = matched_treated_df[COL_SA_DATE_CANON]\n",
    "\n",
    "    matched_controls_df = pd.DataFrame(matched_rows)\n",
    "    if matched_controls_df.empty:\n",
    "        raise ValueError(\"No controls matched in primary stream.\")\n",
    "    matched_controls_df[\"group\"] = \"Control\"\n",
    "    study_idx_map = matched_treated_df.set_index(\"match_id_orig\")[COL_SA_DATE_CANON].to_dict()\n",
    "    matched_controls_df[\"index_date\"] = matched_controls_df[\"match_id_orig\"].map(study_idx_map)\n",
    "\n",
    "    matched_all = pd.concat([matched_treated_df, matched_controls_df], ignore_index=True)\n",
    "\n",
    "    # Risk-set pruning on controls: drop if ANY neuro date < index_date\n",
    "    control_mask = (matched_all[\"group\"] == \"Control\")\n",
    "    lt_matrix = pd.DataFrame({\n",
    "        c: matched_all.loc[control_mask, c].lt(matched_all.loc[control_mask, \"index_date\"])\n",
    "        for c in NEURO_DATE_COLS_CANON\n",
    "    })\n",
    "    control_bad = lt_matrix.any(axis=1)\n",
    "    bad_idx = matched_all.loc[control_mask].index[control_bad.values]\n",
    "    n_ctrl_dropped = len(bad_idx)\n",
    "\n",
    "    matched_all_pruned = matched_all.drop(index=bad_idx).copy()\n",
    "\n",
    "    # Remove SA cases without remaining controls\n",
    "    post_counts = (matched_all_pruned[matched_all_pruned[\"group\"]==\"Control\"]\n",
    "                   .groupby(\"match_id_orig\")[id_col].count())\n",
    "    studies_no_ctrl = (matched_all_pruned[matched_all_pruned[\"group\"]==\"Study\"]\n",
    "                       .loc[lambda d: ~d[\"match_id_orig\"].isin(post_counts.index), [\"match_id_orig\"]])\n",
    "    n_study_dropped = len(studies_no_ctrl)\n",
    "    if n_study_dropped > 0:\n",
    "        matched_all_pruned = matched_all_pruned[\n",
    "            ~matched_all_pruned[\"match_id_orig\"].isin(studies_no_ctrl[\"match_id_orig\"])\n",
    "        ].copy()\n",
    "\n",
    "    # Recode match_id\n",
    "    study_order = (matched_all_pruned[matched_all_pruned[\"group\"]==\"Study\"]\n",
    "                   .sort_values(\"_orig_index_\")[\"match_id_orig\"].drop_duplicates().tolist())\n",
    "    id_map = {orig_id: i for i, orig_id in enumerate(study_order)}\n",
    "    matched_all_pruned[\"match_id\"] = matched_all_pruned[\"match_id_orig\"].map(id_map).astype(int)\n",
    "\n",
    "    # Sort & save\n",
    "    matched_all_pruned[\"group\"] = pd.Categorical(matched_all_pruned[\"group\"],\n",
    "                                                 categories=[\"Control\",\"Study\"], ordered=True)\n",
    "    matched_all_pruned = matched_all_pruned.sort_values(by=[\"group\", \"match_id\", id_col]).reset_index(drop=True)\n",
    "    matched_all_pruned = matched_all_pruned.drop(\n",
    "        columns=[c for c in [\"_available\",\"_logit_ps\",\"_orig_index_\",\"match_id_orig\"] if c in matched_all_pruned.columns]\n",
    "    )\n",
    "    matched_all_pruned.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Diagnostics\n",
    "    n_study_after = (matched_all_pruned[\"group\"]==\"Study\").sum()\n",
    "    n_ctrl_after  = (matched_all_pruned[\"group\"]==\"Control\").sum()\n",
    "    avg_ctrls     = (n_ctrl_after / n_study_after) if n_study_after else 0.0\n",
    "\n",
    "    bal_before = balance_table(df_ps.assign(treatment_var=df_ps[COL_TREAT].astype(int)),\n",
    "                                BASE_COVARIATES, \"treatment_var\", \"before\")\n",
    "    bal_after  = balance_table(matched_all_pruned.assign(treatment_var=(matched_all_pruned[\"group\"]==\"Study\").astype(int)),\n",
    "                               BASE_COVARIATES, \"treatment_var\", \"after\")\n",
    "    pd.concat([bal_before, bal_after], ignore_index=True)\\\n",
    "      .to_csv(diag_dir / \"Balance_Before_After.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    summ = dedent(f\"\"\"\n",
    "    ===== Primary Matching (1:{RATIO}, exact-match on hospitalization_exposure, no caliper, no replacement) =====\n",
    "    Unmatched SA (no controls available): {drop_no_avail}\n",
    "    Unmatched SA (no same-exposure controls): {drop_no_same_exp}\n",
    "    Risk-set pruning:\n",
    "      Controls dropped (ANY neuro < index_date) : {n_ctrl_dropped}\n",
    "      Studies dropped (no controls left)        : {n_study_dropped}\n",
    "\n",
    "    Final matched sizes:\n",
    "      Study (after)   : {n_study_after}\n",
    "      Control (after) : {n_ctrl_after}\n",
    "      Avg controls per Study: {avg_ctrls:.2f} (target={RATIO})\n",
    "\n",
    "    Note: Column aliasing enabled; exposure exact-matching enforced.\n",
    "    \"\"\").strip()\n",
    "    write_text(diag_dir / \"PSM_Summary.txt\", summ)\n",
    "    print(summ)\n",
    "\n",
    "\n",
    "def match_sensitivity(df_ps: pd.DataFrame, ratio: int, out_csv: str, diag_dir: Path):\n",
    "    df_ps = apply_column_aliases(df_ps)\n",
    "    id_col = pick_id_column(df_ps)\n",
    "\n",
    "    need = [id_col, COL_TREAT, \"propensity_score\", COL_SA_DATE_CANON, \"hospitalization_exposure\"]\n",
    "    miss = [c for c in need if c not in df_ps.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Missing required columns for sensitivity matching (after aliasing): {miss}\")\n",
    "\n",
    "    df = ensure_datetime(df_ps, [COL_SA_DATE_CANON]).copy()\n",
    "    df[\"hospitalization_exposure\"] = pd.to_numeric(df[\"hospitalization_exposure\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    treated = df[df[COL_TREAT] == 1].reset_index(drop=True)\n",
    "    control = df[df[COL_TREAT] == 0].reset_index(drop=True)\n",
    "\n",
    "    t_scores = ps_logit(treated[\"propensity_score\"].to_numpy())\n",
    "    c_scores = ps_logit(control[\"propensity_score\"].to_numpy())\n",
    "\n",
    "    matched_flags = np.zeros(len(control), dtype=bool)\n",
    "    match_results, drop_no_avail, drop_no_same_exp = [], 0, 0\n",
    "\n",
    "    for i, ps in enumerate(t_scores):\n",
    "        # available controls, SAME exposure only\n",
    "        same_exp_mask = (control[\"hospitalization_exposure\"].values == treated.loc[i, \"hospitalization_exposure\"])\n",
    "        avail = np.where((~matched_flags) & same_exp_mask)[0]\n",
    "        if avail.size == 0:\n",
    "            # no same-exposure controls left\n",
    "            drop_no_same_exp += 1\n",
    "            continue\n",
    "        diffs = np.abs(c_scores[avail] - ps)\n",
    "        k = min(ratio, len(avail))\n",
    "        best = avail[np.argpartition(diffs, k-1)[:k]]\n",
    "        matched_flags[best] = True\n",
    "        match_results.append((i, best.tolist()))\n",
    "\n",
    "    # Assemble\n",
    "    matched_controls = []\n",
    "    for t_idx, c_idxs in match_results:\n",
    "        for c_idx in c_idxs:\n",
    "            r = control.iloc[c_idx].copy()\n",
    "            r[\"match_id_tmp\"] = t_idx\n",
    "            matched_controls.append(r)\n",
    "    matched_controls = pd.DataFrame(matched_controls)\n",
    "\n",
    "    matched_treated_ids = [t_idx for t_idx, _ in match_results]\n",
    "    treated_sub = treated.loc[matched_treated_ids].copy()\n",
    "    treated_sub[\"old_idx\"] = treated_sub.index\n",
    "\n",
    "    id_map = {old: new for new, old in enumerate(matched_treated_ids)}\n",
    "    treated_sub[\"match_id\"] = treated_sub[\"old_idx\"].map(id_map)\n",
    "    if not matched_controls.empty:\n",
    "        matched_controls[\"match_id\"] = matched_controls[\"match_id_tmp\"].map(id_map)\n",
    "\n",
    "    treated_sub[\"group\"] = \"Study\"\n",
    "    if not matched_controls.empty:\n",
    "        matched_controls[\"group\"] = \"Control\"\n",
    "\n",
    "    # index_date propagation\n",
    "    study_index_map = treated_sub.set_index(\"match_id\")[COL_SA_DATE_CANON].to_dict()\n",
    "    treated_sub[\"index_date\"] = treated_sub[COL_SA_DATE_CANON]\n",
    "    if not matched_controls.empty:\n",
    "        matched_controls[\"index_date\"] = matched_controls[\"match_id\"].map(study_index_map)\n",
    "\n",
    "    matched_all = pd.concat([treated_sub, matched_controls], ignore_index=True)\n",
    "\n",
    "    # Cleanup & sort\n",
    "    for tmp in [\"match_id_tmp\", \"old_idx\"]:\n",
    "        if tmp in matched_all.columns:\n",
    "            matched_all = matched_all.drop(columns=[tmp])\n",
    "\n",
    "    matched_all[\"group\"] = pd.Categorical(matched_all[\"group\"], categories=[\"Control\",\"Study\"], ordered=True)\n",
    "    matched_all = matched_all.sort_values(by=[\"group\",\"match_id\", id_col]).reset_index(drop=True)\n",
    "\n",
    "    matched_all.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Diagnostics\n",
    "    n_study = (matched_all[\"group\"]==\"Study\").sum()\n",
    "    n_ctrl  = (matched_all[\"group\"]==\"Control\").sum()\n",
    "    matched_counts = [len(c_idxs) for _, c_idxs in match_results]\n",
    "    dist_text = \"\"\n",
    "    if matched_counts:\n",
    "        vc = pd.Series(matched_counts).value_counts().sort_index()\n",
    "        dist_text = \"\\n\".join([f\"  {k} controls: {v}\" for k, v in vc.items()])\n",
    "\n",
    "    bal_before = balance_table(df_ps, BASE_COVARIATES, COL_TREAT, \"before\")\n",
    "    bal_after  = balance_table(matched_all.assign(treatment_var=(matched_all[\"group\"]==\"Study\").astype(int)),\n",
    "                               BASE_COVARIATES, \"treatment_var\", \"after\")\n",
    "    pd.concat([bal_before, bal_after], ignore_index=True)\\\n",
    "      .to_csv(diag_dir / \"Balance_Before_After.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    summ = dedent(f\"\"\"\n",
    "    ===== Sensitivity Matching (1:{RATIO}, exact-match on hospitalization_exposure, no caliper, no replacement, no pruning) =====\n",
    "    SA with no same-exposure controls: {drop_no_same_exp}\n",
    "    Final matched sizes:\n",
    "      Study   : {n_study}\n",
    "      Control : {n_ctrl}\n",
    "    \"\"\").strip()\n",
    "    write_text(diag_dir / \"PSM_Summary.txt\", summ)\n",
    "    print(summ)\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# eMethod (Neurology-style Word)\n",
    "# =========================\n",
    "def _doc_style(doc: Document):\n",
    "    style = doc.styles[\"Normal\"]\n",
    "    style.font.name = \"Times New Roman\"\n",
    "    style._element.rPr.rFonts.set(qn(\"w:eastAsia\"), \"Times New Roman\")\n",
    "    style.font.size = Pt(11)\n",
    "\n",
    "def export_emethod_matching(primary_used_covs: list, sens_used_covs: list, engine_primary: str, engine_sens: str):\n",
    "    doc = Document()\n",
    "    _doc_style(doc)\n",
    "    doc.add_heading(\"Supplementary eMethod: Propensity Score Estimation and Matching\", level=1)\n",
    "\n",
    "    # Study population\n",
    "    doc.add_heading(\"Study population\", level=2)\n",
    "    para = (\"UK Biobank imaging participants with usable T1- and T2-FLAIR MRI at Instance 2 were included. \"\n",
    "            \"Participants with missing WMH outcomes were excluded upstream. The primary cohort additionally \"\n",
    "            \"excluded Study participants with any neurologic diagnosis recorded prior to the sleep apnea (SA) \"\n",
    "            \"index date; the sensitivity cohort applied no such exclusion.\")\n",
    "    doc.add_paragraph(para).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Variables\n",
    "    doc.add_heading(\"Variables\", level=2)\n",
    "    para = (\"Prespecified covariates were age at MRI, sex, baseline BMI (Instance 0), Townsend deprivation index, \"\n",
    "            \"genetic ethnic grouping (binary), smoking (ever vs never), and alcohol intake frequency (ordinal). \"\n",
    "            \"Missing data were handled by median imputation for continuous covariates and mode imputation with \"\n",
    "            \"label encoding for categorical covariates.\")\n",
    "    doc.add_paragraph(para).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Propensity score estimation\n",
    "    doc.add_heading(\"Propensity score estimation\", level=2)\n",
    "    para = (f\"Propensity scores were estimated via logistic regression separately for the two cohorts. \"\n",
    "            f\"For the primary cohort, the engine was {engine_primary}; covariates actually used: \"\n",
    "            f\"{', '.join(primary_used_covs) if primary_used_covs else 'None'}. \"\n",
    "            f\"For the sensitivity cohort, the engine was {engine_sens}; covariates actually used: \"\n",
    "            f\"{', '.join(sens_used_covs) if sens_used_covs else 'None'}. \"\n",
    "            f\"Column aliasing was implemented to harmonize date-field names with/without trailing underscores.\")\n",
    "    doc.add_paragraph(para).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Matching procedure\n",
    "    doc.add_heading(\"Matching procedure\", level=2)\n",
    "    para = (f\"Each SA case was matched to up to {RATIO} controls using nearest-neighbor matching on the logit of the \"\n",
    "            \"propensity score, without replacement and without a caliper.\")\n",
    "    doc.add_paragraph(para).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Sensitivity analysis\n",
    "    doc.add_heading(\"Sensitivity analysis\", level=2)\n",
    "    para = (\"In the primary cohort, post-matching risk-set pruning excluded controls with any neurologic diagnosis \"\n",
    "            \"preceding the matched SA index date; SA cases without remaining controls were removed. \"\n",
    "            \"The sensitivity cohort conducted the same matching without pruning.\")\n",
    "    doc.add_paragraph(para).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Balance assessment\n",
    "    doc.add_heading(\"Balance assessment\", level=2)\n",
    "    para = (\"Covariate balance was evaluated using standardized mean differences (SMDs) before and after matching. \"\n",
    "            \"Diagnostics are provided in the Matching directory.\")\n",
    "    doc.add_paragraph(para).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    out_path = MATCH_DIR / \"eMethod_Matching.docx\"\n",
    "    doc.save(out_path)\n",
    "    print(f\"[OK] Neurology-style eMethod saved: {out_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    # Step 1: Neuro exclusion -> primary input\n",
    "    neuro_exclusion_study_only(INPUT_FILE, PRIMARY_INPUT_AFTER_NEURO)\n",
    "\n",
    "    # Step 2: Primary stream\n",
    "    df_primary_base = pd.read_csv(PRIMARY_INPUT_AFTER_NEURO)\n",
    "    df_primary_base = apply_column_aliases(df_primary_base)\n",
    "    df_primary_ps, primary_used_covs, engine_primary = estimate_propensity(df_primary_base, BASE_COVARIATES)\n",
    "    match_primary(df_primary_ps, RATIO, PRIMARY_OUT, MATCH_DIR_PRIMARY)\n",
    "\n",
    "    # Step 2: Sensitivity stream\n",
    "    df_sens_base = pd.read_csv(INPUT_FILE)\n",
    "    df_sens_base = apply_column_aliases(df_sens_base)\n",
    "    df_sens_ps, sens_used_covs, engine_sens = estimate_propensity(df_sens_base, BASE_COVARIATES)\n",
    "    match_sensitivity(df_sens_ps, RATIO, SENSITIVITY_OUT, MATCH_DIR_SENS)\n",
    "\n",
    "    # eMethod (Neurology style)\n",
    "    export_emethod_matching(primary_used_covs, sens_used_covs, engine_primary, engine_sens)\n",
    "\n",
    "    print(\"\\n[DONE] Outputs:\")\n",
    "    print(f\"  • {PRIMARY_INPUT_AFTER_NEURO}\")\n",
    "    print(f\"  • {PRIMARY_OUT}\")\n",
    "    print(f\"  • {SENSITIVITY_OUT}\")\n",
    "    print(f\"  • Matching diagnostics under: {MATCH_DIR}/\")\n",
    "    if not USE_SKLEARN:\n",
    "        print(\"  [Note] sklearn not available in this environment; used NumPy IRLS fallback for PS.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# =========================\n",
    "# ATO (Overlap Weighting) cohorts\n",
    "# =========================\n",
    "\n",
    "# --- New outputs & dirs ---\n",
    "ATO_DIR = Path(\"ATO\")\n",
    "ATO_DIR_SYM = ATO_DIR / \"SymmetricExclusion\"\n",
    "ATO_DIR_NOEX = ATO_DIR / \"NoExclusion\"\n",
    "for p in [ATO_DIR, ATO_DIR_SYM, ATO_DIR_NOEX]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ATO_SYM_OUT    = \"ato_sensitivity_sym.csv\"\n",
    "ATO_NOEX_OUT   = \"ato_sensitivity_noexclusion.csv\"\n",
    "\n",
    "# --- Add a canonical I2 MRI date column & aliases (best-effort guesses; extend if needed) ---\n",
    "COL_I2_DATE_CANON = \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "COLUMN_ALIASES[COL_I2_DATE_CANON] = [\n",
    "    \"Date_of_attending_assessment_centre_Instance_2\",\n",
    "    \"Date_of_attending_assessment_centre_instance_2\",\n",
    "    \"Date_of_attending_assessment_centre_Instance_2_\",\n",
    "    \"Date_of_attending_assessment_centre_instance_2_\",\n",
    "]\n",
    "\n",
    "def detect_i2_date_column(df: pd.DataFrame) -> str | None:\n",
    "    \"\"\"\n",
    "    Heuristic: try to find an I2 date column if aliases failed.\n",
    "    Looks for case-insensitive tokens around 'Instance_2' and any of {'MRI','Imaging','assessment','centre','center','date'}.\n",
    "    \"\"\"\n",
    "    cand = []\n",
    "    toks_any = (\"mri\", \"imaging\", \"assessment\", \"centre\", \"center\", \"date\")\n",
    "    for c in df.columns:\n",
    "        low = c.lower()\n",
    "        if \"instance_2\" in low and any(t in low for t in toks_any):\n",
    "            cand.append(c)\n",
    "    if not cand:\n",
    "        for c in df.columns:\n",
    "            low = c.lower()\n",
    "            if \"instance 2\" in low and any(t in low for t in toks_any):\n",
    "                cand.append(c)\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "def ensure_i2_date_present(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure df has COL_I2_DATE_CANON (rename or detect).\"\"\"\n",
    "    df2 = apply_column_aliases(df)\n",
    "    if COL_I2_DATE_CANON not in df2.columns:\n",
    "        guess = detect_i2_date_column(df2)\n",
    "        if guess is None:\n",
    "            raise KeyError(\n",
    "                f\"Could not find I2 MRI date column. \"\n",
    "                f\"Add an alias for '{COL_I2_DATE_CANON}' to COLUMN_ALIASES or ensure a suitable column exists.\"\n",
    "            )\n",
    "        df2 = df2.rename(columns={guess: COL_I2_DATE_CANON})\n",
    "    return ensure_datetime(df2, [COL_I2_DATE_CANON])\n",
    "\n",
    "# ---------- Weighted balance diagnostics ----------\n",
    "def wmean(x: np.ndarray, w: np.ndarray) -> float:\n",
    "    w = np.asarray(w, float)\n",
    "    x = np.asarray(x, float)\n",
    "    s = w.sum()\n",
    "    return float((w * x).sum() / s) if s > 0 else np.nan\n",
    "\n",
    "def wvar(x: np.ndarray, w: np.ndarray) -> float:\n",
    "    mu = wmean(x, w)\n",
    "    s = w.sum()\n",
    "    if s <= 0:\n",
    "        return np.nan\n",
    "    return float((w * (x - mu) ** 2).sum() / s)\n",
    "\n",
    "def wstd(x: np.ndarray, w: np.ndarray) -> float:\n",
    "    v = wvar(x, w)\n",
    "    return float(np.sqrt(v)) if v >= 0 else np.nan\n",
    "\n",
    "def weighted_smd(x: pd.Series, treat: pd.Series, w: pd.Series) -> float:\n",
    "    \"\"\"Weighted SMD using pooled weighted SD.\"\"\"\n",
    "    x = x.copy()\n",
    "    if not pd.api.types.is_numeric_dtype(x):\n",
    "        x = x.astype(\"category\").cat.codes\n",
    "    g1 = (treat == 1).values\n",
    "    g0 = (treat == 0).values\n",
    "    w = w.values.astype(float)\n",
    "    xv = x.values.astype(float)\n",
    "\n",
    "    mu1 = wmean(xv[g1], w[g1]); mu0 = wmean(xv[g0], w[g0])\n",
    "    s1  = wstd(xv[g1], w[g1]);  s0  = wstd(xv[g0], w[g0])\n",
    "    denom = np.sqrt((s1**2 + s0**2) / 2.0) if np.isfinite(s1) and np.isfinite(s0) else np.nan\n",
    "    if denom is None or not np.isfinite(denom) or denom == 0:\n",
    "        return np.nan\n",
    "    return float((mu1 - mu0) / denom)\n",
    "\n",
    "def weighted_balance_table(df: pd.DataFrame, covs: list, treat_col: str, wcol: str, label: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in covs:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        smd = weighted_smd(df[c], df[treat_col], df[wcol])\n",
    "        rows.append({\"label\": label, \"covariate\": c, \"Weighted_SMD\": smd})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def effective_sample_size(w: np.ndarray) -> float:\n",
    "    w = np.asarray(w, float)\n",
    "    num = (w.sum()) ** 2\n",
    "    den = (w ** 2).sum()\n",
    "    return float(num / den) if den > 0 else np.nan\n",
    "\n",
    "# ---------- ATO weights ----------\n",
    "def add_ato_weights(df: pd.DataFrame, ps_col: str = \"propensity_score\", treat_col: str = COL_TREAT,\n",
    "                    normalize: bool = True, add_group_norm: bool = True) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if ps_col not in out.columns:\n",
    "        raise KeyError(f\"Propensity score column '{ps_col}' not found.\")\n",
    "    if treat_col not in out.columns:\n",
    "        raise KeyError(f\"Treatment column '{treat_col}' not found.\")\n",
    "    p = np.clip(out[ps_col].astype(float).values, 1e-6, 1 - 1e-6)\n",
    "    z = out[treat_col].astype(int).values\n",
    "    w = np.where(z == 1, 1.0 - p, p)  # ATO weights\n",
    "    out[\"ato_weight\"] = w\n",
    "\n",
    "    if normalize:\n",
    "        out[\"ato_weight_norm\"] = out[\"ato_weight\"] * (len(out) / out[\"ato_weight\"].sum())\n",
    "\n",
    "    if add_group_norm:\n",
    "        out[\"ato_weight_gnorm\"] = out[\"ato_weight\"]\n",
    "        for g in [0, 1]:\n",
    "            mask = (out[treat_col] == g)\n",
    "            s = out.loc[mask, \"ato_weight\"].sum()\n",
    "            if s > 0:\n",
    "                out.loc[mask, \"ato_weight_gnorm\"] = out.loc[mask, \"ato_weight\"] * (mask.sum() / s)\n",
    "    return out\n",
    "\n",
    "# ---------- ATO 1: MRI-anchored symmetric exclusion ----------\n",
    "def build_ato_sensitivity_sym(input_csv: str, covariates: list) -> pd.DataFrame:\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df = ensure_i2_date_present(df)\n",
    "    df = apply_column_aliases(df)\n",
    "    need_cols = [COL_TREAT, COL_I2_DATE_CANON] + NEURO_DATE_COLS_CANON\n",
    "    miss = [c for c in need_cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"[ATO-Sym] Missing required columns: {miss}\")\n",
    "\n",
    "    df = ensure_datetime(df, [COL_I2_DATE_CANON, COL_SA_DATE_CANON] + NEURO_DATE_COLS_CANON)\n",
    "\n",
    "    # Exclude ANY neuro date < I2 date (both Study and Control)\n",
    "    neuro_before_i2 = pd.DataFrame({c: df[c].le(df[COL_I2_DATE_CANON]) for c in NEURO_DATE_COLS_CANON})\n",
    "    df = df.loc[~neuro_before_i2.any(axis=1)].copy()\n",
    "\n",
    "    df_ps, used_covs, engine = estimate_propensity(df, covariates)\n",
    "    df_ps = add_ato_weights(df_ps, ps_col=\"propensity_score\", treat_col=COL_TREAT,\n",
    "                            normalize=True, add_group_norm=True)\n",
    "\n",
    "    diag = ATO_DIR_SYM\n",
    "    wbal = weighted_balance_table(df_ps, used_covs, COL_TREAT, \"ato_weight_norm\", \"ATO_sensitivity_sym\")\n",
    "    wbal.to_csv(diag / \"Weighted_Balance.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    ess_all  = effective_sample_size(df_ps[\"ato_weight_norm\"].values)\n",
    "    ess_treat = effective_sample_size(df_ps.loc[df_ps[COL_TREAT]==1, \"ato_weight_norm\"].values)\n",
    "    ess_ctrl  = effective_sample_size(df_ps.loc[df_ps[COL_TREAT]==0, \"ato_weight_norm\"].values)\n",
    "\n",
    "    summ = dedent(f\"\"\"\n",
    "    ===== ATO Sensitivity (MRI-anchored symmetric neuro exclusion at I2) =====\n",
    "    Input : {input_csv}\n",
    "    Rows after exclusion: {len(df_ps)}\n",
    "\n",
    "    Propensity engine: {engine}\n",
    "    Covariates used  : {', '.join(used_covs) if used_covs else 'None'}\n",
    "\n",
    "    Effective Sample Size (normalized weights):\n",
    "      ESS (all)   : {ess_all:.1f}\n",
    "      ESS (Study) : {ess_treat:.1f}\n",
    "      ESS (Control): {ess_ctrl:.1f}\n",
    "    \"\"\").strip()\n",
    "    write_text(diag / \"ATO_Summary.txt\", summ)\n",
    "    print(summ)\n",
    "\n",
    "    out_cols_first = [pick_id_column(df_ps), COL_TREAT, \"propensity_score\",\n",
    "                      \"ato_weight\", \"ato_weight_norm\", \"ato_weight_gnorm\"]\n",
    "    out_cols = out_cols_first + [c for c in df_ps.columns if c not in out_cols_first]\n",
    "    df_ps[out_cols].to_csv(ATO_SYM_OUT, index=False, encoding=\"utf-8-sig\")\n",
    "    return df_ps\n",
    "\n",
    "# ---------- ATO 2: No exclusion ----------\n",
    "def build_ato_sensitivity_noex(input_csv: str, covariates: list) -> pd.DataFrame:\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df = ensure_i2_date_present(df)\n",
    "    df = apply_column_aliases(df)\n",
    "    df = ensure_datetime(df, [COL_I2_DATE_CANON, COL_SA_DATE_CANON] + NEURO_DATE_COLS_CANON)\n",
    "\n",
    "    \n",
    "    df_ps, used_covs, engine = estimate_propensity(df, covariates)\n",
    "    df_ps = add_ato_weights(df_ps, ps_col=\"propensity_score\", treat_col=COL_TREAT,\n",
    "                            normalize=True, add_group_norm=True)\n",
    "\n",
    "    diag = ATO_DIR_NOEX\n",
    "    wbal = weighted_balance_table(df_ps, used_covs, COL_TREAT, \"ato_weight_norm\", \"ATO_sensitivity_noex\")\n",
    "    wbal.to_csv(diag / \"Weighted_Balance.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    ess_all  = effective_sample_size(df_ps[\"ato_weight_norm\"].values)\n",
    "    ess_treat = effective_sample_size(df_ps.loc[df_ps[COL_TREAT]==1, \"ato_weight_norm\"].values)\n",
    "    ess_ctrl  = effective_sample_size(df_ps.loc[df_ps[COL_TREAT]==0, \"ato_weight_norm\"].values)\n",
    "\n",
    "    summ = dedent(f\"\"\"\n",
    "    ===== ATO Sensitivity (No neuro exclusion) =====\n",
    "    Input : {input_csv}\n",
    "    Rows after exclusion: {len(df_ps)} (no exclusion applied)\n",
    "\n",
    "    Propensity engine: {engine}\n",
    "    Covariates used  : {', '.join(used_covs) if used_covs else 'None'}\n",
    "\n",
    "    Effective Sample Size (normalized weights):\n",
    "      ESS (all)   : {ess_all:.1f}\n",
    "      ESS (Study) : {ess_treat:.1f}\n",
    "      ESS (Control): {ess_ctrl:.1f}\n",
    "    \"\"\").strip()\n",
    "    write_text(diag / \"ATO_Summary.txt\", summ)\n",
    "    print(summ)\n",
    "\n",
    "    out_cols_first = [pick_id_column(df_ps), COL_TREAT, \"propensity_score\",\n",
    "                      \"ato_weight\", \"ato_weight_norm\", \"ato_weight_gnorm\"]\n",
    "    out_cols = out_cols_first + [c for c in df_ps.columns if c not in out_cols_first]\n",
    "    df_ps[out_cols].to_csv(ATO_NOEX_OUT, index=False, encoding=\"utf-8-sig\")\n",
    "    return df_ps\n",
    "\n",
    "# ---------- Convenient entrypoint ----------\n",
    "def main_ato():\n",
    "    \"\"\"\n",
    "    Build two ATO sensitivity cohorts:\n",
    "      - Symmetric exclusion at I2\n",
    "      - No exclusion\n",
    "    \"\"\"\n",
    "    df_sym = build_ato_sensitivity_sym(INPUT_FILE, BASE_COVARIATES)\n",
    "    df_noex = build_ato_sensitivity_noex(INPUT_FILE, BASE_COVARIATES)\n",
    "\n",
    "    print(\"\\n[DONE] ATO outputs (sensitivity cohorts):\")\n",
    "    print(f\"  • {ATO_SYM_OUT}    (diagnostics in {ATO_DIR_SYM}/)\")\n",
    "    print(f\"  • {ATO_NOEX_OUT}   (diagnostics in {ATO_DIR_NOEX}/)\")\n",
    "    if not USE_SKLEARN:\n",
    "        print(\"  [Note] sklearn not available; NumPy IRLS fallback was used where needed.\")\n",
    "\n",
    "\n",
    "# Optionally run ATO when this file is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main_ato()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "470695b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: Primary ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\812806688.py:333: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: Sensitivity ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\812806688.py:333: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved summary: Diagnostics\\wmh_transform_diagnostics_summary.csv\n",
      "Figures saved under: Diagnostics\\figures  (each as .png and .pdf)\n",
      "\n",
      "=== Diagnostic summary (key fields) ===\n",
      "    Dataset                                  Outcome  Raw_prop_zeros  Raw_skew  Raw_kurt_excess  Log_skew  Log_kurt_excess     BP_p_raw     BP_p_log  JB_p_raw     JB_p_log       AIC_raw      AIC_log  Improved_BP  Improved_JB  Improved_AIC  Recommend_log1p\n",
      "    Primary            Head-size Normalized Deep WMH        0.000108  7.782640       106.071481  0.002498         0.584397 1.131001e-06 3.803439e-04       0.0 2.888544e-30 178436.594648 30829.659960          1.0          1.0           1.0             True\n",
      "    Primary Head-size Normalized Periventricular WMH        0.000000  2.961961        12.645865 -0.055927         0.023317 3.223738e-74 7.333602e-05       0.0 1.167663e-05 187981.496120 22941.305013          1.0          1.0           1.0             True\n",
      "    Primary           Head-size Normalized Total WMH        0.000000  3.851548        23.124281  0.103299        -0.007365 2.278581e-36 4.917745e-07       0.0 3.388055e-03 195289.777386 23565.496728          1.0          1.0           1.0             True\n",
      "Sensitivity            Head-size Normalized Deep WMH        0.001033  6.449506        62.034015 -0.057452         0.831188 7.401022e-18 8.104567e-02       0.0 1.549789e-67 186995.334002 32530.494907          1.0          1.0           1.0             True\n",
      "Sensitivity Head-size Normalized Periventricular WMH        0.000000  2.951422        12.546760 -0.047857        -0.080579 5.109508e-73 1.403823e-03       0.0 1.215860e-02 197411.479608 24036.862929          1.0          1.0           1.0             True\n",
      "Sensitivity           Head-size Normalized Total WMH        0.000000  3.749357        21.188460  0.094368        -0.035555 2.628100e-46 4.521237e-05       0.0 9.978356e-05 205066.866269 24747.737143          1.0          1.0           1.0             True\n",
      "Saved Word report: Diagnostics\\eMethod_WMH_Transform_Diagnostics.docx\n"
     ]
    }
   ],
   "source": [
    "# WMH Transformation Diagnostics \n",
    "\"\"\"\n",
    "WMH Transformation Diagnostics (publication-ready, robust to missing statsmodels)\n",
    "================================================================================\n",
    "This script evaluates whether head-size–normalized WMH outcomes should be modeled\n",
    "on the raw scale or on the log1p scale. It:\n",
    "\n",
    "  • Loads two matched cohorts:\n",
    "      - primary_cohort.csv\n",
    "      - sensitivity_cohort.csv\n",
    "  • Computes head-size–normalized outcomes: raw × volumetric scaling factor (T1)\n",
    "  • Fits OLS models (raw vs log1p) with prespecified covariates\n",
    "  • Runs diagnostics: Breusch–Pagan (heteroscedasticity), Jarque–Bera (normality), AIC\n",
    "  • Exports histograms, QQ plots, residual–vs–fitted plots (both PNG and PDF)\n",
    "  • Writes a publication-grade summary CSV and a Word eMethod\n",
    "\n",
    "Robustness:\n",
    "  - If statsmodels imports successfully, it is used.\n",
    "  - If not, the script falls back to pure NumPy OLS and approximate p-values\n",
    "    (Wilson–Hilferty for χ²), and implements QQ plots without SciPy.\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "  • Diagnostics/wmh_transform_diagnostics_summary.csv\n",
    "  • Diagnostics/figures/*_hist.(png|pdf), *_qq_raw.(png|pdf), *_qq_log.(png|pdf), *_rvf.(png|pdf)\n",
    "  • Diagnostics/eMethod_WMH_Transform_Diagnostics.docx\n",
    "\n",
    "Dependencies: pandas, numpy, matplotlib, patsy, python-docx\n",
    "(Optional)   : statsmodels  (if available, used for exact tests/QQ plots)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try statsmodels; if not available or broken, use fallback\n",
    "HAVE_SM = True\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    from statsmodels.stats.stattools import jarque_bera\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "# Word export\n",
    "from docx import Document\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "\n",
    "# =========================\n",
    "# File sets (two cohorts)\n",
    "# =========================\n",
    "FILE_SETS = {\n",
    "    \"Primary\": \"primary_cohort.csv\",\n",
    "    \"Sensitivity\": \"sensitivity_cohort.csv\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Output folders\n",
    "# =========================\n",
    "DIAG_DIR = Path(\"Diagnostics\")\n",
    "FIG_DIR = DIAG_DIR / \"figures\"\n",
    "DIAG_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Column names (robust resolution; we DO NOT mutate your column names)\n",
    "# =========================\n",
    "SCALE_FIELD_EXPECTED = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "\n",
    "OUTCOME_DEFS = {\n",
    "    # new_col : (raw_WMH_col_in_input, pretty_label)\n",
    "    \"HSNorm_Deep_WMH\": (\n",
    "        \"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\",\n",
    "        \"Head-size Normalized Deep WMH\"\n",
    "    ),\n",
    "    \"HSNorm_PeriVentricular_WMH\": (\n",
    "        \"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\",\n",
    "        \"Head-size Normalized Periventricular WMH\"\n",
    "    ),\n",
    "    \"HSNorm_Total_WMH_T1_T2\": (\n",
    "        \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\",\n",
    "        \"Head-size Normalized Total WMH\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Prespecified covariates (only those present will be used)\n",
    "BASE_ADJUST_VARS = [\n",
    "    \"Sex\",\n",
    "    \"Age_at_Instance_2\",\n",
    "    \"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\n",
    "    \"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\n",
    "    \"Alcohol_intake_frequency_ordinal\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_VARS_ALL = {\"Sex\", \"Genetic_ethnic_grouping\", \"Smoking_Ever\", \"group\"}\n",
    "GROUP_COL = \"group\"  # expected: \"Control\"/\"Study\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers: robust column resolution\n",
    "# =========================\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"Lowercase; replace non-alnum with '_'; strip underscores.\"\"\"\n",
    "    return re.sub(r\"[^0-9a-zA-Z]+\", \"_\", name).strip(\"_\").lower()\n",
    "\n",
    "def resolve_column(df: pd.DataFrame, desired: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the actual column name in df corresponding to `desired`,\n",
    "    allowing for case/spacing/underscore differences. Raises KeyError if not found.\n",
    "    \"\"\"\n",
    "    if desired in df.columns:\n",
    "        return desired\n",
    "    norm_map = {normalize_name(c): c for c in df.columns}\n",
    "    key = normalize_name(desired)\n",
    "    if key in norm_map:\n",
    "        return norm_map[key]\n",
    "    # Relaxed endswith match\n",
    "    candidates = [c for k, c in norm_map.items() if k.endswith(key)]\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    raise KeyError(f\"Column not found (wanted='{desired}'). Sample columns: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Modeling utilities (statsmodels OR NumPy fallback)\n",
    "# =========================\n",
    "def build_formula(outcome: str, present_adjust_vars: list, categorical_vars: set, group_var: str = GROUP_COL) -> str:\n",
    "    \"\"\"OLS formula: outcome ~ C(group) + covariates (categoricals wrapped in C()).\"\"\"\n",
    "    terms = [f\"C({group_var})\"]\n",
    "    for v in present_adjust_vars:\n",
    "        if v == group_var:\n",
    "            continue\n",
    "        terms.append(f\"C({v})\" if v in categorical_vars else v)\n",
    "    return f\"{outcome} ~ \" + \" + \".join(terms)\n",
    "\n",
    "def fit_ols_sm(formula: str, data: pd.DataFrame):\n",
    "    \"\"\"OLS via statsmodels; returns (model, y_df, X_df).\"\"\"\n",
    "    y, X = dmatrices(formula, data, return_type='dataframe', NA_action='drop')\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    return model, y, X\n",
    "\n",
    "# --- Fallback math helpers (no SciPy) ---\n",
    "def _norm_ppf(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Acklam's approximation for the inverse standard normal CDF (ppf).\n",
    "    Valid for p in (0,1). Vectorized.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    # Coefficients\n",
    "    a = [-3.969683028665376e+01,  2.209460984245205e+02,\n",
    "         -2.759285104469687e+02,  1.383577518672690e+02,\n",
    "         -3.066479806614716e+01,  2.506628277459239e+00]\n",
    "    b = [-5.447609879822406e+01,  1.615858368580409e+02,\n",
    "         -1.556989798598866e+02,  6.680131188771972e+01,\n",
    "         -1.328068155288572e+01]\n",
    "    c = [-7.784894002430293e-03, -3.223964580411365e-01,\n",
    "         -2.400758277161838e+00, -2.549732539343734e+00,\n",
    "          4.374664141464968e+00,  2.938163982698783e+00]\n",
    "    d = [7.784695709041462e-03,  3.224671290700398e-01,\n",
    "         2.445134137142996e+00,  3.754408661907416e+00]\n",
    "\n",
    "    plow = 0.02425\n",
    "    phigh = 1 - plow\n",
    "    q = np.zeros_like(p)\n",
    "    # lower region\n",
    "    mask = p < plow\n",
    "    if mask.any():\n",
    "        pp = p[mask]\n",
    "        ql = np.sqrt(-2*np.log(pp))\n",
    "        q[mask] = (((((c[0]*ql + c[1])*ql + c[2])*ql + c[3])*ql + c[4])*ql + c[5]) / \\\n",
    "                   ((((d[0]*ql + d[1])*ql + d[2])*ql + d[3])*ql + 1)\n",
    "    # central\n",
    "    mask = (p >= plow) & (p <= phigh)\n",
    "    if mask.any():\n",
    "        pp = p[mask] - 0.5\n",
    "        r = pp*pp\n",
    "        q[mask] = (((((a[0]*r + a[1])*r + a[2])*r + a[3])*r + a[4])*r + a[5])*pp / \\\n",
    "                   (((((b[0]*r + b[1])*r + b[2])*r + b[3])*r + b[4])*r + 1)\n",
    "    # upper\n",
    "    mask = p > phigh\n",
    "    if mask.any():\n",
    "        pp = 1 - p[mask]\n",
    "        ql = np.sqrt(-2*np.log(pp))\n",
    "        q[mask] = -(((((c[0]*ql + c[1])*ql + c[2])*ql + c[3])*ql + c[4])*ql + c[5]) / \\\n",
    "                    ((((d[0]*ql + d[1])*ql + d[2])*ql + d[3])*ql + 1)\n",
    "    return q\n",
    "\n",
    "def _chi2_sf_wh(x: float, df: int) -> float:\n",
    "    \"\"\"\n",
    "    Wilson–Hilferty approximation of χ² survival function P[Chi2_df >= x].\n",
    "    Accurate enough for reporting in fallback mode.\n",
    "    \"\"\"\n",
    "    if x < 0 or df <= 0:\n",
    "        return float(\"nan\")\n",
    "    z = ((x/df)**(1/3) - (1 - 2/(9*df))) / math.sqrt(2/(9*df))\n",
    "    # standard normal SF\n",
    "    return 0.5 * math.erfc(z / math.sqrt(2))\n",
    "\n",
    "def fit_ols_np(formula: str, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    OLS via NumPy with patsy matrices; returns a lightweight model-like dict.\n",
    "    Provides fields: resid, fittedvalues, params, aic, model_exog, nobs, k_params\n",
    "    \"\"\"\n",
    "    y, X = dmatrices(formula, data, return_type='dataframe', NA_action='drop')\n",
    "    yv = y.values.ravel()\n",
    "    Xv = X.values\n",
    "    # Solve (X'X)β = X'y\n",
    "    XtX = Xv.T @ Xv\n",
    "    Xty = Xv.T @ yv\n",
    "    try:\n",
    "        beta = np.linalg.solve(XtX, Xty)\n",
    "    except np.linalg.LinAlgError:\n",
    "        beta = np.linalg.pinv(XtX) @ Xty\n",
    "    fitted = Xv @ beta\n",
    "    resid = yv - fitted\n",
    "    n = yv.size\n",
    "    k = Xv.shape[1]\n",
    "    rss = float(np.dot(resid, resid))\n",
    "    # Gaussian AIC up to an additive constant: n*ln(RSS/n) + 2k\n",
    "    aic = n * math.log(rss / max(n, 1)) + 2 * k\n",
    "\n",
    "    # Store what we need\n",
    "    model = {\n",
    "        \"resid\": resid,\n",
    "        \"fittedvalues\": fitted,\n",
    "        \"params\": beta,\n",
    "        \"aic\": aic,\n",
    "        \"model_exog\": Xv,  # for BP\n",
    "        \"nobs\": n,\n",
    "        \"k_params\": k,\n",
    "        \"y_df\": y,\n",
    "        \"X_df\": X,\n",
    "    }\n",
    "    return model\n",
    "\n",
    "def bp_lm_np(model) -> float:\n",
    "    \"\"\"\n",
    "    Breusch–Pagan LM statistic using auxiliary regression of squared residuals on exog.\n",
    "    Returns LM = n * R^2. DoF approx = k - 1 (excluding intercept).\n",
    "    \"\"\"\n",
    "    u2 = model[\"resid\"]**2\n",
    "    Z = model[\"model_exog\"]  # includes intercept\n",
    "    # OLS of u2 ~ Z\n",
    "    beta = np.linalg.pinv(Z) @ u2\n",
    "    u2_hat = Z @ beta\n",
    "    # R^2\n",
    "    ss_tot = float(((u2 - u2.mean())**2).sum())\n",
    "    ss_res = float(((u2 - u2_hat)**2).sum())\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "    LM = model[\"nobs\"] * r2\n",
    "    dof = max(model[\"k_params\"] - 1, 1)\n",
    "    p = _chi2_sf_wh(LM, dof)\n",
    "    return LM, p\n",
    "\n",
    "def jb_np(model) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Jarque–Bera statistic and approximated p-value (χ²_2), along with skew/kurtosis.\n",
    "    \"\"\"\n",
    "    r = model[\"resid\"]\n",
    "    n = r.size\n",
    "    if n == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    m = r.mean()\n",
    "    s2 = np.mean((r - m)**2)\n",
    "    if s2 <= 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    s = np.mean(((r - m)/math.sqrt(s2))**3)\n",
    "    k = np.mean(((r - m)/math.sqrt(s2))**4)\n",
    "    JB = n/6 * (s**2 + (k - 3)**2 / 4)\n",
    "    p = _chi2_sf_wh(JB, 2)\n",
    "    return JB, p, s, k\n",
    "\n",
    "def qqplot_fallback(resid: np.ndarray, title: str, out_base: Path):\n",
    "    \"\"\"\n",
    "    Normal QQ plot without SciPy/statsmodels:\n",
    "      - theoretical quantiles via Acklam ppf\n",
    "      - sample residuals sorted\n",
    "    \"\"\"\n",
    "    r = np.asarray(resid, dtype=float)\n",
    "    r = r[np.isfinite(r)]\n",
    "    if r.size < 3:\n",
    "        return\n",
    "    r_sorted = np.sort(r)\n",
    "    n = r_sorted.size\n",
    "    probs = (np.arange(1, n+1) - 0.5) / n\n",
    "    theo = _norm_ppf(probs)\n",
    "\n",
    "    fig = plt.figure(figsize=(5.5, 4.5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.scatter(theo, r_sorted, s=8)\n",
    "    # 45-degree fit line\n",
    "    xline = np.linspace(theo.min(), theo.max(), 100)\n",
    "    # Fit slope/intercept by least squares\n",
    "    A = np.c_[theo, np.ones_like(theo)]\n",
    "    b = np.linalg.lstsq(A, r_sorted, rcond=None)[0]\n",
    "    ax.plot(xline, b[0]*xline + b[1], linestyle=\"--\", color=\"gray\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Theoretical quantiles (N(0,1))\")\n",
    "    ax.set_ylabel(\"Sample residual quantiles\")\n",
    "    fig.tight_layout()\n",
    "    # Save PNG+PDF\n",
    "    fig.savefig(out_base.with_suffix(\".png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    fig.savefig(out_base.with_suffix(\".pdf\"), bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Plot saving helper (PNG + PDF)\n",
    "# =========================\n",
    "def _save_fig_both(fig: plt.Figure, base_path: Path):\n",
    "    fig.savefig(base_path.with_suffix(\".png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    fig.savefig(base_path.with_suffix(\".pdf\"), bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main analysis\n",
    "# =========================\n",
    "all_rows = []\n",
    "\n",
    "for label, csv_path in FILE_SETS.items():\n",
    "    print(f\"\\n=== Dataset: {label} ===\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # group checks\n",
    "    if GROUP_COL not in df.columns or df[GROUP_COL].dropna().nunique() < 2:\n",
    "        print(\"  [Skip] Invalid group column.\")\n",
    "        continue\n",
    "    df[GROUP_COL] = pd.Categorical(df[GROUP_COL], categories=[\"Control\", \"Study\"], ordered=False)\n",
    "\n",
    "    # resolve scaling factor\n",
    "    try:\n",
    "        scale_col = resolve_column(df, SCALE_FIELD_EXPECTED)\n",
    "    except KeyError as e:\n",
    "        print(f\"  [Skip] Scaling factor missing: {e}\")\n",
    "        continue\n",
    "    scale = pd.to_numeric(df[scale_col], errors=\"coerce\")\n",
    "\n",
    "    # derive outcomes and log1p\n",
    "    for new_out, (raw_expected, pretty) in OUTCOME_DEFS.items():\n",
    "        try:\n",
    "            raw_col = resolve_column(df, raw_expected)\n",
    "        except KeyError as e:\n",
    "            print(f\"  [Skip] Outcome missing: {e}\")\n",
    "            continue\n",
    "        df[new_out] = pd.to_numeric(df[raw_col], errors=\"coerce\") * scale\n",
    "        df[f\"log_{new_out}\"] = np.log1p(df[new_out])\n",
    "\n",
    "    # covariates present\n",
    "    present_adjust = [v for v in BASE_ADJUST_VARS if v in df.columns]\n",
    "    categorical_vars = {v for v in present_adjust if v in CATEGORICAL_VARS_ALL} | {\"group\"}\n",
    "\n",
    "    # gentle imputation\n",
    "    df_model = df.copy()\n",
    "    for v in present_adjust:\n",
    "        if pd.api.types.is_numeric_dtype(df_model[v]):\n",
    "            df_model[v] = pd.to_numeric(df_model[v], errors=\"coerce\")\n",
    "            df_model[v] = df_model[v].fillna(df_model[v].median())\n",
    "        else:\n",
    "            mode_val = df_model[v].mode(dropna=True)\n",
    "            mode_val = mode_val.iloc[0] if not mode_val.empty else \"Unknown\"\n",
    "            df_model[v] = df_model[v].fillna(mode_val)\n",
    "\n",
    "    # run per-outcome diagnostics\n",
    "    for new_out, (_, pretty) in OUTCOME_DEFS.items():\n",
    "        raw_var = new_out\n",
    "        log_var = f\"log_{new_out}\"\n",
    "        if raw_var not in df_model.columns or log_var not in df_model.columns:\n",
    "            continue\n",
    "\n",
    "        # build formulas\n",
    "        f_raw = build_formula(raw_var, present_adjust, categorical_vars, group_var=GROUP_COL)\n",
    "        f_log = build_formula(log_var, present_adjust, categorical_vars, group_var=GROUP_COL)\n",
    "\n",
    "        if HAVE_SM:\n",
    "            # statsmodels path\n",
    "            m_raw, y_raw, X_raw = fit_ols_sm(f_raw, df_model)\n",
    "            m_log, y_log, X_log = fit_ols_sm(f_log, df_model)\n",
    "            # BP & JB via statsmodels\n",
    "            lm, lmp, fval, fp = het_breuschpagan(m_raw.resid, m_raw.model.exog)\n",
    "            bp_lmp_raw = float(lmp)\n",
    "            lm, lmp, fval, fp = het_breuschpagan(m_log.resid, m_log.model.exog)\n",
    "            bp_lmp_log = float(lmp)\n",
    "            jb_stat, jb_p_raw, _, _ = jarque_bera(m_raw.resid)\n",
    "            jb_stat, jb_p_log, _, _ = jarque_bera(m_log.resid)\n",
    "            aic_raw = float(m_raw.aic)\n",
    "            aic_log = float(m_log.aic)\n",
    "            # y series\n",
    "            y_raw_vals = y_raw.iloc[:, 0].values\n",
    "            y_log_vals = y_log.iloc[:, 0].values\n",
    "        else:\n",
    "            # NumPy fallback\n",
    "            m_raw = fit_ols_np(f_raw, df_model)\n",
    "            m_log = fit_ols_np(f_log, df_model)\n",
    "            # BP (LM + approx p)\n",
    "            LM_raw, bp_p_raw = bp_lm_np(m_raw)\n",
    "            LM_log, bp_p_log  = bp_lm_np(m_log)\n",
    "            bp_lmp_raw, bp_lmp_log = float(bp_p_raw), float(bp_p_log)\n",
    "            # JB (stat + approx p); also get skew/kurt if needed\n",
    "            jb_stat, jb_p_raw, _, _ = jb_np(m_raw)\n",
    "            jb_stat, jb_p_log,  _, _ = jb_np(m_log)\n",
    "            aic_raw = float(m_raw[\"aic\"])\n",
    "            aic_log = float(m_log[\"aic\"])\n",
    "            y_raw_vals = m_raw[\"y_df\"].iloc[:, 0].values\n",
    "            y_log_vals = m_log[\"y_df\"].iloc[:, 0].values\n",
    "\n",
    "        # distribution descriptors (on the modeling sample)\n",
    "        def _desc(yv):\n",
    "            yv = pd.Series(yv).dropna()\n",
    "            return {\n",
    "                \"prop_zeros\": float((yv == 0).mean()) if len(yv) else np.nan,\n",
    "                \"skew\": float(yv.skew()) if len(yv) else np.nan,\n",
    "                \"kurt\": float(yv.kurtosis()) if len(yv) else np.nan,\n",
    "            }\n",
    "        desc_raw = _desc(y_raw_vals)\n",
    "        desc_log = _desc(y_log_vals)\n",
    "\n",
    "        # record row\n",
    "        row = {\n",
    "            \"Dataset\": label,\n",
    "            \"Outcome\": pretty,\n",
    "            \"n_raw\": int(len(y_raw_vals)),\n",
    "            \"n_log\": int(len(y_log_vals)),\n",
    "            \"Raw_prop_zeros\": desc_raw[\"prop_zeros\"],\n",
    "            \"Raw_skew\": desc_raw[\"skew\"],\n",
    "            \"Raw_kurt_excess\": desc_raw[\"kurt\"],\n",
    "            \"Log_skew\": desc_log[\"skew\"],\n",
    "            \"Log_kurt_excess\": desc_log[\"kurt\"],\n",
    "            \"BP_p_raw\": bp_lmp_raw,\n",
    "            \"BP_p_log\": bp_lmp_log,\n",
    "            \"JB_p_raw\": float(jb_p_raw),\n",
    "            \"JB_p_log\": float(jb_p_log),\n",
    "            \"AIC_raw\": aic_raw,\n",
    "            \"AIC_log\": aic_log,\n",
    "            \"Improved_BP\": float(bp_lmp_log > bp_lmp_raw),\n",
    "            \"Improved_JB\": float(jb_p_log > jb_p_raw),\n",
    "            \"Improved_AIC\": float(aic_log < aic_raw - 2),\n",
    "        }\n",
    "        all_rows.append(row)\n",
    "\n",
    "        # plots (PNG + PDF)\n",
    "        save_prefix = FIG_DIR / f\"{label}_{raw_var}\"\n",
    "        title_prefix = f\"{label} — {pretty}\"\n",
    "\n",
    "        # 1) Histograms\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax1.hist(y_raw_vals, bins=40)\n",
    "        ax1.set_title(f\"{title_prefix} — Raw\")\n",
    "        ax1.set_xlabel(\"Outcome\"); ax1.set_ylabel(\"Count\")\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "        ax2.hist(y_log_vals, bins=40)\n",
    "        ax2.set_title(f\"{title_prefix} — log1p\")\n",
    "        ax2.set_xlabel(\"log1p(Outcome)\")\n",
    "        fig.tight_layout()\n",
    "        _save_fig_both(fig, save_prefix.with_name(save_prefix.name + \"_hist\"))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # 2) QQ plots\n",
    "        if HAVE_SM:\n",
    "            fig_raw = sm.qqplot(m_raw.resid, line='45')\n",
    "            fig_raw.suptitle(f\"{title_prefix} — QQ (Raw)\")\n",
    "            _save_fig_both(fig_raw, save_prefix.with_name(save_prefix.name + \"_qq_raw\"))\n",
    "            plt.close(fig_raw)\n",
    "\n",
    "            fig_log = sm.qqplot(m_log.resid, line='45')\n",
    "            fig_log.suptitle(f\"{title_prefix} — QQ (log1p)\")\n",
    "            _save_fig_both(fig_log, save_prefix.with_name(save_prefix.name + \"_qq_log\"))\n",
    "            plt.close(fig_log)\n",
    "        else:\n",
    "            qqplot_fallback(m_raw[\"resid\"], f\"{title_prefix} — QQ (Raw)\",\n",
    "                            save_prefix.with_name(save_prefix.name + \"_qq_raw\"))\n",
    "            qqplot_fallback(m_log[\"resid\"], f\"{title_prefix} — QQ (log1p)\",\n",
    "                            save_prefix.with_name(save_prefix.name + \"_qq_log\"))\n",
    "\n",
    "        # 3) Residual vs Fitted\n",
    "        if HAVE_SM:\n",
    "            r_raw = m_raw.resid\n",
    "            f_raw = m_raw.fittedvalues\n",
    "            r_log = m_log.resid\n",
    "            f_log = m_log.fittedvalues\n",
    "        else:\n",
    "            r_raw = m_raw[\"resid\"]\n",
    "            f_raw = m_raw[\"fittedvalues\"]\n",
    "            r_log = m_log[\"resid\"]\n",
    "            f_log = m_log[\"fittedvalues\"]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax1.scatter(f_raw, r_raw, s=8)\n",
    "        ax1.axhline(0, color='gray', linestyle='--')\n",
    "        ax1.set_title(f\"{title_prefix} — Residual vs Fitted (Raw)\")\n",
    "        ax1.set_xlabel(\"Fitted\"); ax1.set_ylabel(\"Residuals\")\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "        ax2.scatter(f_log, r_log, s=8)\n",
    "        ax2.axhline(0, color='gray', linestyle='--')\n",
    "        ax2.set_title(f\"{title_prefix} — Residual vs Fitted (log1p)\")\n",
    "        ax2.set_xlabel(\"Fitted\"); ax2.set_ylabel(\"Residuals\")\n",
    "        fig.tight_layout()\n",
    "        _save_fig_both(fig, save_prefix.with_name(save_prefix.name + \"_rvf\"))\n",
    "        plt.close(fig)\n",
    "\n",
    "# Export summary CSV\n",
    "diag_df = pd.DataFrame(all_rows)\n",
    "summary_csv = DIAG_DIR / \"wmh_transform_diagnostics_summary.csv\"\n",
    "diag_df.to_csv(summary_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nSaved summary: {summary_csv}\")\n",
    "print(f\"Figures saved under: {FIG_DIR}  (each as .png and .pdf)\")\n",
    "\n",
    "# Console view with overall recommendation\n",
    "if not diag_df.empty:\n",
    "    view = diag_df[[\n",
    "        \"Dataset\", \"Outcome\",\n",
    "        \"Raw_prop_zeros\", \"Raw_skew\", \"Raw_kurt_excess\",\n",
    "        \"Log_skew\", \"Log_kurt_excess\",\n",
    "        \"BP_p_raw\", \"BP_p_log\", \"JB_p_raw\", \"JB_p_log\",\n",
    "        \"AIC_raw\", \"AIC_log\",\n",
    "        \"Improved_BP\", \"Improved_JB\", \"Improved_AIC\"\n",
    "    ]].copy()\n",
    "    view[\"Recommend_log1p\"] = (view[[\"Improved_BP\", \"Improved_JB\", \"Improved_AIC\"]].sum(axis=1) >= 2)\n",
    "    print(\"\\n=== Diagnostic summary (key fields) ===\")\n",
    "    print(view.to_string(index=False))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Word report (Methods + results + figure thumbnails)\n",
    "# =========================\n",
    "def word_style(doc: Document):\n",
    "    style = doc.styles[\"Normal\"]\n",
    "    style.font.name = \"Times New Roman\"\n",
    "    style._element.rPr.rFonts.set(qn(\"w:eastAsia\"), \"Times New Roman\")\n",
    "    style.font.size = Pt(11)\n",
    "\n",
    "def add_picture(doc: Document, path: Path, caption: str):\n",
    "    if path.exists():\n",
    "        doc.add_picture(str(path), width=Inches(5.8))\n",
    "        p = doc.add_paragraph(caption)\n",
    "        p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "def build_word_report(diag_df: pd.DataFrame):\n",
    "    doc = Document()\n",
    "    word_style(doc)\n",
    "    doc.add_heading(\"Supplementary eMethod: WMH Transformation Diagnostics\", level=1)\n",
    "\n",
    "    # Methods\n",
    "    doc.add_heading(\"Objective\", level=2)\n",
    "    doc.add_paragraph(\n",
    "        \"We assessed whether head-size–normalized white matter hyperintensity (WMH) volumes \"\n",
    "        \"should be modeled on the raw scale or on the log1p scale for regression analyses.\"\n",
    "    ).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    doc.add_heading(\"Datasets\", level=2)\n",
    "    doc.add_paragraph(\n",
    "        \"Two analytic cohorts were evaluated: (i) the primary matched cohort \"\n",
    "        \"(`primary_cohort.csv`), and (ii) the sensitivity matched cohort \"\n",
    "        \"(`sensitivity_cohort.csv`).\"\n",
    "    ).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    doc.add_heading(\"Outcomes\", level=2)\n",
    "    doc.add_paragraph(\n",
    "        \"For each cohort, we computed head-size–normalized outcomes as the raw WMH volume \"\n",
    "        \"multiplied by the volumetric scaling factor from T1 (registration to standard space). \"\n",
    "        \"Outcomes considered: total WMH (T1+T2 FLAIR), periventricular WMH, and deep WMH. \"\n",
    "        \"For each, a log1p-transformed variant was created.\"\n",
    "    ).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    doc.add_heading(\"Modeling and diagnostics\", level=2)\n",
    "    sm_note = (\"Exact BP/JB p-values and QQ plots were computed using statsmodels.\"\n",
    "               if HAVE_SM else\n",
    "               \"OLS and test statistics were computed via NumPy; p-values were approximated \"\n",
    "               \"using Wilson–Hilferty for χ² and QQ plots were generated without SciPy. \"\n",
    "               \"This approximation is suitable for diagnostic purposes.\")\n",
    "    doc.add_paragraph(\n",
    "        \"Ordinary least squares (OLS) models were fit using prespecified covariates \"\n",
    "        \"(age at MRI, sex, baseline BMI, Townsend deprivation index, genetic ethnic grouping, \"\n",
    "        \"smoking ever, and alcohol intake frequency). Categorical covariates were entered as \"\n",
    "        \"indicator variables. Missing data were handled by median imputation for continuous \"\n",
    "        \"covariates and mode imputation for categorical covariates. For each outcome, raw and \"\n",
    "        \"log1p models were compared using Breusch–Pagan (heteroscedasticity), Jarque–Bera \"\n",
    "        \"(normality of residuals), and AIC. \" + sm_note\n",
    "    ).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    doc.add_heading(\"Decision rule\", level=2)\n",
    "    doc.add_paragraph(\n",
    "        \"As a heuristic, we recommend the log1p transformation if at least two of the three \"\n",
    "        \"indicators improved (higher BP p-value, higher JB p-value, and lower AIC by ≥2).\"\n",
    "    ).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Results table\n",
    "    doc.add_heading(\"Summary results\", level=2)\n",
    "    if not diag_df.empty:\n",
    "        cols = [\n",
    "            \"Dataset\", \"Outcome\",\n",
    "            \"Raw_prop_zeros\", \"Raw_skew\", \"Raw_kurt_excess\",\n",
    "            \"Log_skew\", \"Log_kurt_excess\",\n",
    "            \"BP_p_raw\", \"BP_p_log\", \"JB_p_raw\", \"JB_p_log\",\n",
    "            \"AIC_raw\", \"AIC_log\", \"Improved_BP\", \"Improved_JB\", \"Improved_AIC\"\n",
    "        ]\n",
    "        tbl = doc.add_table(rows=1, cols=len(cols))\n",
    "        hdr = tbl.rows[0].cells\n",
    "        for j, c in enumerate(cols):\n",
    "            hdr[j].text = c\n",
    "        for _, r in diag_df[cols].iterrows():\n",
    "            row_cells = tbl.add_row().cells\n",
    "            for j, c in enumerate(cols):\n",
    "                row_cells[j].text = str(r[c])\n",
    "        doc.add_paragraph(\n",
    "            \"Note: 'Improved_*' flags indicate better fit on log1p vs raw.\"\n",
    "        ).alignment = WD_ALIGN_PARAGRAPH.JUSTIFY\n",
    "\n",
    "    # Figure thumbnails (embed PNG; PDFs are saved alongside)\n",
    "    doc.add_heading(\"Figure thumbnails\", level=2)\n",
    "    for label, _ in FILE_SETS.items():\n",
    "        for new_out, (_, pretty) in OUTCOME_DEFS.items():\n",
    "            base = FIG_DIR / f\"{label}_{new_out}\"\n",
    "            add_picture(doc, base.with_name(base.name + \"_hist.png\"),\n",
    "                        f\"{label} — {pretty}: Histograms (raw vs log1p)\")\n",
    "            add_picture(doc, base.with_name(base.name + \"_qq_raw.png\"),\n",
    "                        f\"{label} — {pretty}: QQ plot (raw residuals)\")\n",
    "            add_picture(doc, base.with_name(base.name + \"_qq_log.png\"),\n",
    "                        f\"{label} — {pretty}: QQ plot (log1p residuals)\")\n",
    "            add_picture(doc, base.with_name(base.name + \"_rvf.png\"),\n",
    "                        f\"{label} — {pretty}: Residual vs Fitted (raw vs log1p)\")\n",
    "\n",
    "    out_doc = DIAG_DIR / \"eMethod_WMH_Transform_Diagnostics.docx\"\n",
    "    doc.save(out_doc)\n",
    "    print(f\"Saved Word report: {out_doc}\")\n",
    "\n",
    "# Build Word report\n",
    "build_word_report(diag_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2b08e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Primary 1:10 (NoNeuro) (log1p main analysis) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3770354349.py:943: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Analysis             Outcome  % Change  % CI Lower  % CI Upper    Log β  Log CI Lower  Log CI Upper  p_value                                           SE_type  q_value sig_FDR(q≤0.05)  gatekeep_total_sig  secondary_interpretable\n",
      "Primary 1:10 (NoNeuro)           Total WMH 11.393078    4.803942   18.396481 0.107895      0.046921      0.168869 0.000524 cluster (groups=match_id, n_clusters=855, n=9231)      NaN             NaN                True                      NaN\n",
      "Primary 1:10 (NoNeuro) Periventricular WMH 10.713402    4.433031   17.371460 0.101775      0.043376      0.160174 0.000636 cluster (groups=match_id, n_clusters=855, n=9229) 0.001272            True                True                      1.0\n",
      "Primary 1:10 (NoNeuro)            Deep WMH 15.445615    5.499031   26.329976 0.143629      0.053532      0.233727 0.001781 cluster (groups=match_id, n_clusters=855, n=9229) 0.001781            True                True                      1.0\n",
      "Unadjusted results saved for Primary 1:10 (NoNeuro).\n",
      "Table 1 CSV saved: Main_Outcome\\Primary_1_10_NoNeuro_Table1_Baseline.csv\n",
      "Table 1 Word saved: Main_Outcome\\Primary_1_10_NoNeuro_Table1_Baseline.docx\n",
      "SMD table saved: Main_Outcome\\Primary_1_10_NoNeuro_SMD.csv\n",
      "\n",
      "=== Sensitivity 1:10 (WithNeuro) (log1p main analysis) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3770354349.py:943: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Analysis             Outcome  % Change  % CI Lower  % CI Upper    Log β  Log CI Lower  Log CI Upper  p_value                                           SE_type  q_value sig_FDR(q≤0.05)  gatekeep_total_sig  secondary_interpretable\n",
      "Sensitivity 1:10 (WithNeuro)           Total WMH 11.727645    5.160297   18.705129 0.110894      0.050316      0.171472 0.000333 cluster (groups=match_id, n_clusters=880, n=9680)      NaN             NaN                True                      NaN\n",
      "Sensitivity 1:10 (WithNeuro) Periventricular WMH 10.629155    4.417512   17.210320 0.101013      0.043227      0.158800 0.000612 cluster (groups=match_id, n_clusters=880, n=9678) 0.000612            True                True                      1.0\n",
      "Sensitivity 1:10 (WithNeuro)            Deep WMH 17.439751    7.298497   28.539501 0.160755      0.070444      0.251066 0.000485 cluster (groups=match_id, n_clusters=880, n=9678) 0.000612            True                True                      1.0\n",
      "Unadjusted results saved for Sensitivity 1:10 (WithNeuro).\n",
      "Table 1 CSV saved: Main_Outcome\\Sensitivity_1_10_WithNeuro_Table1_Baseline.csv\n",
      "Table 1 Word saved: Main_Outcome\\Sensitivity_1_10_WithNeuro_Table1_Baseline.docx\n",
      "SMD table saved: Main_Outcome\\Sensitivity_1_10_WithNeuro_SMD.csv\n",
      "Overlay figure saved (Primary vs Sensitivity).\n",
      "Supplementary SMD CSV saved: Main_Outcome\\Supplement_SMD_Balance.csv\n",
      "Supplementary SMD Word saved: Main_Outcome\\Supplement_SMD_Balance.docx\n",
      "Love plot (overlay) saved.\n",
      "Figure legends (TXT) saved: Main_Outcome\\Figure_legends.txt\n",
      "Figure legends (Word) saved: Main_Outcome\\Figure_legends.docx\n",
      "OLS results table CSV saved: Main_Outcome\\OLS_Results_Manuscript_Table.csv\n",
      "OLS results Word table saved: Main_Outcome\\OLS_Results_Manuscript_Table.docx\n",
      "OLS results table Word saved: Main_Outcome\\OLS_Results_Manuscript_Table.docx\n",
      "Unadjusted results table CSV saved: Main_Outcome\\UNADJ_Results_Manuscript_Table.csv\n",
      "Unadjusted results Word table saved: Main_Outcome\\UNADJ_Results_Manuscript_Table.docx\n",
      "Unadjusted results table Word saved: Main_Outcome\\UNADJ_Results_Manuscript_Table.docx\n",
      "Combined Table 1 Word (block format) saved.\n",
      "\n",
      "All outputs saved in \"Main_Outcome/\" (timestamp=20251111-123752).\n",
      "Tables exported as three-line tables in Word. Figures saved as PNG/PDF/SVG.\n",
      "\n",
      "=== Primary 1:10 (NoNeuro) (CMC-adjusted sensitivity analysis) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3770354349.py:1373: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df0 = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sensitivity 1:10 (WithNeuro) (CMC-adjusted sensitivity analysis) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3770354349.py:1373: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df0 = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlay eFigure (+CMC adjustment) saved.\n",
      "eTable (+CMC adjustment) CSV saved: Main_Outcome\\eSupp_CMC\\eTable_CMCadj_Results.csv\n",
      "OLS results Word table saved: Main_Outcome\\eSupp_CMC\\OLS_Results_Manuscript_Table.docx\n",
      "eTable (+CMC adjustment) Word saved: Main_Outcome\\eSupp_CMC\\eTable_CMCadj_Results.docx\n"
     ]
    }
   ],
   "source": [
    "# Main + Sensitiviy PSM Cohort WMH pipeline\n",
    "\"\"\"\n",
    "WMH pipeline\n",
    "- Head-size normalized log1p OLS\n",
    "- Forest plots (single + overlay)\n",
    "- Table 1 (p-values) CSV + Word (three-line table)\n",
    "- Supplement SMD tables + Love plot\n",
    "- Manuscript OLS table (Primary & Sensitivity together) CSV + Word (three-line)\n",
    "- Figure legends (TXT + Word)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from datetime import datetime\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from docx.shared import Pt\n",
    "\n",
    "# optional deps\n",
    "try:\n",
    "    import scipy.stats as st\n",
    "except Exception:\n",
    "    st = None\n",
    "\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docx.oxml import OxmlElement\n",
    "    from docx.oxml.ns import qn\n",
    "except Exception:\n",
    "    Document = None\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CLUSTER_VAR = \"match_id\"\n",
    "GROUP_BASELINE = [\"Control\", \"Study\"]\n",
    "\n",
    "MAIN_OUT_DIR = \"Main_Outcome\"\n",
    "os.makedirs(MAIN_OUT_DIR, exist_ok=True)\n",
    "\n",
    "file_sets = {\n",
    "    \"Primary 1:10 (NoNeuro)\":       \"primary_cohort.csv\",\n",
    "    \"Sensitivity 1:10 (WithNeuro)\": \"sensitivity_cohort.csv\"\n",
    "}\n",
    "\n",
    "SHORT = {\n",
    "    \"Log_HSNorm_Total_WMH_T1_T2\":     \"Total WMH\",\n",
    "    \"Log_HSNorm_PeriVentricular_WMH\": \"Periventricular WMH\",\n",
    "    \"Log_HSNorm_Deep_WMH\":            \"Deep WMH\",\n",
    "}\n",
    "\n",
    "# plot order (and table order)\n",
    "PLOT_ORDER = [\"Total WMH\", \"Periventricular WMH\", \"Deep WMH\"]\n",
    "\n",
    "# covariates\n",
    "BASE_ADJ = [\n",
    "    \"Sex\",\n",
    "    \"Age_at_Instance_2\",\n",
    "    \"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\n",
    "    \"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\n",
    "    \"Alcohol_intake_frequency_ordinal\",\n",
    "]\n",
    "CATEGORICAL = {\"Sex\", \"Smoking_Ever\", \"Genetic_ethnic_grouping\"}\n",
    "\n",
    "CATEGORICAL.update({\"CMC_score_cat\"})\n",
    "\n",
    "# threshold for gatekeeping\n",
    "PRIMARY_P_ALPHA = 0.05\n",
    "\n",
    "# Table 1 settings\n",
    "TABLE1_USE_PVALUE = True      # show p-values\n",
    "TABLE1_INCLUDE_SMD = False    # add an SMD column in Table 1 (usually False; SMD in supplement)\n",
    "\n",
    "# Word export appearance\n",
    "THREELINE_TABLES = True       # export Word tables as three-line tables\n",
    "RIGHT_ALIGN_NUMERIC = True    # right-align numeric columns\n",
    "\n",
    "# labels shown on figures (tables keep the full names)\n",
    "DISPLAY_LABEL = {\n",
    "    \"Total WMH\":            \"Total WMH\",\n",
    "    \"Periventricular WMH\":  \"PWMH\",\n",
    "    \"Deep WMH\":             \"DWMH\",\n",
    "}\n",
    "# global style\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 600,\n",
    "    \"font.size\": 12, \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11, \"ytick.labelsize\": 11,\n",
    "    \"figure.autolayout\": True\n",
    "})\n",
    "\n",
    "# --- High-quality TIFF export defaults ---\n",
    "SAVEFIG_DPI = 600\n",
    "PNG_KW  = {\"dpi\": SAVEFIG_DPI, \"bbox_inches\": \"tight\", \"facecolor\": \"white\"}\n",
    "PDF_KW  = {\"bbox_inches\": \"tight\"}\n",
    "SVG_KW  = {\"bbox_inches\": \"tight\"}\n",
    "TIFF_KW = {\n",
    "    \"dpi\": SAVEFIG_DPI,\n",
    "    \"bbox_inches\": \"tight\",\n",
    "    \"facecolor\": \"white\",\n",
    "    \"format\": \"tiff\",\n",
    "    # Pillow compression (will be ignored if backend not Pillow)\n",
    "    \"pil_kwargs\": {\"compression\": \"tiff_lzw\"}\n",
    "}\n",
    "# ---- PLOT STYLE TOGGLES ----\n",
    "SHOW_X_GRID = False            # TOGGLE: draw vertical dotted grid on x-axis (Neurology style: False)\n",
    "STRIP_TOP_RIGHT_SPINES = True  # TOGGLE: remove top/right panel spines for a cleaner look\n",
    "\n",
    "# ---------------- UTILITIES ----------------\n",
    "def build_formula(outcome, adjust_vars, categorical_vars, group_var=\"group\"):\n",
    "    terms = [f\"C({group_var})\"]\n",
    "    for v in adjust_vars:\n",
    "        if v == group_var:\n",
    "            continue\n",
    "        terms.append(f\"C({v})\" if v in categorical_vars else v)\n",
    "    return f\"{outcome} ~ \" + \" + \".join(terms)\n",
    "\n",
    "def find_group_term(params_index, group_var=\"group\", target_level=\"Study\"):\n",
    "    cands = [p for p in params_index if p.startswith(f\"C({group_var})[T.\")]\n",
    "    if not cands: return None\n",
    "    for p in cands:\n",
    "        if p.endswith(f\"[T.{target_level}]\"): return p\n",
    "    return cands[0]\n",
    "\n",
    "def fit_with_cluster(formula, data, cluster_var):\n",
    "    \"\"\"return: model, SE_info, y_used, X_used, n_clusters, n_obs\"\"\"\n",
    "    y, X = dmatrices(formula, data, return_type=\"dataframe\", NA_action=\"drop\")\n",
    "    n_obs = len(y)\n",
    "    base = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "    if cluster_var not in data.columns:\n",
    "        return base, \"HC3 (no cluster var)\", y, X, 0, n_obs\n",
    "    groups = data.loc[y.index, cluster_var]\n",
    "    if groups.notna().sum() == 0:\n",
    "        return base, \"HC3 (all cluster NaN)\", y, X, 0, n_obs\n",
    "    if groups.isna().any():\n",
    "        m = groups.notna(); y, X, groups = y.loc[m], X.loc[m], groups.loc[m]; n_obs = len(y)\n",
    "    n_clusters = int(groups.nunique())\n",
    "    if n_clusters > 1:\n",
    "        mod = sm.OLS(y, X).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups.to_numpy()})\n",
    "        used = f\"cluster (groups={cluster_var}, n_clusters={n_clusters}, n={n_obs})\"\n",
    "        return mod, used, y, X, n_clusters, n_obs\n",
    "    return base, \"HC3 (<=1 cluster)\", y, X, n_clusters, n_obs\n",
    "\n",
    "def count_group_n(df_like):\n",
    "    if \"Participant_ID\" in df_like.columns:\n",
    "        n_c = int(df_like[df_like[\"group\"]==\"Control\"][\"Participant_ID\"].nunique())\n",
    "        n_s = int(df_like[df_like[\"group\"]==\"Study\"][\"Participant_ID\"].nunique())\n",
    "    else:\n",
    "        n_c = int((df_like[\"group\"]==\"Control\").sum())\n",
    "        n_s = int((df_like[\"group\"]==\"Study\").sum())\n",
    "    return n_c, n_s, n_c + n_s\n",
    "\n",
    "# ---------------- PLOTTING ----------------\n",
    "def plot_single_forest(res_log: pd.DataFrame, label: str, outdir: str):\n",
    "    \"\"\"Horizontal forest plot, compact layout, legend omitted (single cohort).\"\"\"\n",
    "    if res_log is None or res_log.empty:\n",
    "        return\n",
    "\n",
    "    # keep your order logic\n",
    "    order_map = {v: i for i, v in enumerate(PLOT_ORDER)}\n",
    "    dfp = res_log.sort_values(by=\"Outcome\", key=lambda s: s.map(order_map)).copy()\n",
    "\n",
    "    # --- NEW: ensure numeric + valid CI and pre-compute xlim that includes 0 ---\n",
    "    for col in [\"% Change\", \"% CI Lower\", \"% CI Upper\"]:\n",
    "        dfp[col] = pd.to_numeric(dfp[col], errors=\"coerce\")\n",
    "\n",
    "    # fix accidental swapped CI (rare but defensive)\n",
    "    bad = dfp[\"% CI Lower\"] > dfp[\"% CI Upper\"]\n",
    "    if bad.any():\n",
    "        lo = dfp.loc[bad, \"% CI Upper\"].values\n",
    "        hi = dfp.loc[bad, \"% CI Lower\"].values\n",
    "        dfp.loc[bad, \"% CI Lower\"] = lo\n",
    "        dfp.loc[bad, \"% CI Upper\"] = hi\n",
    "\n",
    "    # compute axis limits from CI and FORCE include 0\n",
    "    xmin = float(np.nanmin(dfp[\"% CI Lower\"]))\n",
    "    xmax = float(np.nanmax(dfp[\"% CI Upper\"]))\n",
    "    xmin = min(xmin, 0.0)\n",
    "    xmax = max(xmax, 0.0)\n",
    "    # add 8% padding; also guard degenerate range\n",
    "    span = xmax - xmin\n",
    "    if not np.isfinite(span) or span <= 0:\n",
    "        span = 1.0\n",
    "    pad = 0.08 * span\n",
    "\n",
    "    fig = plt.figure(figsize=(6.8, 3.8))\n",
    "    ax = plt.gca()\n",
    "    y = np.arange(len(dfp))[::-1]\n",
    "    pad_y = 0.45\n",
    "\n",
    "    # build symmetric xerr safely (non-negative widths; NaN -> 0)\n",
    "    lo = (dfp[\"% Change\"] - dfp[\"% CI Lower\"]).astype(float).to_numpy()\n",
    "    hi = (dfp[\"% CI Upper\"] - dfp[\"% Change\"]).astype(float).to_numpy()\n",
    "    lo = np.where(np.isfinite(lo) & (lo >= 0), lo, 0.0)\n",
    "    hi = np.where(np.isfinite(hi) & (hi >= 0), hi, 0.0)\n",
    "\n",
    "    ax.errorbar(\n",
    "        dfp[\"% Change\"], y,\n",
    "        xerr=[lo, hi],\n",
    "        fmt='o', ms=5.5, mfc=\"#1f3b4d\", mec=\"#1f3b4d\",\n",
    "        ecolor=\"#1f3b4d\", elinewidth=1.6, capsize=4,\n",
    "        linestyle=\"none\", color=\"#1f3b4d\"\n",
    "    )\n",
    "\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=1)  # zero reference line\n",
    "    ax.set_yticks(y, [DISPLAY_LABEL.get(v, v) for v in dfp[\"Outcome\"]])\n",
    "    ax.set_ylim(y.min()-pad_y, y.max()+pad_y)\n",
    "    ax.set_xlabel(\"% change (Study - Control)\")\n",
    "\n",
    "    # --- NEW: apply the xlim after plotting so it always shows 0 ---\n",
    "    ax.set_xlim(xmin - pad, xmax + pad)\n",
    "\n",
    "    if SHOW_X_GRID:\n",
    "        ax.grid(axis='x', linestyle=':', linewidth=0.7, alpha=0.8)\n",
    "    else:\n",
    "        ax.grid(False)\n",
    "    if STRIP_TOP_RIGHT_SPINES:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", label).strip(\"_\")\n",
    "    base = os.path.join(outdir, f\"{safe}_LOG1P_pct_CI\")\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".svg\", **SVG_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_overlay_forest(res_primary: pd.DataFrame,\n",
    "                        res_sensitivity: pd.DataFrame,\n",
    "                        outdir: str,\n",
    "                        basename: str):\n",
    "    \"\"\"Overlay: Primary vs Sensitivity. Legend outside (right-centered), no box.\"\"\"\n",
    "    if res_primary is None or res_primary.empty or res_sensitivity is None or res_sensitivity.empty:\n",
    "        return\n",
    "\n",
    "    order_map = {v: i for i, v in enumerate(PLOT_ORDER)}\n",
    "    L = res_primary.sort_values(\"Outcome\", key=lambda s: s.map(order_map)).reset_index(drop=True).copy()\n",
    "    R = res_sensitivity.sort_values(\"Outcome\", key=lambda s: s.map(order_map)).reset_index(drop=True).copy()\n",
    "\n",
    "    if not L[\"Outcome\"].equals(R[\"Outcome\"]):\n",
    "        common = pd.Index(L[\"Outcome\"]).intersection(R[\"Outcome\"])\n",
    "        L = L[L[\"Outcome\"].isin(common)].sort_values(\"Outcome\", key=lambda s: s.map(order_map))\n",
    "        R = R[R[\"Outcome\"].isin(common)].sort_values(\"Outcome\", key=lambda s: s.map(order_map))\n",
    "\n",
    "    # --- NEW: force include 0 in combined xlim ---\n",
    "    for df in (L, R):\n",
    "        for col in [\"% Change\", \"% CI Lower\", \"% CI Upper\"]:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    xmin = float(np.nanmin([L[\"% CI Lower\"].min(), R[\"% CI Lower\"].min()]))\n",
    "    xmax = float(np.nanmax([L[\"% CI Upper\"].max(), R[\"% CI Upper\"].max()]))\n",
    "    xmin = min(xmin, 0.0)\n",
    "    xmax = max(xmax, 0.0)\n",
    "    span = xmax - xmin\n",
    "    if not np.isfinite(span) or span <= 0:\n",
    "        span = 1.0\n",
    "    pad_x = 0.07 * span\n",
    "    xmin -= pad_x\n",
    "    xmax += pad_x\n",
    "\n",
    "    c_primary, c_sens = \"#1f3b4d\", \"#6e6e6e\"\n",
    "    m_primary, m_sens = \"o\", \"s\"\n",
    "    lw, cap, jitter, pad_y = 1.6, 4, 0.12, 0.45\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.9, 3.8))\n",
    "    y = np.arange(len(L))[::-1]\n",
    "\n",
    "    # build xerr safely\n",
    "    L_lo = (L[\"% Change\"] - L[\"% CI Lower\"]).astype(float).to_numpy()\n",
    "    L_hi = (L[\"% CI Upper\"] - L[\"% Change\"]).astype(float).to_numpy()\n",
    "    L_lo = np.where(np.isfinite(L_lo) & (L_lo >= 0), L_lo, 0.0)\n",
    "    L_hi = np.where(np.isfinite(L_hi) & (L_hi >= 0), L_hi, 0.0)\n",
    "\n",
    "    R_lo = (R[\"% Change\"] - R[\"% CI Lower\"]).astype(float).to_numpy()\n",
    "    R_hi = (R[\"% CI Upper\"] - R[\"% Change\"]).astype(float).to_numpy()\n",
    "    R_lo = np.where(np.isfinite(R_lo) & (R_lo >= 0), R_lo, 0.0)\n",
    "    R_hi = np.where(np.isfinite(R_hi) & (R_hi >= 0), R_hi, 0.0)\n",
    "\n",
    "    # Primary\n",
    "    ax.errorbar(\n",
    "        L[\"% Change\"], y + jitter,\n",
    "        xerr=[L_lo, L_hi],\n",
    "        fmt=m_primary, ms=5.5, mfc=c_primary, mec=c_primary,\n",
    "        ecolor=c_primary, elinewidth=lw, capsize=cap,\n",
    "        linestyle=\"none\", label=\"Primary PSM cohort\"\n",
    "    )\n",
    "    # Sensitivity\n",
    "    ax.errorbar(\n",
    "        R[\"% Change\"], y - jitter,\n",
    "        xerr=[R_lo, R_hi],\n",
    "        fmt=m_sens, ms=5.2, mfc=\"white\", mec=c_sens,\n",
    "        ecolor=c_sens, elinewidth=lw, capsize=cap,\n",
    "        linestyle=\"none\", label=\"PSM-sensitivity cohort\"\n",
    "    )\n",
    "\n",
    "    ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_yticks(y, [DISPLAY_LABEL.get(v, v) for v in L[\"Outcome\"]])\n",
    "    ax.set_ylim(y.min() - pad_y, y.max() + pad_y)\n",
    "    ax.set_xlabel(\"% change (SA - Control)\")\n",
    "    ax.set_xlim(xmin, xmax)  # <-- ensure 0 is visible\n",
    "\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax.legend(\n",
    "        frameon=False, loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        handlelength=1.8, fontsize=9,\n",
    "        labelspacing=0.6,\n",
    "        markerscale=0.9\n",
    "    )\n",
    "\n",
    "    plt.subplots_adjust(right=0.78)\n",
    "\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", basename).strip(\"_\")\n",
    "    base = os.path.join(outdir, safe)\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".svg\", **SVG_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------- Table 1 helpers ----------------\n",
    "def _as_num(s): return pd.to_numeric(s, errors=\"coerce\")\n",
    "def _format_mean_sd(x): x=_as_num(x); return f\"{x.mean():.2f} ({x.std(ddof=1):.2f})\"\n",
    "def _format_median_iqr(x):\n",
    "    x=_as_num(x); q1,q3=x.quantile([0.25,0.75]); return f\"{x.median():.2f} [{q1:.2f}, {q3:.2f}]\"\n",
    "def _is_true_series(s, true=(\"yes\",\"true\",\"ever\",\"y\",\"1\")):\n",
    "    if pd.api.types.is_numeric_dtype(s): return s.astype(float)==1.0\n",
    "    ss=s.astype(str).str.strip().str.lower(); return ss.isin(true)\n",
    "def _is_female_series(s):\n",
    "    if pd.api.types.is_numeric_dtype(s): return s.astype(float)==0.0\n",
    "    ss=s.astype(str).str.strip().str.lower(); return ss.str.startswith(\"f\")|(ss==\"female\")\n",
    "def _is_white_series(s):\n",
    "    if pd.api.types.is_numeric_dtype(s): return s.astype(float)==1.0\n",
    "    ss=s.astype(str).str.strip().str.lower()\n",
    "    return ss.str.contains(\"white\")|ss.str.contains(\"british\")|ss.str.contains(\"cauc\")\n",
    "def _pct(b): b=b.fillna(False); return 100.0*b.mean()\n",
    "def _smd_cont(x1,x0):\n",
    "    x1=_as_num(x1); x0=_as_num(x0); m1,m0=x1.mean(),x0.mean(); s1,s0=x1.std(ddof=1),x0.std(ddof=1)\n",
    "    sp=np.sqrt((s1**2+s0**2)/2.0); return (m1-m0)/sp if sp>0 else np.nan\n",
    "def _smd_binary(b1,b0):\n",
    "    p1,p0=b1.mean(),b0.mean(); p=(p1+p0)/2.0; denom=np.sqrt(p*(1-p)); return (p1-p0)/denom if denom>0 else np.nan\n",
    "def _p_ttest_welch(x1,x0):\n",
    "    if st is None: return np.nan\n",
    "    x1=_as_num(x1).dropna(); x0=_as_num(x0).dropna()\n",
    "    if len(x1)<2 or len(x0)<2: return np.nan\n",
    "    return float(st.ttest_ind(x1,x0,equal_var=False).pvalue)\n",
    "def _p_mannwhitney(x1,x0):\n",
    "    if st is None: return np.nan\n",
    "    x1=_as_num(x1).dropna(); x0=_as_num(x0).dropna()\n",
    "    if len(x1)<1 or len(x0)<1: return np.nan\n",
    "    try: return float(st.mannwhitneyu(x1,x0,alternative=\"two-sided\").pvalue)\n",
    "    except Exception: return np.nan\n",
    "def _p_cat_2x2(b1,b0):\n",
    "    if st is None: return np.nan\n",
    "    a=int(b1.sum()); b=int((~b1).sum()); c=int(b0.sum()); d=int((~b0).sum())\n",
    "    table=np.array([[a,b],[c,d]],dtype=float)\n",
    "    try:\n",
    "        chi2,p_chi,_,exp = st.chi2_contingency(table, correction=False)\n",
    "        if (exp<5).any():\n",
    "            _,p = st.fisher_exact(table, alternative=\"two-sided\"); return float(p)\n",
    "        return float(p_chi)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def make_table1(dmod: pd.DataFrame, analysis_label: str, id_col=\"Participant_ID\") -> pd.DataFrame:\n",
    "    dfb = dmod.copy()\n",
    "    g1 = dfb[\"group\"] == \"Study\"\n",
    "    g0 = dfb[\"group\"] == \"Control\"\n",
    "\n",
    "    if id_col in dfb.columns:\n",
    "        n1 = dfb.loc[g1, id_col].nunique()\n",
    "        n0 = dfb.loc[g0, id_col].nunique()\n",
    "    else:\n",
    "        n1 = int(g1.sum())\n",
    "        n0 = int(g0.sum())\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    def add(name, s_val, c_val, p=None, smd=None):\n",
    "        row = {\"Characteristic\": name, \"Study\": s_val, \"Control\": c_val}\n",
    "        if TABLE1_USE_PVALUE:\n",
    "            row[\"p_value\"] = p\n",
    "        if TABLE1_INCLUDE_SMD:\n",
    "            row[\"SMD\"] = smd\n",
    "        rows.append(row)\n",
    "\n",
    "    # ---- Formatters (1 decimal place throughout) ----\n",
    "    def _as_num(s): \n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    def _format_mean_sd(x):\n",
    "        x = _as_num(x)\n",
    "        return f\"{x.mean():.1f} ({x.std(ddof=1):.1f})\"\n",
    "\n",
    "    def _format_median_iqr(x):\n",
    "        x = _as_num(x)\n",
    "        q1, q3 = x.quantile([0.25, 0.75])\n",
    "        return f\"{x.median():.1f} [{q1:.1f}, {q3:.1f}]\"\n",
    "\n",
    "    def _pct(b):\n",
    "        b = b.fillna(False)\n",
    "        return 100.0 * b.mean()\n",
    "\n",
    "    # ---- Variables ----\n",
    "    if \"Age_at_Instance_2\" in dfb.columns:\n",
    "        s = dfb.loc[g1, \"Age_at_Instance_2\"]\n",
    "        c = dfb.loc[g0, \"Age_at_Instance_2\"]\n",
    "        add(\"Age at Instance 2, mean (SD), y\",\n",
    "            _format_mean_sd(s), _format_mean_sd(c),\n",
    "            p=_p_ttest_welch(s, c),\n",
    "            smd=round(_smd_cont(s, c), 3))\n",
    "\n",
    "    if \"Sex\" in dfb.columns:\n",
    "        sF = _is_female_series(dfb.loc[g1, \"Sex\"])\n",
    "        cF = _is_female_series(dfb.loc[g0, \"Sex\"])\n",
    "        add(\"Female sex, %\",\n",
    "            f\"{_pct(sF):.1f}\", f\"{_pct(cF):.1f}\",\n",
    "            p=_p_cat_2x2(sF.astype(bool), cF.astype(bool)),\n",
    "            smd=round(_smd_binary(sF.astype(float), cF.astype(float)), 3))\n",
    "\n",
    "    if \"Body_mass_index_BMI_Instance_0\" in dfb.columns:\n",
    "        s = dfb.loc[g1, \"Body_mass_index_BMI_Instance_0\"]\n",
    "        c = dfb.loc[g0, \"Body_mass_index_BMI_Instance_0\"]\n",
    "        add(\"BMI at Instance 0, mean (SD), kg/m²\",\n",
    "            _format_mean_sd(s), _format_mean_sd(c),\n",
    "            p=_p_ttest_welch(s, c),\n",
    "            smd=round(_smd_cont(s, c), 3))\n",
    "\n",
    "    if \"Genetic_ethnic_grouping\" in dfb.columns:\n",
    "        sW = _is_white_series(dfb.loc[g1, \"Genetic_ethnic_grouping\"])\n",
    "        cW = _is_white_series(dfb.loc[g0, \"Genetic_ethnic_grouping\"])\n",
    "        add(\"Genetic ethnic grouping, White %\",\n",
    "            f\"{_pct(sW):.1f}\", f\"{_pct(cW):.1f}\",\n",
    "            p=_p_cat_2x2(sW.astype(bool), cW.astype(bool)),\n",
    "            smd=round(_smd_binary(sW.astype(float), cW.astype(float)), 3))\n",
    "\n",
    "    if \"Townsend_deprivation_index_at_recruitment\" in dfb.columns:\n",
    "        s = dfb.loc[g1, \"Townsend_deprivation_index_at_recruitment\"]\n",
    "        c = dfb.loc[g0, \"Townsend_deprivation_index_at_recruitment\"]\n",
    "        add(\"Townsend deprivation index, mean (SD)\",\n",
    "            _format_mean_sd(s), _format_mean_sd(c),\n",
    "            p=_p_ttest_welch(s, c),\n",
    "            smd=round(_smd_cont(s, c), 3))\n",
    "\n",
    "    if \"Smoking_Ever\" in dfb.columns:\n",
    "        sE = _is_true_series(dfb.loc[g1, \"Smoking_Ever\"])\n",
    "        cE = _is_true_series(dfb.loc[g0, \"Smoking_Ever\"])\n",
    "        add(\"Smoking, ever, %\",\n",
    "            f\"{_pct(sE):.1f}\", f\"{_pct(cE):.1f}\",\n",
    "            p=_p_cat_2x2(sE.astype(bool), cE.astype(bool)),\n",
    "            smd=round(_smd_binary(sE.astype(float), cE.astype(float)), 3))\n",
    "\n",
    "    if \"Alcohol_intake_frequency_ordinal\" in dfb.columns:\n",
    "        s = dfb.loc[g1, \"Alcohol_intake_frequency_ordinal\"]\n",
    "        c = dfb.loc[g0, \"Alcohol_intake_frequency_ordinal\"]\n",
    "        add(\"Alcohol intake frequency, median [IQR]\",\n",
    "            _format_median_iqr(s), _format_median_iqr(c),\n",
    "            p=_p_mannwhitney(s, c),\n",
    "            smd=round(_smd_cont(_as_num(s), _as_num(c)), 3))\n",
    "        \n",
    "\n",
    "    # ---- CMC (comorbidity count): show only ≥1, n (%) WITHOUT p-value/SMD ----\n",
    "    _cmc_candidates = [\"CMC_score_raw\", \"CMC_score_cat\", \"CMC\", \"Comorbidity_count\"]\n",
    "    _cmc_found = [c for c in _cmc_candidates if c in dfb.columns]\n",
    "\n",
    "    if _cmc_found:\n",
    "        cmc_col = _cmc_found[0]\n",
    "        cmc_s = pd.to_numeric(dfb.loc[g1, cmc_col], errors=\"coerce\")\n",
    "        cmc_c = pd.to_numeric(dfb.loc[g0, cmc_col], errors=\"coerce\")\n",
    "\n",
    "        car_s = cmc_s >= 1\n",
    "        car_c = cmc_c >= 1\n",
    "\n",
    "        n_s = int(car_s.sum(skipna=True))\n",
    "        n_c = int(car_c.sum(skipna=True))\n",
    "\n",
    "        den_s = int(cmc_s.notna().sum())\n",
    "        den_c = int(cmc_c.notna().sum())\n",
    "\n",
    "        pct_s = (100.0 * n_s / den_s) if den_s > 0 else float(\"nan\")\n",
    "        pct_c = (100.0 * n_c / den_c) if den_c > 0 else float(\"nan\")\n",
    "\n",
    "        add(\"CMC ≥1, n (%)\",\n",
    "            f\"{n_s} ({pct_s:.1f}%)\",\n",
    "            f\"{n_c} ({pct_c:.1f}%)\",\n",
    "            p=None)\n",
    "\n",
    "    # ---- APOE ε4: show only carrier (≥1), n (%) WITHOUT p-value/SMD ----\n",
    "    _apoe_count_candidates = [\n",
    "        \"APOE_e4_count\", \"APOE_e4count\", \"e4_count\", \"APOE_e4\",\n",
    "        \"APOE_e4_cnt\", \"APOE_e4_num\"\n",
    "    ]\n",
    "    _apoe_carrier_candidates = [\"APOE_e4_carrier\", \"APOE_e4_any\", \"APOE_e4_has\"]\n",
    "\n",
    "    _ap_cnt = [c for c in _apoe_count_candidates if c in dfb.columns]\n",
    "    _ap_car = [c for c in _apoe_carrier_candidates if c in dfb.columns]\n",
    "\n",
    "    if _ap_cnt or _ap_car:\n",
    "        if _ap_cnt:\n",
    "            ap_col = _ap_cnt[0]\n",
    "            ap_s = pd.to_numeric(dfb.loc[g1, ap_col], errors=\"coerce\")\n",
    "            ap_c = pd.to_numeric(dfb.loc[g0, ap_col], errors=\"coerce\")\n",
    "            car_s = ap_s >= 1\n",
    "            car_c = ap_c >= 1\n",
    "            den_s = int(ap_s.notna().sum())\n",
    "            den_c = int(ap_c.notna().sum())\n",
    "        else:\n",
    "            ap_col = _ap_car[0]\n",
    "            # accept 1/0, True/False, or strings \"1\"/\"0\"\n",
    "            ap_s_raw = dfb.loc[g1, ap_col]\n",
    "            ap_c_raw = dfb.loc[g0, ap_col]\n",
    "            # normalize to boolean with NaN respected\n",
    "            def _to_bool(s):\n",
    "                if pd.api.types.is_bool_dtype(s):\n",
    "                    return s\n",
    "                s2 = pd.to_numeric(s, errors=\"coerce\")\n",
    "                if s2.notna().any():\n",
    "                    return s2 == 1\n",
    "                ss = s.astype(str).str.strip().str.lower()\n",
    "                return ss.isin([\"1\", \"true\", \"yes\", \"y\"])\n",
    "            car_s = _to_bool(ap_s_raw)\n",
    "            car_c = _to_bool(ap_c_raw)\n",
    "            den_s = int(pd.notna(ap_s_raw).sum())\n",
    "            den_c = int(pd.notna(ap_c_raw).sum())\n",
    "\n",
    "        n_s = int(pd.Series(car_s).sum(skipna=True))\n",
    "        n_c = int(pd.Series(car_c).sum(skipna=True))\n",
    "\n",
    "        pct_s = (100.0 * n_s / den_s) if den_s > 0 else float(\"nan\")\n",
    "        pct_c = (100.0 * n_c / den_c) if den_c > 0 else float(\"nan\")\n",
    "\n",
    "        add(\"APOE ε4 carrier (≥1), n (%)\",\n",
    "            f\"{n_s} ({pct_s:.1f}%)\",\n",
    "            f\"{n_c} ({pct_c:.1f}%)\",\n",
    "            p=None)\n",
    "            # ---- Has Degree: show only Yes n (%) WITHOUT p-value/SMD ----\n",
    "    _degree_candidates = [\n",
    "        \"Has_Degree\", \"has_degree\", \"degree\", \"Qualifications_has_degree\"\n",
    "    ]\n",
    "    _degree_found = [c for c in _degree_candidates if c in dfb.columns]\n",
    "\n",
    "    if _degree_found:\n",
    "        deg_col = _degree_found[0]\n",
    "        deg_s = dfb.loc[g1, deg_col]\n",
    "        deg_c = dfb.loc[g0, deg_col]\n",
    "\n",
    "        # Normalize to boolean (1/0, True/False, string)\n",
    "        def _to_bool(s):\n",
    "            if pd.api.types.is_bool_dtype(s):\n",
    "                return s\n",
    "            s2 = pd.to_numeric(s, errors=\"coerce\")\n",
    "            if s2.notna().any():\n",
    "                return s2 == 1\n",
    "            ss = s.astype(str).str.strip().str.lower()\n",
    "            return ss.isin([\"1\", \"true\", \"yes\", \"y\"])\n",
    "\n",
    "        car_s = _to_bool(deg_s)\n",
    "        car_c = _to_bool(deg_c)\n",
    "\n",
    "        n_s = int(pd.Series(car_s).sum(skipna=True))\n",
    "        n_c = int(pd.Series(car_c).sum(skipna=True))\n",
    "        den_s = int(pd.notna(deg_s).sum())\n",
    "        den_c = int(pd.notna(deg_c).sum())\n",
    "\n",
    "        pct_s = (100.0 * n_s / den_s) if den_s > 0 else float(\"nan\")\n",
    "        pct_c = (100.0 * n_c / den_c) if den_c > 0 else float(\"nan\")\n",
    "\n",
    "        add(\"Has degree, n (%)\",\n",
    "            f\"{n_s} ({pct_s:.1f}%)\",\n",
    "            f\"{n_c} ({pct_c:.1f}%)\",\n",
    "            p=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Build Table ----\n",
    "    tbl = pd.DataFrame(rows)\n",
    "    top = {\"Characteristic\": \"Sample size (unique participants), n\",\n",
    "           \"Study\": n1, \"Control\": n0}\n",
    "    if TABLE1_USE_PVALUE:\n",
    "        top[\"p_value\"] = np.nan\n",
    "    if TABLE1_INCLUDE_SMD:\n",
    "        top[\"SMD\"] = np.nan\n",
    "    tbl = pd.concat([pd.DataFrame([top]), tbl], ignore_index=True)\n",
    "    tbl.insert(0, \"Analysis\", analysis_label)\n",
    "\n",
    "    return tbl\n",
    "\n",
    "\n",
    "# ---------------- WORD helpers (three-line tables) ----------------\n",
    "def _set_cell_border(cell, **kwargs):\n",
    "    \"\"\"\n",
    "    Set cell borders: _set_cell_border(cell, top={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"}, ...)\n",
    "    \"\"\"\n",
    "    tc = cell._tc\n",
    "    tcPr = tc.get_or_add_tcPr()\n",
    "    tcBorders = tcPr.find(qn('w:tcBorders'))\n",
    "    if tcBorders is None:\n",
    "        tcBorders = OxmlElement('w:tcBorders')\n",
    "        tcPr.append(tcBorders)\n",
    "    for edge in ('left', 'right', 'top', 'bottom', 'insideH', 'insideV'):\n",
    "        if edge in kwargs:\n",
    "            edge_data = kwargs.get(edge)\n",
    "            tag = OxmlElement(f'w:{edge}')\n",
    "            for key in [\"val\",\"sz\",\"color\",\"space\"]:\n",
    "                if key in edge_data:\n",
    "                    tag.set(qn(f'w:{key}'), str(edge_data[key]))\n",
    "            tcBorders.append(tag)\n",
    "\n",
    "def apply_three_line_table(table, header_row_idx=0):\n",
    "    \"\"\"\n",
    "    Convert a Word table to classic 'three-line' format:\n",
    "    - top border on the header row (table top)\n",
    "    - bottom border on the header row (header underline)\n",
    "    - bottom border on the last row (table bottom)\n",
    "    - remove vertical borders\n",
    "    \"\"\"\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            _set_cell_border(cell, left={\"val\":\"nil\"}, right={\"val\":\"nil\"}, top={\"val\":\"nil\"}, bottom={\"val\":\"nil\"})\n",
    "\n",
    "    for cell in table.rows[header_row_idx].cells:\n",
    "        _set_cell_border(cell, top={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "\n",
    "    for cell in table.rows[header_row_idx].cells:\n",
    "        _set_cell_border(cell, bottom={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "\n",
    "    for cell in table.rows[-1].cells:\n",
    "        _set_cell_border(cell, bottom={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "\n",
    "def set_table_font(table, font_name=\"Times New Roman\", font_size=10):\n",
    "    \"\"\"Apply uniform font and size to all table content (Neurology style).\"\"\"\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            for paragraph in cell.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    run.font.name = font_name\n",
    "                    run._element.rPr.rFonts.set(qn('w:eastAsia'), font_name)\n",
    "                    run.font.size = Pt(font_size)\n",
    "\n",
    "def _right_align_numeric(table, numeric_col_idx):\n",
    "    if not RIGHT_ALIGN_NUMERIC: return\n",
    "    for row in table.rows[1:]:\n",
    "        for j in numeric_col_idx:\n",
    "            if j < len(row.cells):\n",
    "                for p in row.cells[j].paragraphs:\n",
    "                    p.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "\n",
    "# ---------------- Word exporters ----------------\n",
    "def export_table1_word(table_df: pd.DataFrame, analysis_label: str, outdir: str, timestamp: str):\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: Word export skipped.\"); return None\n",
    "    doc = Document()\n",
    "    title = doc.add_paragraph()\n",
    "    r = title.add_run(f\"Table 1. Baseline characteristics — {analysis_label}\")\n",
    "    r.bold = True; title.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    cols = [\"Characteristic\",\"Study\",\"Control\"]\n",
    "    if TABLE1_USE_PVALUE: cols.append(\"p_value\")\n",
    "    if TABLE1_INCLUDE_SMD: cols.append(\"SMD\")\n",
    "\n",
    "    t = doc.add_table(rows=1, cols=len(cols))\n",
    "    hdr = t.rows[0].cells\n",
    "    for i,c in enumerate(cols): hdr[i].text = c\n",
    "\n",
    "    for _, row in table_df[[\"Characteristic\",\"Study\",\"Control\"] + ([\"p_value\"] if TABLE1_USE_PVALUE else []) + ([\"SMD\"] if TABLE1_INCLUDE_SMD else [])].iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        for j,c in enumerate(cols):\n",
    "            val = row[c] if c in row else \"\"\n",
    "            if c in (\"p_value\",\"SMD\") and isinstance(val,float) and not np.isnan(val):\n",
    "                cells[j].text = f\"{val:.3g}\"\n",
    "            else:\n",
    "                cells[j].text = \"\" if pd.isna(val) else str(val)\n",
    "\n",
    "    num_idx = [1,2] + ([3] if TABLE1_USE_PVALUE else []) + ([4] if TABLE1_INCLUDE_SMD else [])\n",
    "    _right_align_numeric(t, num_idx)\n",
    "\n",
    "    if THREELINE_TABLES:\n",
    "        apply_three_line_table(t, header_row_idx=0)\n",
    "\n",
    "    legend = doc.add_paragraph()\n",
    "    legend.add_run(\n",
    "        \"Legend: Values are mean (SD), median [IQR], or n (%). \"\n",
    "        \"p-values from Welch’s t test (continuous), Mann–Whitney U for the ordinal alcohol-frequency variable, \"\n",
    "        \"and χ² or Fisher’s exact test for categorical variables, as appropriate. \"\n",
    "        \"Groups are Study (sleep apnea) and Control at Instance 2.\"\n",
    "    ).italic = True\n",
    "\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\",\"_\", analysis_label).strip(\"_\")\n",
    "    path = os.path.join(outdir, f\"{safe}_Table1_Baseline.docx\")\n",
    "    doc.save(path)\n",
    "    return path\n",
    "\n",
    "def export_smd_word(df_combined: pd.DataFrame, outdir: str, timestamp: str):\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: SMD Word export skipped.\"); return None\n",
    "    doc = Document()\n",
    "    title = doc.add_paragraph()\n",
    "    r = title.add_run(\"Supplementary Table S1. Covariate balance after matching (SMD)\")\n",
    "    r.bold = True; title.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    cols = [\"Characteristic\",\"SMD (Primary)\",\"|SMD| (Primary)\",\"SMD (Sensitivity)\",\"|SMD| (Sensitivity)\"]\n",
    "    t = doc.add_table(rows=1, cols=len(cols)); hdr = t.rows[0].cells\n",
    "    for i,c in enumerate(cols): hdr[i].text = c\n",
    "\n",
    "    for _, row in df_combined.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        for j,c in enumerate(cols):\n",
    "            val = row.get(c, \"\")\n",
    "            cells[j].text = f\"{val:.3f}\" if (isinstance(val,float) and not np.isnan(val)) else (\"\" if pd.isna(val) else str(val))\n",
    "\n",
    "    _right_align_numeric(t, [1,2,3,4])\n",
    "    if THREELINE_TABLES:\n",
    "        apply_three_line_table(t, header_row_idx=0)\n",
    "\n",
    "    foot = doc.add_paragraph()\n",
    "    foot.add_run(\n",
    "        \"Legend: Standardized mean differences (SMD) summarize post-matching covariate balance. \"\n",
    "        \"Values <0.10 indicate acceptable balance. Continuous SMDs use pooled SD; \"\n",
    "        \"binary SMDs use Cohen’s h. Alcohol intake frequency is treated as an ordinal continuous variable.\"\n",
    "    ).italic = True\n",
    "\n",
    "    path = os.path.join(outdir, f\"Supplement_SMD_Balance.docx\")\n",
    "    doc.save(path)\n",
    "    return path\n",
    "\n",
    "# -------- OLS results table (manuscript, adjusted) --------\n",
    "def _fmt_pct_ci(pct, lo, hi):\n",
    "    if any(pd.isna(x) for x in [pct, lo, hi]): return \"—\"\n",
    "    return f\"{pct:.1f}% ({lo:.1f}, {hi:.1f})\"\n",
    "def _fmt_p(p):  return \"—\" if pd.isna(p) else (\"<0.001\" if p < 1e-3 else f\"{p:.3f}\")\n",
    "def _fmt_q(q):  return \"—\" if pd.isna(q) else (\"<0.001\" if q < 1e-3 else f\"{q:.3f}\")\n",
    "def _fmt_logci(beta, lo, hi):\n",
    "    \"\"\"Format raw log1p coefficient and CI as: β (lo, hi).\"\"\"\n",
    "    if any(pd.isna(x) for x in [beta, lo, hi]):\n",
    "        return \"—\"\n",
    "    return f\"{beta:.3f} ({lo:.3f}, {hi:.3f})\"\n",
    "\n",
    "def make_ols_results_table(res_primary: pd.DataFrame,\n",
    "                           res_sens: pd.DataFrame,\n",
    "                           meta_primary: pd.DataFrame|None,\n",
    "                           meta_sens: pd.DataFrame|None) -> pd.DataFrame:\n",
    "    order_map = {v:i for i,v in enumerate(PLOT_ORDER)}\n",
    "    rp = res_primary.sort_values(\"Outcome\", key=lambda s: s.map(order_map))\n",
    "    rs = res_sens.sort_values(\"Outcome\", key=lambda s: s.map(order_map))\n",
    "\n",
    "    def _get_ns(meta_df):\n",
    "        try:\n",
    "            m=meta_df.iloc[0]; return int(m[\"N_study(used)\"]), int(m[\"N_control(used)\"])\n",
    "        except Exception: return (np.nan, np.nan)\n",
    "\n",
    "    n_sp,n_cp=_get_ns(meta_primary); n_ss,n_cs=_get_ns(meta_sens)\n",
    "    rows=[]\n",
    "    for oc in PLOT_ORDER:\n",
    "        r1=rp[rp[\"Outcome\"]==oc]; r2=rs[rs[\"Outcome\"]==oc]\n",
    "        if r1.empty or r2.empty: \n",
    "            continue\n",
    "        r1=r1.iloc[0]; r2=r2.iloc[0]\n",
    "        rows.append({\n",
    "            \"Outcome\": oc,\n",
    "            \"Primary %Δ (95% CI)\": _fmt_pct_ci(r1[\"% Change\"], r1[\"% CI Lower\"], r1[\"% CI Upper\"]),\n",
    "            \"Primary log1p β (95% CI)\": _fmt_logci(r1[\"Log β\"], r1[\"Log CI Lower\"], r1[\"Log CI Upper\"]),\n",
    "            \"Primary p\": _fmt_p(r1[\"p_value\"]),\n",
    "            \"Primary q\": _fmt_q(r1.get(\"q_value\", np.nan)),\n",
    "\n",
    "            \"Sensitivity %Δ (95% CI)\": _fmt_pct_ci(r2[\"% Change\"], r2[\"% CI Lower\"], r2[\"% CI Upper\"]),\n",
    "            \"Sensitivity log1p β (95% CI)\": _fmt_logci(r2[\"Log β\"], r2[\"Log CI Lower\"], r2[\"Log CI Upper\"]),\n",
    "            \"Sensitivity p\": _fmt_p(r2[\"p_value\"]),\n",
    "            \"Sensitivity q\": _fmt_q(r2.get(\"q_value\", np.nan)),\n",
    "        })\n",
    "    df=pd.DataFrame(rows)\n",
    "    df.attrs[\"N_primary\"]=(n_sp,n_cp)\n",
    "    df.attrs[\"N_sensitivity\"]=(n_ss,n_cs)\n",
    "    return df\n",
    "\n",
    "def export_ols_table_word(df: pd.DataFrame, outdir: str, timestamp: str):\n",
    "    \"\"\"\n",
    "    Export OLS results to a Word table in vertical format (first column = Analytic cohort).\n",
    "    Adjusted model version.\n",
    "    \"\"\"\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: OLS Word export skipped.\")\n",
    "        return None\n",
    "\n",
    "    (nsp, ncp) = df.attrs.get(\"N_primary\", (np.nan, np.nan))\n",
    "    (nss, ncs) = df.attrs.get(\"N_sensitivity\", (np.nan, np.nan))\n",
    "\n",
    "    doc = Document()\n",
    "    title = doc.add_paragraph()\n",
    "    run = title.add_run(\"Table 2. Adjusted differences in WMH (linear models)\")\n",
    "    run.bold = True\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    cols = [\"Analytic cohort\", \"Outcome\", \"%Δ (95% CI)\", \"log1p β (95% CI)\", \"p\", \"q\"]\n",
    "    t = doc.add_table(rows=1, cols=len(cols))\n",
    "    hdr = t.rows[0].cells\n",
    "    for i, c in enumerate(cols):\n",
    "        hdr[i].text = c\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        cells[0].text = \"Primary PSM cohort\"\n",
    "        cells[1].text = row[\"Outcome\"]\n",
    "        cells[2].text = row[\"Primary %Δ (95% CI)\"]\n",
    "        cells[3].text = row[\"Primary log1p β (95% CI)\"]\n",
    "        cells[4].text = row[\"Primary p\"]\n",
    "        cells[5].text = row[\"Primary q\"]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        cells[0].text = \"PSM–sensitivity cohort\"\n",
    "        cells[1].text = row[\"Outcome\"]\n",
    "        cells[2].text = row[\"Sensitivity %Δ (95% CI)\"]\n",
    "        cells[3].text = row[\"Sensitivity log1p β (95% CI)\"]\n",
    "        cells[4].text = row[\"Sensitivity p\"]\n",
    "        cells[5].text = row[\"Sensitivity q\"]\n",
    "\n",
    "    _right_align_numeric(t, [2, 3, 4, 5])\n",
    "\n",
    "    if THREELINE_TABLES:\n",
    "        apply_three_line_table(t, header_row_idx=0)\n",
    "    set_table_font(t, font_name=\"Times New Roman\", font_size=10)\n",
    "\n",
    "    leg = doc.add_paragraph()\n",
    "    leg.add_run(\n",
    "        \"Values are adjusted between-group differences in log1p WMH expressed as percent change \"\n",
    "        \"(Study vs Control) with 95% confidence intervals; the raw log1p coefficient and its 95% CI \"\n",
    "        \"are provided for reference. p values are from the coefficient of the Study indicator. \"\n",
    "        \"q values are Benjamini–Hochberg FDR–adjusted within the secondary outcome family \"\n",
    "        \"(Periventricular, Deep); Total is the primary endpoint and not FDR-adjusted. \"\n",
    "        \"Models adjusted for age at Instance 2, sex, BMI at Instance 0, smoking (ever), alcohol-intake \"\n",
    "        \"frequency (ordinal), Townsend deprivation index, and genetic ethnic grouping; cluster-robust \"\n",
    "        \"standard errors by matched set (match_id). Gatekeeping: secondary outcomes are interpreted \"\n",
    "        \"only if Total WMH is significant at α=0.05.\"\n",
    "    ).italic = True\n",
    "\n",
    "    ninfo = doc.add_paragraph()\n",
    "    ninfo.add_run(\n",
    "        f\"N (Study / Control): Primary = {nsp} / {ncp}; Sensitivity = {nss} / {ncs}.\"\n",
    "    ).italic = True\n",
    "\n",
    "    path = os.path.join(outdir, f\"OLS_Results_Manuscript_Table.docx\")\n",
    "    doc.save(path)\n",
    "    print(f\"OLS results Word table saved: {path}\")\n",
    "    return path\n",
    "\n",
    "# -------- NEW: Unadjusted OLS results Word table --------\n",
    "def export_unadjusted_table_word(df: pd.DataFrame, outdir: str, timestamp: str):\n",
    "    \"\"\"\n",
    "    Export UNADJUSTED OLS results (only C(group)) to a Word table,\n",
    "    mirroring the adjusted table's layout and three-line styling.\n",
    "    \"\"\"\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: Unadjusted Word export skipped.\")\n",
    "        return None\n",
    "\n",
    "    (nsp, ncp) = df.attrs.get(\"N_primary\", (np.nan, np.nan))\n",
    "    (nss, ncs) = df.attrs.get(\"N_sensitivity\", (np.nan, np.nan))\n",
    "\n",
    "    doc = Document()\n",
    "    title = doc.add_paragraph()\n",
    "    run = title.add_run(\"Supplementary Table U1. Unadjusted differences in WMH (linear models)\")\n",
    "    run.bold = True\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    cols = [\"Analytic cohort\", \"Outcome\", \"%Δ (95% CI)\", \"log1p β (95% CI)\", \"p\", \"q\"]\n",
    "    t = doc.add_table(rows=1, cols=len(cols))\n",
    "    hdr = t.rows[0].cells\n",
    "    for i, c in enumerate(cols):\n",
    "        hdr[i].text = c\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        cells[0].text = \"Primary PSM cohort\"\n",
    "        cells[1].text = row[\"Outcome\"]\n",
    "        cells[2].text = row[\"Primary %Δ (95% CI)\"]\n",
    "        cells[3].text = row[\"Primary log1p β (95% CI)\"]\n",
    "        cells[4].text = row[\"Primary p\"]\n",
    "        cells[5].text = row[\"Primary q\"]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        cells[0].text = \"PSM–sensitivity cohort\"\n",
    "        cells[1].text = row[\"Outcome\"]\n",
    "        cells[2].text = row[\"Sensitivity %Δ (95% CI)\"]\n",
    "        cells[3].text = row[\"Sensitivity log1p β (95% CI)\"]\n",
    "        cells[4].text = row[\"Sensitivity p\"]\n",
    "        cells[5].text = row[\"Sensitivity q\"]\n",
    "\n",
    "    _right_align_numeric(t, [2, 3, 4, 5])\n",
    "\n",
    "    if THREELINE_TABLES:\n",
    "        apply_three_line_table(t, header_row_idx=0)\n",
    "    set_table_font(t, font_name=\"Times New Roman\", font_size=10)\n",
    "\n",
    "    # Legend tailored for unadjusted models\n",
    "    leg = doc.add_paragraph()\n",
    "    leg.add_run(\n",
    "        \"Values are UNADJUSTED between-group differences in log1p WMH expressed as percent change \"\n",
    "        \"(Study vs Control) with 95% confidence intervals; the raw log1p coefficient and its 95% CI \"\n",
    "        \"are provided for reference. p values are from the coefficient of the Study indicator. \"\n",
    "        \"q values are Benjamini–Hochberg FDR–adjusted within the secondary outcome family \"\n",
    "        \"(Periventricular, Deep); Total is the primary endpoint and not FDR-adjusted. \"\n",
    "        \"Models include only the group indicator with cluster-robust standard errors by matched set (match_id).\"\n",
    "    ).italic = True\n",
    "\n",
    "    ninfo = doc.add_paragraph()\n",
    "    ninfo.add_run(\n",
    "        f\"N (Study / Control): Primary = {nsp} / {ncp}; Sensitivity = {nss} / {ncs}.\"\n",
    "    ).italic = True\n",
    "\n",
    "    path = os.path.join(outdir, f\"UNADJ_Results_Manuscript_Table.docx\")\n",
    "    doc.save(path)\n",
    "    print(f\"Unadjusted results Word table saved: {path}\")\n",
    "    return path\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "all_log, all_meta, all_tbl1 = [], [], []\n",
    "res_by_label, meta_by_label, smd_by_label = {}, {}, {}\n",
    "\n",
    "# NEW containers for UNADJUSTED results\n",
    "res_unadj_by_label, meta_unadj_by_label = {}, {}\n",
    "\n",
    "for label, file in file_sets.items():\n",
    "    print(f\"\\n=== {label} (log1p main analysis) ===\")\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = df.columns.str.replace(r\"[^0-9a-zA-Z]+\",\"_\", regex=True)\n",
    "\n",
    "    if \"group\" not in df.columns or df[\"group\"].dropna().nunique()<2:\n",
    "        print(\"Skipping: invalid 'group'.\"); continue\n",
    "    df[\"group\"]=pd.Categorical(df[\"group\"], categories=GROUP_BASELINE, ordered=False)\n",
    "\n",
    "    sc=\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "    if sc not in df.columns:\n",
    "        print(\"Skipping: missing scaling factor.\"); continue\n",
    "    scale=pd.to_numeric(df[sc], errors=\"coerce\")\n",
    "\n",
    "    # head-size normalized WMH + log1p\n",
    "    df[\"HSNorm_Deep_WMH\"]            = pd.to_numeric(df[\"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\"], errors=\"coerce\") * scale\n",
    "    df[\"HSNorm_PeriVentricular_WMH\"] = pd.to_numeric(df[\"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\"], errors=\"coerce\") * scale\n",
    "    df[\"HSNorm_Total_WMH_T1_T2\"]     = pd.to_numeric(df[\"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"], errors=\"coerce\") * scale\n",
    "    df[\"Log_HSNorm_Deep_WMH\"]            = np.log1p(df[\"HSNorm_Deep_WMH\"])\n",
    "    df[\"Log_HSNorm_PeriVentricular_WMH\"] = np.log1p(df[\"HSNorm_PeriVentricular_WMH\"])\n",
    "    df[\"Log_HSNorm_Total_WMH_T1_T2\"]     = np.log1p(df[\"HSNorm_Total_WMH_T1_T2\"])\n",
    "\n",
    "    present_adj=[v for v in BASE_ADJ if v in df.columns]\n",
    "    categorical_in=set([v for v in present_adj if v in CATEGORICAL])\n",
    "\n",
    "    # impute covariates\n",
    "    dmod=df.copy()\n",
    "    for v in present_adj:\n",
    "        if pd.api.types.is_numeric_dtype(dmod[v]):\n",
    "            dmod[v]=pd.to_numeric(dmod[v], errors=\"coerce\").fillna(dmod[v].median())\n",
    "        else:\n",
    "            mode_vals=dmod[v].mode(dropna=True)\n",
    "            dmod[v]=dmod[v].fillna(mode_vals.iloc[0] if not mode_vals.empty else dmod[v])\n",
    "    dmod=dmod.dropna(subset=[\"group\"])\n",
    "\n",
    "    outcomes=[\n",
    "        \"Log_HSNorm_Total_WMH_T1_T2\",\n",
    "        \"Log_HSNorm_PeriVentricular_WMH\",\n",
    "        \"Log_HSNorm_Deep_WMH\"\n",
    "    ]\n",
    "\n",
    "    # -------- Adjusted models --------\n",
    "    rows, meta = [], []\n",
    "    for oc in outcomes:\n",
    "        if oc not in dmod.columns: print(f\"Missing {oc}; skip.\"); continue\n",
    "        fml = build_formula(oc, present_adj, categorical_in)\n",
    "        model, seinfo, y_used, X_used, n_clu, n_obs = fit_with_cluster(fml, dmod, CLUSTER_VAR)\n",
    "        ols_plain = sm.OLS(y_used, X_used).fit()\n",
    "        try: bp_p = float(het_breuschpagan(ols_plain.resid, ols_plain.model.exog)[1])\n",
    "        except Exception: bp_p = np.nan\n",
    "        used_df = dmod.loc[y_used.index]\n",
    "        n_ctrl, n_study, n_tot = count_group_n(used_df)\n",
    "        key = find_group_term(model.params.index, \"group\", \"Study\")\n",
    "        if key is None: print(\"Group contrast not found; skip.\"); continue\n",
    "        beta=float(model.params[key]); ci_l,ci_u=[float(x) for x in model.conf_int().loc[key]]\n",
    "        pval=float(model.pvalues[key]); pct, pct_l, pct_u = 100*(np.exp(beta)-1), 100*(np.exp(ci_l)-1), 100*(np.exp(ci_u)-1)\n",
    "        rows.append({\"Analysis\":label,\"Outcome\":SHORT[oc],\"% Change\":pct,\"% CI Lower\":pct_l,\"% CI Upper\":pct_u,\n",
    "                     \"Log β\":beta,\"Log CI Lower\":ci_l,\"Log CI Upper\":ci_u,\"p_value\":pval,\"SE_type\":seinfo})\n",
    "        meta.append({\"Analysis\":label,\"Outcome\":SHORT[oc],\"Model formula\":fml,\"SE_type\":seinfo,\"BP_p\":bp_p,\n",
    "                     \"N_total(used)\":n_tot,\"N_control(used)\":n_ctrl,\"N_study(used)\":n_study,\n",
    "                     \"n_clusters\":n_clu,\"n_obs\":n_obs,\"Covariates_used\":\", \".join(present_adj),\n",
    "                     \"Back-transform\":\"%Δ = (e^β - 1) × 100%\"})\n",
    "\n",
    "    res=pd.DataFrame(rows); meta_df=pd.DataFrame(meta)\n",
    "\n",
    "    # ---------- FDR + gatekeeping (adjusted) ----------\n",
    "    if not res.empty:\n",
    "        p_total = res.loc[res[\"Outcome\"]==\"Total WMH\",\"p_value\"]\n",
    "        total_sig = (len(p_total)>0) and (float(p_total.iloc[0]) < PRIMARY_P_ALPHA)\n",
    "\n",
    "        mask_sec = res[\"Outcome\"].isin([\"Periventricular WMH\",\"Deep WMH\"])\n",
    "        if mask_sec.any():\n",
    "            rej, qvals, _, _ = multipletests(res.loc[mask_sec,\"p_value\"].values, method=\"fdr_bh\", alpha=0.05)\n",
    "            res.loc[mask_sec,\"q_value\"]=qvals; res.loc[mask_sec,\"sig_FDR(q≤0.05)\"]=rej\n",
    "\n",
    "        res.loc[res[\"Outcome\"]==\"Total WMH\",[\"q_value\",\"sig_FDR(q≤0.05)\"]]=np.nan\n",
    "        res[\"gatekeep_total_sig\"]=total_sig\n",
    "        res[\"secondary_interpretable\"]=np.where(res[\"Outcome\"].isin([\"Periventricular WMH\",\"Deep WMH\"]), bool(total_sig), np.nan)\n",
    "\n",
    "    # cache (adjusted)\n",
    "    res_by_label[label]=res.copy(); meta_by_label[label]=meta_df.copy()\n",
    "\n",
    "    if not res.empty:\n",
    "        order_map={v:i for i,v in enumerate(PLOT_ORDER)}\n",
    "        print(res.sort_values(\"Outcome\", key=lambda s: s.map(order_map)).round(6).to_string(index=False))\n",
    "        safe=re.sub(r\"[^0-9a-zA-Z]+\",\"_\", label).strip(\"_\")\n",
    "        res.to_csv(os.path.join(MAIN_OUT_DIR, f\"{safe}_LOG1P_results_gatekept.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        meta_df.to_csv(os.path.join(MAIN_OUT_DIR, f\"{safe}_LOG1P_metadata.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        plot_single_forest(res, label, MAIN_OUT_DIR)\n",
    "\n",
    "    # --- NEW: Unadjusted models (only C(group)) ---\n",
    "    rows_u, meta_u = [], []\n",
    "    for oc in outcomes:\n",
    "        if oc not in dmod.columns: print(f\"Missing {oc}; skip (unadjusted).\"); continue\n",
    "        fml_u = build_formula(oc, [], set())  # only C(group)\n",
    "        model_u, seinfo_u, y_u, X_u, n_clu_u, n_obs_u = fit_with_cluster(fml_u, dmod, CLUSTER_VAR)\n",
    "        ols_plain_u = sm.OLS(y_u, X_u).fit()\n",
    "        try: bp_p_u = float(het_breuschpagan(ols_plain_u.resid, ols_plain_u.model.exog)[1])\n",
    "        except Exception: bp_p_u = np.nan\n",
    "        used_u = dmod.loc[y_u.index]\n",
    "        n_ctrl_u, n_study_u, n_tot_u = count_group_n(used_u)\n",
    "        key_u = find_group_term(model_u.params.index, \"group\", \"Study\")\n",
    "        if key_u is None: print(\"Group contrast not found (unadjusted); skip.\"); continue\n",
    "        beta_u=float(model_u.params[key_u]); ci_lu,ci_uu=[float(x) for x in model_u.conf_int().loc[key_u]]\n",
    "        pval_u=float(model_u.pvalues[key_u]); pct_u, pct_lu, pct_uu = 100*(np.exp(beta_u)-1), 100*(np.exp(ci_lu)-1), 100*(np.exp(ci_uu)-1)\n",
    "        rows_u.append({\"Analysis\":label,\"Outcome\":SHORT[oc],\"% Change\":pct_u,\"% CI Lower\":pct_lu,\"% CI Upper\":pct_uu,\n",
    "                       \"Log β\":beta_u,\"Log CI Lower\":ci_lu,\"Log CI Upper\":ci_uu,\"p_value\":pval_u,\"SE_type\":seinfo_u})\n",
    "        meta_u.append({\"Analysis\":label,\"Outcome\":SHORT[oc],\"Model formula\":fml_u,\"SE_type\":seinfo_u,\"BP_p\":bp_p_u,\n",
    "                       \"N_total(used)\":n_tot_u,\"N_control(used)\":n_ctrl_u,\"N_study(used)\":n_study_u,\n",
    "                       \"n_clusters\":n_clu_u,\"n_obs\":n_obs_u,\"Covariates_used\":\"(none; unadjusted)\",\n",
    "                       \"Back-transform\":\"%Δ = (e^β - 1) × 100%\"} )\n",
    "\n",
    "    res_u = pd.DataFrame(rows_u); meta_u_df = pd.DataFrame(meta_u)\n",
    "\n",
    "    # FDR for unadjusted (secondary outcomes only); no gatekeeping flag needed for export\n",
    "    if not res_u.empty:\n",
    "        mask_sec_u = res_u[\"Outcome\"].isin([\"Periventricular WMH\",\"Deep WMH\"])\n",
    "        if mask_sec_u.any():\n",
    "            rej_u, qvals_u, _, _ = multipletests(res_u.loc[mask_sec_u,\"p_value\"].values, method=\"fdr_bh\", alpha=0.05)\n",
    "            res_u.loc[mask_sec_u,\"q_value\"]=qvals_u; res_u.loc[mask_sec_u,\"sig_FDR(q≤0.05)\"]=rej_u\n",
    "        res_u.loc[res_u[\"Outcome\"]==\"Total WMH\",[\"q_value\",\"sig_FDR(q≤0.05)\"]]=np.nan\n",
    "\n",
    "        # cache + CSV\n",
    "        res_unadj_by_label[label] = res_u.copy()\n",
    "        meta_unadj_by_label[label] = meta_u_df.copy()\n",
    "        safe = re.sub(r\"[^0-9a-zA-Z]+\",\"_\", label).strip(\"_\")\n",
    "        res_u.to_csv(os.path.join(MAIN_OUT_DIR, f\"{safe}_UNADJ_results.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        meta_u_df.to_csv(os.path.join(MAIN_OUT_DIR, f\"{safe}_UNADJ_metadata.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Unadjusted results saved for {label}.\")\n",
    "\n",
    "    # --- Table 1 ---\n",
    "    try:\n",
    "        tbl1 = make_table1(dmod, label, id_col=\"Participant_ID\")\n",
    "        safe=re.sub(r\"[^0-9a-zA-Z]+\",\"_\", label).strip(\"_\")\n",
    "        csv_path=os.path.join(MAIN_OUT_DIR, f\"{safe}_Table1_Baseline.csv\")\n",
    "        tbl1.to_csv(csv_path, index=False, encoding=\"utf-8-sig\"); print(f\"Table 1 CSV saved: {csv_path}\")\n",
    "        docx_path = export_table1_word(tbl1, label, MAIN_OUT_DIR, timestamp)\n",
    "        if docx_path: print(f\"Table 1 Word saved: {docx_path}\")\n",
    "        elif Document is None: print(\"Install 'python-docx' for Word export:  pip install python-docx\")\n",
    "        all_tbl1.append(tbl1)\n",
    "    except Exception as e:\n",
    "        print(f\"Table 1 failed for {label}: {e}\")\n",
    "\n",
    "    # --- SMD for supplement ---\n",
    "    try:\n",
    "        smd_tbl = pd.DataFrame({\n",
    "            \"Characteristic\":[\n",
    "                \"Age at Instance 2, y\",\n",
    "                \"Female sex\",\n",
    "                \"BMI at Instance 0, kg/m²\",\n",
    "                \"Genetic ethnic grouping, White\",\n",
    "                \"Townsend deprivation index\",\n",
    "                \"Smoking, ever\",\n",
    "                \"Alcohol intake frequency (ordinal)\"\n",
    "            ],\n",
    "            \"SMD\":[\n",
    "                _smd_cont(dmod.loc[dmod[\"group\"]==\"Study\",\"Age_at_Instance_2\"],\n",
    "                          dmod.loc[dmod[\"group\"]==\"Control\",\"Age_at_Instance_2\"]) if \"Age_at_Instance_2\" in dmod.columns else np.nan,\n",
    "                _smd_binary(_is_female_series(dmod.loc[dmod[\"group\"]==\"Study\",\"Sex\"]).astype(float),\n",
    "                            _is_female_series(dmod.loc[dmod[\"group\"]==\"Control\",\"Sex\"]).astype(float)) if \"Sex\" in dmod.columns else np.nan,\n",
    "                _smd_cont(dmod.loc[dmod[\"group\"]==\"Study\",\"Body_mass_index_BMI_Instance_0\"],\n",
    "                          dmod.loc[dmod[\"group\"]==\"Control\",\"Body_mass_index_BMI_Instance_0\"]) if \"Body_mass_index_BMI_Instance_0\" in dmod.columns else np.nan,\n",
    "                _smd_binary(_is_white_series(dmod.loc[dmod[\"group\"]==\"Study\",\"Genetic_ethnic_grouping\"]).astype(float),\n",
    "                            _is_white_series(dmod.loc[dmod[\"group\"]==\"Control\",\"Genetic_ethnic_grouping\"]).astype(float)) if \"Genetic_ethnic_grouping\" in dmod.columns else np.nan,\n",
    "                _smd_cont(dmod.loc[dmod[\"group\"]==\"Study\",\"Townsend_deprivation_index_at_recruitment\"],\n",
    "                          dmod.loc[dmod[\"group\"]==\"Control\",\"Townsend_deprivation_index_at_recruitment\"]) if \"Townsend_deprivation_index_at_recruitment\" in dmod.columns else np.nan,\n",
    "                _smd_binary(_is_true_series(dmod.loc[dmod[\"group\"]==\"Study\",\"Smoking_Ever\"]).astype(float),\n",
    "                            _is_true_series(dmod.loc[dmod[\"group\"]==\"Control\",\"Smoking_Ever\"]).astype(float)) if \"Smoking_Ever\" in dmod.columns else np.nan,\n",
    "                _smd_cont(_as_num(dmod.loc[dmod[\"group\"]==\"Study\",\"Alcohol_intake_frequency_ordinal\"]),\n",
    "                          _as_num(dmod.loc[dmod[\"group\"]==\"Control\",\"Alcohol_intake_frequency_ordinal\"])) if \"Alcohol_intake_frequency_ordinal\" in dmod.columns else np.nan\n",
    "            ]\n",
    "        })\n",
    "        smd_tbl[\"|SMD|\"]=smd_tbl[\"SMD\"].abs()\n",
    "        safe=re.sub(r\"[^0-9a-zA-Z]+\",\"_\", label).strip(\"_\")\n",
    "        smd_csv=os.path.join(MAIN_OUT_DIR, f\"{safe}_SMD.csv\")\n",
    "        smd_tbl.to_csv(smd_csv, index=False, encoding=\"utf-8-sig\"); print(f\"SMD table saved: {smd_csv}\")\n",
    "        smd_by_label[label]=smd_tbl.rename(columns={\"SMD\":\"SMD\",\"|SMD|\":\"|SMD|\"})\n",
    "    except Exception as e:\n",
    "        print(f\"SMD computation failed for {label}: {e}\")\n",
    "\n",
    "    all_log.append(res); all_meta.append(meta_df)\n",
    "\n",
    "# Overlay forest figure\n",
    "LEFT_KEY=\"Primary 1:10 (NoNeuro)\"; RIGHT_KEY=\"Sensitivity 1:10 (WithNeuro)\"\n",
    "if LEFT_KEY in res_by_label and RIGHT_KEY in res_by_label and \\\n",
    "   (not res_by_label[LEFT_KEY].empty) and (not res_by_label[RIGHT_KEY].empty):\n",
    "    plot_overlay_forest(\n",
    "        res_primary=res_by_label[LEFT_KEY],\n",
    "        res_sensitivity=res_by_label[RIGHT_KEY],\n",
    "        outdir=MAIN_OUT_DIR,\n",
    "        basename=f\"Overlay_Forest_Primary_vs_Sensitivity\"\n",
    "    )\n",
    "    print(\"Overlay figure saved (Primary vs Sensitivity).\")\n",
    "else:\n",
    "    print(\"Overlay figure not created: missing one of the cohorts.\")\n",
    "\n",
    "# SMD supplement (combined table + Love plot)\n",
    "def combine_smd_tables(tbl_primary: pd.DataFrame, tbl_sens: pd.DataFrame) -> pd.DataFrame:\n",
    "    p = tbl_primary.set_index(\"Characteristic\")[[\"SMD\",\"|SMD|\"]].rename(\n",
    "        columns={\"SMD\":\"SMD (Primary)\",\"|SMD|\":\"|SMD| (Primary)\"})\n",
    "    s = tbl_sens.set_index(\"Characteristic\")[[\"SMD\",\"|SMD|\"]].rename(\n",
    "        columns={\"SMD\":\"SMD (Sensitivity)\",\"|SMD|\":\"|SMD| (Sensitivity)\"})\n",
    "    combo = p.join(s, how=\"outer\").reset_index()\n",
    "    combo[\"Balanced Primary (<0.1)\"] = combo[\"|SMD| (Primary)\"] < 0.1\n",
    "    combo[\"Balanced Sensitivity (<0.1)\"] = combo[\"|SMD| (Sensitivity)\"] < 0.1\n",
    "    order = [\"Age at Instance 2, y\",\"Female sex\",\"BMI at Instance 0, kg/m²\",\n",
    "             \"Genetic ethnic grouping, White\",\"Townsend deprivation index\",\n",
    "             \"Smoking, ever\",\"Alcohol intake frequency (ordinal)\"]\n",
    "    combo = combo.sort_values(\"Characteristic\", key=lambda s: s.map({v:i for i,v in enumerate(order)}))\n",
    "    return combo\n",
    "\n",
    "def plot_love_overlay(tbl_primary: pd.DataFrame, tbl_sens: pd.DataFrame, outdir: str, basename: str):\n",
    "    order=[\"Age at Instance 2, y\",\"Female sex\",\"BMI at Instance 0, kg/m²\",\n",
    "           \"Genetic ethnic grouping, White\",\"Townsend deprivation index\",\n",
    "           \"Smoking, ever\",\"Alcohol intake frequency (ordinal)\"]\n",
    "    p=tbl_primary.set_index(\"Characteristic\").reindex(order)\n",
    "    s=tbl_sens.set_index(\"Characteristic\").reindex(order)\n",
    "    y=np.arange(len(order))[::-1]; jitter=0.10; pad_y=0.45\n",
    "    fig=plt.figure(figsize=(6.8,3.6)); ax=plt.gca()\n",
    "    ax.errorbar(p[\"|SMD|\"], y+jitter, fmt=\"o\", ms=5.5, mfc=\"#1f3b4d\", mec=\"#1f3b4d\",\n",
    "                linestyle=\"none\", color=\"#1f3b4d\", label=\"Primary\")\n",
    "    ax.errorbar(s[\"|SMD|\"], y-jitter, fmt=\"s\", ms=5.2, mfc=\"white\", mec=\"#6e6e6e\",\n",
    "                linestyle=\"none\", color=\"#6e6e6e\", label=\"Sensitivity\")\n",
    "    ax.axvline(0.1, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(0.2, color=\"grey\", linestyle=\":\",  linewidth=1)\n",
    "    ax.set_yticks(y, order); ax.set_xlabel(\"|SMD|\")\n",
    "    xmax=np.nanmax([p[\"|SMD|\"], s[\"|SMD|\"]]); ax.set_xlim(left=0, right=float(xmax)*1.2+0.02)\n",
    "    ax.set_ylim(y.min()-pad_y, y.max()+pad_y); ax.grid(axis=\"x\", linestyle=\":\", linewidth=0.7, alpha=0.8)\n",
    "    leg=ax.legend(frameon=False, loc=\"upper right\", handlelength=1.6)\n",
    "    for attr in (\"legendHandles\",\"legend_handles\"):\n",
    "        if hasattr(leg,attr):\n",
    "            try:\n",
    "                for lh in getattr(leg,attr):\n",
    "                    if hasattr(lh,\"set_linewidth\"): lh.set_linewidth(1.6)\n",
    "            except Exception: pass; break\n",
    "    plt.tight_layout(); safe=re.sub(r\"[^0-9a-zA-Z]+\",\"_\", basename).strip(\"_\"); base=os.path.join(outdir, safe)\n",
    "    plt.savefig(base+\".png\", dpi=600); plt.savefig(base+\".pdf\"); plt.savefig(base+\".svg\"); plt.close(fig)\n",
    "\n",
    "if LEFT_KEY in smd_by_label and RIGHT_KEY in smd_by_label:\n",
    "    smd_combo = combine_smd_tables(smd_by_label[LEFT_KEY], smd_by_label[RIGHT_KEY])\n",
    "    combo_csv = os.path.join(MAIN_OUT_DIR, f\"Supplement_SMD_Balance.csv\")\n",
    "    smd_combo.to_csv(combo_csv, index=False, encoding=\"utf-8-sig\"); print(f\"Supplementary SMD CSV saved: {combo_csv}\")\n",
    "    if Document is not None:\n",
    "        path_docx = export_smd_word(smd_combo, MAIN_OUT_DIR, timestamp)\n",
    "        print(f\"Supplementary SMD Word saved: {path_docx}\")\n",
    "    else:\n",
    "        print(\"Install 'python-docx' for SMD Word:  pip install python-docx\")\n",
    "    plot_love_overlay(smd_by_label[LEFT_KEY], smd_by_label[RIGHT_KEY],\n",
    "                      outdir=MAIN_OUT_DIR,\n",
    "                      basename=f\"Love_Plot_Primary_vs_Sensitivity\")\n",
    "    print(\"Love plot (overlay) saved.\")\n",
    "else:\n",
    "    print(\"SMD supplement not created: missing SMD tables.\")\n",
    "\n",
    "# Figure legends\n",
    "figure_legends = f\"\"\"Figure X. Adjusted differences in WMH (overlay forest plot)\n",
    "Primary (deep blue, filled circles) and Sensitivity (dark gray, open squares) cohorts are overlaid in a single panel without connecting lines. Points show adjusted between-group differences in log1p-transformed WMH volumes expressed as percent change (Study vs Control); error bars indicate 95% CIs. Models adjusted for age at Instance 2, sex, BMI at Instance 0, smoking (ever), alcohol-intake frequency (ordinal), Townsend deprivation index, and genetic ethnic grouping; cluster-robust standard errors by matched set (match_id). Total WMH is the primary endpoint; Periventricular and Deep WMH are secondary endpoints with BH-FDR applied within the secondary family. The Sensitivity cohort repeats matching without pre-index or index-date exclusions.\n",
    "\n",
    "Figure Y-1. Primary cohort forest plot\n",
    "Single-cohort version of Figure X for the primary cohort (excluding pre-index neurological/cerebrovascular diagnoses; index-date assignment and risk-set pruning applied in controls).\n",
    "\n",
    "Figure Y-2. Sensitivity cohort forest plot\n",
    "Single-cohort version of Figure X for the sensitivity cohort (no pre-index or index-date exclusions).\"\"\"\n",
    "leg_txt=os.path.join(MAIN_OUT_DIR, f\"Figure_legends.txt\")\n",
    "with open(leg_txt,\"w\",encoding=\"utf-8\") as f: f.write(figure_legends)\n",
    "print(f\"Figure legends (TXT) saved: {leg_txt}\")\n",
    "if Document is not None:\n",
    "    doc=Document(); p=doc.add_paragraph(); r=p.add_run(\"Figure Legends\"); r.bold=True\n",
    "    for block in figure_legends.strip().split(\"\\n\\n\"): doc.add_paragraph(block)\n",
    "    leg_docx=os.path.join(MAIN_OUT_DIR, f\"Figure_legends.docx\")\n",
    "    doc.save(leg_docx); print(f\"Figure legends (Word) saved: {leg_docx}\")\n",
    "else:\n",
    "    print(\"Install 'python-docx' for Word legends:  pip install python-docx\")\n",
    "\n",
    "# OLS manuscript table (Primary & Sensitivity together; with q) -- ADJUSTED\n",
    "if LEFT_KEY in res_by_label and RIGHT_KEY in res_by_label and \\\n",
    "   (not res_by_label[LEFT_KEY].empty) and (not res_by_label[RIGHT_KEY].empty):\n",
    "    ols_tbl = make_ols_results_table(\n",
    "        res_by_label[LEFT_KEY], res_by_label[RIGHT_KEY],\n",
    "        meta_by_label.get(LEFT_KEY), meta_by_label.get(RIGHT_KEY)\n",
    "    )\n",
    "    csv_ols = os.path.join(MAIN_OUT_DIR, f\"OLS_Results_Manuscript_Table.csv\")\n",
    "    ols_tbl.to_csv(csv_ols, index=False, encoding=\"utf-8-sig\"); print(f\"OLS results table CSV saved: {csv_ols}\")\n",
    "    if Document is not None:\n",
    "        docx_ols = export_ols_table_word(ols_tbl, MAIN_OUT_DIR, timestamp)\n",
    "        print(f\"OLS results table Word saved: {docx_ols}\")\n",
    "    else:\n",
    "        print(\"Install 'python-docx' for OLS Word export:  pip install python-docx\")\n",
    "else:\n",
    "    print(\"OLS results table not created: missing one of the cohorts.\")\n",
    "\n",
    "# NEW: Unadjusted manuscript-like table (Primary & Sensitivity together)\n",
    "if LEFT_KEY in res_unadj_by_label and RIGHT_KEY in res_unadj_by_label and \\\n",
    "   (not res_unadj_by_label[LEFT_KEY].empty) and (not res_unadj_by_label[RIGHT_KEY].empty):\n",
    "    unadj_tbl = make_ols_results_table(\n",
    "        res_unadj_by_label[LEFT_KEY], res_unadj_by_label[RIGHT_KEY],\n",
    "        meta_unadj_by_label.get(LEFT_KEY), meta_unadj_by_label.get(RIGHT_KEY)\n",
    "    )\n",
    "    csv_unadj = os.path.join(MAIN_OUT_DIR, f\"UNADJ_Results_Manuscript_Table.csv\")\n",
    "    unadj_tbl.to_csv(csv_unadj, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Unadjusted results table CSV saved: {csv_unadj}\")\n",
    "    if Document is not None:\n",
    "        docx_unadj = export_unadjusted_table_word(unadj_tbl, MAIN_OUT_DIR, timestamp)\n",
    "        print(f\"Unadjusted results table Word saved: {docx_unadj}\")\n",
    "    else:\n",
    "        print(\"Install 'python-docx' for unadjusted Word export:  pip install python-docx\")\n",
    "else:\n",
    "    print(\"Unadjusted results table not created: missing one of the cohorts or results.\")\n",
    "\n",
    "# combine CSV exports for record\n",
    "final_log  = pd.concat(all_log,  ignore_index=True) if all_log  else pd.DataFrame()\n",
    "final_meta = pd.concat(all_meta, ignore_index=True) if all_meta else pd.DataFrame()\n",
    "if not final_log.empty:\n",
    "    final_log.to_csv(os.path.join(MAIN_OUT_DIR, f\"Main_all_LOG1P_results_gatekept.csv\"),\n",
    "                     index=False, encoding=\"utf-8-sig\")\n",
    "if not final_meta.empty:\n",
    "    final_meta.to_csv(os.path.join(MAIN_OUT_DIR, f\"Main_all_LOG1P_metadata.csv\"),\n",
    "                      index=False, encoding=\"utf-8-sig\")\n",
    "if all_tbl1:\n",
    "    all_tbl1_df = pd.concat(all_tbl1, ignore_index=True)\n",
    "    all_tbl1_df.to_csv(os.path.join(MAIN_OUT_DIR, f\"All_Table1_Baseline.csv\"),\n",
    "                       index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Export combined Word Table 1 (Primary + Sensitivity with block format)\n",
    "    if Document is not None:\n",
    "        doc = Document()\n",
    "        r = doc.add_paragraph().add_run(\"Table 1. Baseline characteristics — Primary PSM and PSM–Sensitivity cohort\")\n",
    "        r.bold = True\n",
    "        doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "        cols = [\"Characteristic\", \"SA\", \"Control\"]\n",
    "        if TABLE1_USE_PVALUE: cols.append(\"p_value\")\n",
    "\n",
    "        t = doc.add_table(rows=1, cols=len(cols))\n",
    "        hdr = t.rows[0].cells\n",
    "        for i, c in enumerate(cols):\n",
    "            hdr[i].text = c\n",
    "\n",
    "        for cohort in [\"Primary 1:10 (NoNeuro)\", \"Sensitivity 1:10 (WithNeuro)\"]:\n",
    "            sub = all_tbl1_df[all_tbl1_df[\"Analysis\"] == cohort]\n",
    "\n",
    "            row = t.add_row().cells\n",
    "            row[0].text = \"Primary PSM cohort\" if \"Primary\" in cohort else \"PSM–sensitivity cohort\"\n",
    "            for j in range(1, len(cols)):\n",
    "                row[j].text = \"\"\n",
    "\n",
    "            for _, r0 in sub.iterrows():\n",
    "                cells = t.add_row().cells\n",
    "                cells[0].text = str(r0[\"Characteristic\"])\n",
    "                cells[1].text = str(r0[\"Study\"])\n",
    "                cells[2].text = str(r0[\"Control\"])\n",
    "                if TABLE1_USE_PVALUE:\n",
    "                    val = r0.get(\"p_value\", \"\")\n",
    "                    if isinstance(val, float) and not np.isnan(val):\n",
    "                        cells[3].text = \"<0.001\" if val < 0.001 else f\"{val:.3f}\"\n",
    "                    else:\n",
    "                        cells[3].text = str(val) if val not in (None, np.nan) else \"\"\n",
    "\n",
    "        num_idx = [1, 2] + ([3] if TABLE1_USE_PVALUE else [])\n",
    "        _right_align_numeric(t, num_idx)\n",
    "\n",
    "        if THREELINE_TABLES:\n",
    "            apply_three_line_table(t, header_row_idx=0)\n",
    "        set_table_font(t, font_name=\"Times New Roman\", font_size=10)\n",
    "\n",
    "        legend = doc.add_paragraph()\n",
    "        legend.add_run(\n",
    "            \"Legend: Values are mean (SD), median [IQR], or n (%). \"\n",
    "            \"p-values from Welch’s t test (continuous), Mann–Whitney U (alcohol intake), \"\n",
    "            \"and χ² or Fisher’s exact test (categorical). \"\n",
    "            \"SA = sleep apnea; Control = matched controls.\"\n",
    "        ).italic = True\n",
    "\n",
    "        doc.save(os.path.join(MAIN_OUT_DIR, f\"Table1_Baseline_Primary_Sensitivity.docx\"))\n",
    "        print(\"Combined Table 1 Word (block format) saved.\")\n",
    "\n",
    "print(f'\\nAll outputs saved in \"{MAIN_OUT_DIR}/\" (timestamp={timestamp}).')\n",
    "print(\"Tables exported as three-line tables in Word. Figures saved as PNG/PDF/SVG.\")\n",
    "\n",
    "\n",
    "# ================== CMC-adjusted sensitivity analysis (+CMC adjustment explicitly labeled) ==================\n",
    "# Goal:\n",
    "#   Conduct a sensitivity analysis in both cohorts by adding CMC (comorbidity count)\n",
    "#   as an additional covariate to the main fully adjusted models.\n",
    "#   This block generates eFigures (with “+CMC adjustment” label) and eTables (CSV + Word),\n",
    "#   in the same format as the main analysis.\n",
    "\n",
    "SUPP_DIR = os.path.join(MAIN_OUT_DIR, \"eSupp_CMC\")\n",
    "os.makedirs(SUPP_DIR, exist_ok=True)\n",
    "\n",
    "res_cmc_by_label, meta_cmc_by_label = {}, {}\n",
    "\n",
    "def _choose_cmc_var(df):\n",
    "    \"\"\"Choose which CMC variable to use (prefer categorical).\"\"\"\n",
    "    if \"CMC_score_cat\" in df.columns:\n",
    "        return \"CMC_score_cat\", True\n",
    "    if \"CMC_score_raw\" in df.columns:\n",
    "        return \"CMC_score_raw\", False\n",
    "    return None, None\n",
    "\n",
    "def _attach_cmc_and_impute(dmod0: pd.DataFrame, base_adj: list, categorical_set: set):\n",
    "    \"\"\"\n",
    "    Add the chosen CMC variable to the adjustment set and perform imputation:\n",
    "      - numeric → median\n",
    "      - categorical → mode\n",
    "    \"\"\"\n",
    "    dmod = dmod0.copy()\n",
    "    cmc_var, is_cat = _choose_cmc_var(dmod)\n",
    "    if cmc_var is None:\n",
    "        return dmod, base_adj, categorical_set, None\n",
    "\n",
    "    adj_vars = base_adj + [cmc_var]\n",
    "    cat_vars = set(categorical_set)\n",
    "    if is_cat:\n",
    "        cat_vars.add(cmc_var)\n",
    "\n",
    "    for v in adj_vars:\n",
    "        if pd.api.types.is_numeric_dtype(dmod[v]):\n",
    "            dmod[v] = pd.to_numeric(dmod[v], errors=\"coerce\").fillna(dmod[v].median())\n",
    "        else:\n",
    "            mode_vals = dmod[v].mode(dropna=True)\n",
    "            dmod[v] = dmod[v].fillna(mode_vals.iloc[0] if not mode_vals.empty else dmod[v])\n",
    "\n",
    "    return dmod, adj_vars, cat_vars, cmc_var\n",
    "\n",
    "# Iterate through both cohorts\n",
    "for label, file in file_sets.items():\n",
    "    print(f\"\\n=== {label} (CMC-adjusted sensitivity analysis) ===\")\n",
    "    if label not in res_by_label:\n",
    "        print(\"Skipping CMC sensitivity: main analysis results not found for this cohort.\")\n",
    "        continue\n",
    "\n",
    "    df0 = pd.read_csv(file)\n",
    "    df0.columns = df0.columns.str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "    if \"group\" not in df0.columns or df0[\"group\"].dropna().nunique() < 2:\n",
    "        print(\"Skipping: invalid or missing 'group' variable.\")\n",
    "        continue\n",
    "    df0[\"group\"] = pd.Categorical(df0[\"group\"], categories=GROUP_BASELINE, ordered=False)\n",
    "\n",
    "    # Ensure log-transformed outcomes exist\n",
    "    need_build = not all(c in df0.columns for c in [\n",
    "        \"Log_HSNorm_Total_WMH_T1_T2\",\n",
    "        \"Log_HSNorm_PeriVentricular_WMH\",\n",
    "        \"Log_HSNorm_Deep_WMH\"\n",
    "    ])\n",
    "    if need_build:\n",
    "        sc = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "        if sc not in df0.columns:\n",
    "            print(\"Skipping: missing head-size scaling factor.\")\n",
    "            continue\n",
    "        scale = pd.to_numeric(df0[sc], errors=\"coerce\")\n",
    "        df0[\"HSNorm_Deep_WMH\"] = pd.to_numeric(df0[\"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\"], errors=\"coerce\") * scale\n",
    "        df0[\"HSNorm_PeriVentricular_WMH\"] = pd.to_numeric(df0[\"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\"], errors=\"coerce\") * scale\n",
    "        df0[\"HSNorm_Total_WMH_T1_T2\"] = pd.to_numeric(df0[\"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"], errors=\"coerce\") * scale\n",
    "        df0[\"Log_HSNorm_Deep_WMH\"] = np.log1p(df0[\"HSNorm_Deep_WMH\"])\n",
    "        df0[\"Log_HSNorm_PeriVentricular_WMH\"] = np.log1p(df0[\"HSNorm_PeriVentricular_WMH\"])\n",
    "        df0[\"Log_HSNorm_Total_WMH_T1_T2\"] = np.log1p(df0[\"HSNorm_Total_WMH_T1_T2\"])\n",
    "\n",
    "    # Prepare covariates\n",
    "    present_adj = [v for v in BASE_ADJ if v in df0.columns]\n",
    "    categorical_in = set([v for v in present_adj if v in CATEGORICAL])\n",
    "\n",
    "    # Impute base covariates first\n",
    "    dmod_base = df0.copy()\n",
    "    for v in present_adj:\n",
    "        if pd.api.types.is_numeric_dtype(dmod_base[v]):\n",
    "            dmod_base[v] = pd.to_numeric(dmod_base[v], errors=\"coerce\").fillna(dmod_base[v].median())\n",
    "        else:\n",
    "            mode_vals = dmod_base[v].mode(dropna=True)\n",
    "            dmod_base[v] = dmod_base[v].fillna(mode_vals.iloc[0] if not mode_vals.empty else dmod_base[v])\n",
    "\n",
    "    # Add and impute CMC\n",
    "    dmod, adj_vars, cat_vars, cmc_used = _attach_cmc_and_impute(dmod_base, present_adj, categorical_in)\n",
    "    if cmc_used is None:\n",
    "        print(\"Skipping: no CMC variable found.\")\n",
    "        continue\n",
    "    dmod = dmod.dropna(subset=[\"group\"])\n",
    "\n",
    "    outcomes = [\n",
    "        \"Log_HSNorm_Total_WMH_T1_T2\",\n",
    "        \"Log_HSNorm_PeriVentricular_WMH\",\n",
    "        \"Log_HSNorm_Deep_WMH\"\n",
    "    ]\n",
    "\n",
    "    rows, meta = [], []\n",
    "    for oc in outcomes:\n",
    "        if oc not in dmod.columns:\n",
    "            print(f\"Outcome {oc} missing; skipped.\")\n",
    "            continue\n",
    "\n",
    "        fml = build_formula(oc, adj_vars, cat_vars)\n",
    "        model, seinfo, y_used, X_used, n_clu, n_obs = fit_with_cluster(fml, dmod, CLUSTER_VAR)\n",
    "\n",
    "        ols_plain = sm.OLS(y_used, X_used).fit()\n",
    "        try:\n",
    "            bp_p = float(het_breuschpagan(ols_plain.resid, ols_plain.model.exog)[1])\n",
    "        except Exception:\n",
    "            bp_p = np.nan\n",
    "\n",
    "        used_df = dmod.loc[y_used.index]\n",
    "        n_ctrl, n_study, n_tot = count_group_n(used_df)\n",
    "\n",
    "        key = find_group_term(model.params.index, \"group\", \"Study\")\n",
    "        if key is None:\n",
    "            print(\"Group contrast not found; skipped.\")\n",
    "            continue\n",
    "\n",
    "        beta = float(model.params[key])\n",
    "        ci_l, ci_u = [float(x) for x in model.conf_int().loc[key]]\n",
    "        pval = float(model.pvalues[key])\n",
    "        pct, pct_l, pct_u = 100*(np.exp(beta)-1), 100*(np.exp(ci_l)-1), 100*(np.exp(ci_u)-1)\n",
    "\n",
    "        rows.append({\n",
    "            \"Analysis\": label,\n",
    "            \"Outcome\": SHORT[oc],\n",
    "            \"% Change\": pct, \"% CI Lower\": pct_l, \"% CI Upper\": pct_u,\n",
    "            \"Log β\": beta, \"Log CI Lower\": ci_l, \"Log CI Upper\": ci_u,\n",
    "            \"p_value\": pval, \"SE_type\": seinfo\n",
    "        })\n",
    "        meta.append({\n",
    "            \"Analysis\": label, \"Outcome\": SHORT[oc],\n",
    "            \"Model formula\": fml, \"SE_type\": seinfo, \"BP_p\": bp_p,\n",
    "            \"N_total(used)\": n_tot, \"N_control(used)\": n_ctrl, \"N_study(used)\": n_study,\n",
    "            \"n_clusters\": n_clu, \"n_obs\": n_obs,\n",
    "            \"Covariates_used\": \", \".join(adj_vars),\n",
    "            \"CMC_var\": cmc_used,\n",
    "            \"Back-transform\": \"%Δ = (e^β − 1) × 100%\"\n",
    "        })\n",
    "\n",
    "    res_cmc = pd.DataFrame(rows)\n",
    "    meta_cmc = pd.DataFrame(meta)\n",
    "\n",
    "    # FDR + gatekeeping (same as main analysis)\n",
    "    if not res_cmc.empty:\n",
    "        p_total = res_cmc.loc[res_cmc[\"Outcome\"] == \"Total WMH\", \"p_value\"]\n",
    "        total_sig = (len(p_total) > 0) and (float(p_total.iloc[0]) < PRIMARY_P_ALPHA)\n",
    "\n",
    "        mask_sec = res_cmc[\"Outcome\"].isin([\"Periventricular WMH\", \"Deep WMH\"])\n",
    "        if mask_sec.any():\n",
    "            rej, qvals, _, _ = multipletests(res_cmc.loc[mask_sec, \"p_value\"].values,\n",
    "                                             method=\"fdr_bh\", alpha=0.05)\n",
    "            res_cmc.loc[mask_sec, \"q_value\"] = qvals\n",
    "            res_cmc.loc[mask_sec, \"sig_FDR(q≤0.05)\"] = rej\n",
    "\n",
    "        res_cmc.loc[res_cmc[\"Outcome\"] == \"Total WMH\", [\"q_value\", \"sig_FDR(q≤0.05)\"]] = np.nan\n",
    "        res_cmc[\"gatekeep_total_sig\"] = total_sig\n",
    "        res_cmc[\"secondary_interpretable\"] = np.where(\n",
    "            res_cmc[\"Outcome\"].isin([\"Periventricular WMH\", \"Deep WMH\"]),\n",
    "            bool(total_sig), np.nan\n",
    "        )\n",
    "\n",
    "    res_cmc_by_label[label] = res_cmc.copy()\n",
    "    meta_cmc_by_label[label] = meta_cmc.copy()\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", label).strip(\"_\")\n",
    "\n",
    "    res_cmc.to_csv(os.path.join(SUPP_DIR, f\"{safe}_CMCadj_results.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    meta_cmc.to_csv(os.path.join(SUPP_DIR, f\"{safe}_CMCadj_metadata.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Single-cohort eFigure (explicitly labeled as “+CMC adjustment”)\n",
    "    if not res_cmc.empty:\n",
    "        plot_single_forest(res_cmc, f\"eFigure (+CMC adjustment) — {label}\", SUPP_DIR)\n",
    "\n",
    "# Overlay eFigure (Primary vs Sensitivity)\n",
    "if (LEFT_KEY in res_cmc_by_label and RIGHT_KEY in res_cmc_by_label and\n",
    "    not res_cmc_by_label[LEFT_KEY].empty and not res_cmc_by_label[RIGHT_KEY].empty):\n",
    "    plot_overlay_forest(\n",
    "        res_primary=res_cmc_by_label[LEFT_KEY],\n",
    "        res_sensitivity=res_cmc_by_label[RIGHT_KEY],\n",
    "        outdir=SUPP_DIR,\n",
    "        basename=\"eFigure_CMCadj_Overlay_Primary_vs_Sensitivity (+CMC adjustment)\"\n",
    "    )\n",
    "    print(\"Overlay eFigure (+CMC adjustment) saved.\")\n",
    "else:\n",
    "    print(\"Overlay eFigure (+CMC adjustment) not created: one or both cohorts missing.\")\n",
    "\n",
    "# eTable: merged results (both cohorts)\n",
    "if (LEFT_KEY in res_cmc_by_label and RIGHT_KEY in res_cmc_by_label and\n",
    "    not res_cmc_by_label[LEFT_KEY].empty and not res_cmc_by_label[RIGHT_KEY].empty):\n",
    "    etbl = make_ols_results_table(\n",
    "        res_cmc_by_label[LEFT_KEY], res_cmc_by_label[RIGHT_KEY],\n",
    "        meta_cmc_by_label.get(LEFT_KEY), meta_cmc_by_label.get(RIGHT_KEY)\n",
    "    )\n",
    "    etbl_csv = os.path.join(SUPP_DIR, \"eTable_CMCadj_Results.csv\")\n",
    "    etbl.to_csv(etbl_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"eTable (+CMC adjustment) CSV saved: {etbl_csv}\")\n",
    "\n",
    "    if Document is not None:\n",
    "        path_docx = export_ols_table_word(etbl, SUPP_DIR, timestamp)\n",
    "        if path_docx:\n",
    "            new_path = os.path.join(SUPP_DIR, \"eTable_CMCadj_Results.docx\")\n",
    "            try:\n",
    "                os.replace(path_docx, new_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(f\"eTable (+CMC adjustment) Word saved: {new_path}\")\n",
    "    else:\n",
    "        print(\"Install 'python-docx' to enable Word export.\")\n",
    "else:\n",
    "    print(\"eTable (+CMC adjustment) not created: one or both cohorts missing.\")\n",
    "# ================== END CMC-adjusted sensitivity analysis ==================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "911967ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Primary PSM cohort ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\2353028811.py:1428: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Primary PSM cohort] RT: removed 0, retained 8517\n",
      "[Primary PSM cohort] TMT-B: removed 249 (incl. zeros), retained 6832\n",
      "[Primary PSM cohort] FI: removed 0 (incl. -1), retained 8407\n",
      "[Primary PSM cohort] Digit span: removed 110 (incl. -1), retained 5495\n",
      "[Primary PSM cohort] TMT-B non-completion logistic results saved: Cognitive\\Primary_PSM_cohort\\TMTB_NonCompletion_Logistic_Results.csv\n",
      "[Primary PSM cohort] CSV saved: Cognitive\\Primary_PSM_cohort\\Cognitive_Results.csv\n",
      "[Primary PSM cohort] Word table saved: Cognitive\\Primary_PSM_cohort\\Cognitive_Results.docx\n",
      "\n",
      "=== PSM–sensitivity cohort ===\n",
      "[PSM–sensitivity cohort] RT: removed 0, retained 8873\n",
      "[PSM–sensitivity cohort] TMT-B: removed 286 (incl. zeros), retained 7080\n",
      "[PSM–sensitivity cohort] FI: removed 0 (incl. -1), retained 8756\n",
      "[PSM–sensitivity cohort] Digit span: removed 111 (incl. -1), retained 5675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\2353028811.py:1428: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PSM–sensitivity cohort] TMT-B non-completion logistic results saved: Cognitive\\PSM_sensitivity_cohort\\TMTB_NonCompletion_Logistic_Results.csv\n",
      "[PSM–sensitivity cohort] CSV saved: Cognitive\\PSM_sensitivity_cohort\\Cognitive_Results.csv\n",
      "[PSM–sensitivity cohort] Word table saved: Cognitive\\PSM_sensitivity_cohort\\Cognitive_Results.docx\n",
      "Combined CSV saved: Cognitive\\Cognitive_Results_AllCohorts_Combined.csv\n",
      "Combined Word table saved: Cognitive\\Cognitive_Results_AllCohorts_Combined.docx\n",
      "Combined 4-panel cognitive figure saved.\n",
      "\n",
      "Cognitive secondary endpoint analysis completed.\n",
      "Cognitive Figure 3 legend (TXT) saved: Cognitive\\Cognitive_Figure3_Legend.txt\n",
      "Cognitive Figure 3 legend (Word) saved: Cognitive\\Cognitive_Figure3_Legend.docx\n"
     ]
    }
   ],
   "source": [
    "# Cognitive secondary outcomes analysis\n",
    "# =============================================================================\n",
    "# Design:\n",
    "#   - Two cohorts: Primary & Sensitivity (same CSVs as main WMH analysis).\n",
    "#   - Outcomes (all standardized to SD units):\n",
    "#       1) Reaction time (RT):\n",
    "#            Mean_time_to_correctly_identify_matches_Instance_2\n",
    "#            -> drop NaN\n",
    "#            -> log transform (natural log)\n",
    "#            -> z-score\n",
    "#            -> multiply by -1 (higher = better)\n",
    "#       2) Trail making test-B (TMT-B):\n",
    "#            Duration_to_complete_alphanumeric_path_trail_2_Instance_2\n",
    "#            -> drop NaN and 0\n",
    "#            -> log transform\n",
    "#            -> z-score\n",
    "#            -> multiply by -1 (higher = better)\n",
    "#       3) Fluid intelligence (FI):\n",
    "#            Fluid_intelligence_score_Instance_2\n",
    "#            -> treat -1 as missing (drop)\n",
    "#            -> z-score\n",
    "#       4) Digit span (Memory):\n",
    "#            Maximum_digits_remembered_correctly_Instance_2\n",
    "#            -> treat -1 as missing (drop)\n",
    "#            -> z-score\n",
    "#\n",
    "#   - For each endpoint, missing (after these rules) are excluded *before* modeling.\n",
    "#     The script prints N removed / N retained per cohort & endpoint.\n",
    "#\n",
    "#   - Covariates (Base model):\n",
    "#       Sex, Age_at_Instance_2, Townsend_deprivation_index_at_recruitment,\n",
    "#       Body_mass_index_BMI_Instance_0, Genetic_ethnic_grouping,\n",
    "#       Smoking_Ever, Alcohol_intake_frequency_ordinal,\n",
    "#       has_degree, e4_count\n",
    "#     Categorical: Sex, Genetic_ethnic_grouping, Smoking_Ever, has_degree.\n",
    "#\n",
    "#   - WMH covariates (for sensitivity models):\n",
    "#       Head-size normalized + log1p only (NO z-score):\n",
    "#         Log_HSNorm_Total_WMH_T1_T2\n",
    "#         Log_HSNorm_PeriVentricular_WMH\n",
    "#         Log_HSNorm_Deep_WMH\n",
    "#\n",
    "#   - CMC covariate:\n",
    "#       Prefer CMC_score_cat (categorical), else CMC_score_raw (continuous).\n",
    "#\n",
    "#   - Models (per endpoint, per WMH family):\n",
    "#       Model 1: Base\n",
    "#       Model 2: Base + corresponding WMH (Total / PWMH / DWMH)\n",
    "#       Model 3: Base + CMC\n",
    "#       Model 4: Base + corresponding WMH + CMC\n",
    "#\n",
    "#   - Outcomes are standardized cognitive scores, so effect size β is in SD units.\n",
    "#\n",
    "#   - Outputs:\n",
    "#       For each cohort:\n",
    "#         /Cognitive/<Cohort>/Cognitive_Results.csv\n",
    "#         /Cognitive/<Cohort>/Cognitive_Results.docx  (publication-ready table)\n",
    "#         /Cognitive/<Cohort>/Cognitive_Forest_3Panel.png/.pdf/.tiff\n",
    "#       Root:\n",
    "#         /Cognitive/Cognitive_Results_AllCohorts_Combined.csv\n",
    "#         /Cognitive/Cognitive_Results_AllCohorts.xlsx (if xlsxwriter available)\n",
    "#         /Cognitive/Cognitive_Results_AllCohorts.docx (combined table)\n",
    "#         /Cognitive/Cognitive_Figure_Legend.docx + .txt\n",
    "#\n",
    "#   - Style:\n",
    "#       Match main WMH analysis:\n",
    "#         Arial for figures, Times New Roman 10pt for tables,\n",
    "#         deep blue markers, dashed vertical zero-line,\n",
    "#         top/right spines removed, 600 dpi, TIFF LZW.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from datetime import datetime\n",
    "\n",
    "# Optional Word export\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docx.oxml import OxmlElement\n",
    "    from docx.oxml.ns import qn\n",
    "    from docx.shared import Pt\n",
    "except Exception:\n",
    "    Document = None\n",
    "\n",
    "# ======================== CONFIG ========================\n",
    "\n",
    "ROOT_DIR = \"Cognitive\"\n",
    "os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "\n",
    "COHORT_FILES = {\n",
    "    \"Primary PSM cohort\":       \"primary_cohort.csv\",\n",
    "    \"PSM–sensitivity cohort\":   \"sensitivity_cohort.csv\",\n",
    "}\n",
    "\n",
    "GROUP_COL = \"group\"\n",
    "CLUSTER_VAR = \"match_id\"\n",
    "GROUP_LEVELS = [\"Control\", \"Study\"]\n",
    "\n",
    "SCALE_COL = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "RAW_TOT = \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"\n",
    "RAW_PV  = \"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\"\n",
    "RAW_DW  = \"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\"\n",
    "\n",
    "# Cognitive raw variables\n",
    "RT_COL   = \"Mean_time_to_correctly_identify_matches_Instance_2\"\n",
    "TM_COL   = \"Duration_to_complete_alphanumeric_path_trail_2_Instance_2\"\n",
    "FI_COL   = \"Fluid_intelligence_score_Instance_2\"\n",
    "MEM_COL  = \"Maximum_digits_remembered_correctly_Instance_2\"\n",
    "\n",
    "# Labels\n",
    "COG_ENDPOINTS = [\n",
    "    (\"RT_z\",   \"Reaction time\"),\n",
    "    (\"TM_z\",   \"Trail making test-B\"),\n",
    "    (\"FI_z\",   \"Fluid intelligence\"),\n",
    "    (\"MEM_z\",  \"Digit span\"),\n",
    "]\n",
    "\n",
    "WMH_PANELS = [\n",
    "    (\"Log_HSNorm_Total_WMH_T1_T2\",     \"Total WMH\",          \"A\"),\n",
    "    (\"Log_HSNorm_PeriVentricular_WMH\", \"PWMH\",               \"B\"),\n",
    "    (\"Log_HSNorm_Deep_WMH\",            \"DWMH\",               \"C\"),\n",
    "]\n",
    "\n",
    "MODEL_SPECS = [\n",
    "    (\"Model 1 (Base)\",              1),\n",
    "    (\"Model 2 (+WMH)\",              2),\n",
    "    (\"Model 3 (+CMC)\",              3),\n",
    "    (\"Model 4 (+WMH+CMC)\",          4),\n",
    "]\n",
    "\n",
    "# Base covariates\n",
    "BASE_COVARIATES = [\n",
    "    \"Sex\",\n",
    "    \"Age_at_Instance_2\",\n",
    "    \"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\n",
    "    \"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\n",
    "    \"Alcohol_intake_frequency_ordinal\",\n",
    "    \"has_degree\",\n",
    "    \"e4_count\",\n",
    "]\n",
    "CATEGORICAL_VARS = {\n",
    "    \"Sex\",\n",
    "    \"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\n",
    "    \"has_degree\",\n",
    "}\n",
    "\n",
    "# Export style\n",
    "SAVEFIG_DPI = 600\n",
    "PNG_KW  = {\"dpi\": SAVEFIG_DPI, \"bbox_inches\": \"tight\", \"facecolor\": \"white\"}\n",
    "PDF_KW  = {\"bbox_inches\": \"tight\"}\n",
    "TIFF_KW = {\n",
    "    \"dpi\": SAVEFIG_DPI,\n",
    "    \"bbox_inches\": \"tight\",\n",
    "    \"facecolor\": \"white\",\n",
    "    \"format\": \"tiff\",\n",
    "    \"pil_kwargs\": {\"compression\": \"tiff_lzw\"},\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"savefig.dpi\": SAVEFIG_DPI,\n",
    "})\n",
    "\n",
    "# ======================== HELPERS ========================\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize column names to underscores.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "    return df\n",
    "\n",
    "def ensure_group(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure group column exists and is a categorical with fixed levels.\"\"\"\n",
    "    if GROUP_COL not in df.columns:\n",
    "        raise ValueError(\"Missing 'group' column.\")\n",
    "    df[GROUP_COL] = pd.Categorical(df[GROUP_COL], GROUP_LEVELS)\n",
    "    return df\n",
    "\n",
    "def build_wmh_covariates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create head-size normalized log1p WMH covariates (no z-score).\"\"\"\n",
    "    df = df.copy()\n",
    "    if SCALE_COL not in df.columns:\n",
    "        raise ValueError(\"Missing scaling factor column for WMH.\")\n",
    "    scale = pd.to_numeric(df[SCALE_COL], errors=\"coerce\")\n",
    "\n",
    "    df[\"HSNorm_Total_WMH_T1_T2\"] = pd.to_numeric(df[RAW_TOT], errors=\"coerce\") * scale\n",
    "    df[\"HSNorm_PeriVentricular_WMH\"] = pd.to_numeric(df[RAW_PV], errors=\"coerce\") * scale\n",
    "    df[\"HSNorm_Deep_WMH\"] = pd.to_numeric(df[RAW_DW], errors=\"coerce\") * scale\n",
    "\n",
    "    df[\"Log_HSNorm_Total_WMH_T1_T2\"] = np.log1p(df[\"HSNorm_Total_WMH_T1_T2\"])\n",
    "    df[\"Log_HSNorm_PeriVentricular_WMH\"] = np.log1p(df[\"HSNorm_PeriVentricular_WMH\"])\n",
    "    df[\"Log_HSNorm_Deep_WMH\"] = np.log1p(df[\"HSNorm_Deep_WMH\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def choose_cmc(df: pd.DataFrame):\n",
    "    \"\"\"Prefer categorical CMC, else continuous; return (varname, is_categorical).\"\"\"\n",
    "    if \"CMC_score_cat\" in df.columns:\n",
    "        return \"CMC_score_cat\", True\n",
    "    if \"CMC_score_raw\" in df.columns:\n",
    "        return \"CMC_score_raw\", False\n",
    "    return None, None\n",
    "\n",
    "def impute_covariates(df: pd.DataFrame,\n",
    "                      covariates: list,\n",
    "                      categoricals: set) -> pd.DataFrame:\n",
    "    \"\"\"Simple median/mode imputation for covariates only (not outcomes).\"\"\"\n",
    "    df = df.copy()\n",
    "    for v in covariates:\n",
    "        if v not in df.columns:\n",
    "            continue\n",
    "        if v in categoricals:\n",
    "            mode_vals = df[v].mode(dropna=True)\n",
    "            if not mode_vals.empty:\n",
    "                df[v] = df[v].fillna(mode_vals.iloc[0])\n",
    "        else:\n",
    "            df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "            df[v] = df[v].fillna(df[v].median())\n",
    "    return df\n",
    "\n",
    "def standardize(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Z-score with ddof=1; returns NaN vector if sd==0.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    mu = s.mean()\n",
    "    sd = s.std(ddof=1)\n",
    "    return (s - mu) / sd if sd > 0 else pd.Series(np.nan, index=s.index)\n",
    "\n",
    "def prepare_cognitive_scores(df: pd.DataFrame,\n",
    "                             cohort_label: str) -> pd.DataFrame:\n",
    "    \"\"\"Create cleaned & z-scored cognitive endpoints; print N removed/retained.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Reaction time\n",
    "    rt = pd.to_numeric(df[RT_COL], errors=\"coerce\")\n",
    "    n_total = rt.notna().sum()\n",
    "    rt_z = standardize(np.log(rt))\n",
    "    rt_z = -rt_z   # higher = better\n",
    "    n_used = rt_z.notna().sum()\n",
    "    print(f\"[{cohort_label}] RT: removed {n_total - n_used}, retained {n_used}\")\n",
    "    df[\"RT_z\"] = rt_z\n",
    "\n",
    "    # Trail making test-B\n",
    "    tm = pd.to_numeric(df[TM_COL], errors=\"coerce\")\n",
    "    invalid = tm.isna() | (tm == 0)\n",
    "    n_total = (~tm.isna()).sum()\n",
    "    tm_clean = tm.mask(invalid)\n",
    "    tm_z = standardize(np.log(tm_clean))\n",
    "    tm_z = -tm_z\n",
    "    n_used = tm_z.notna().sum()\n",
    "    print(f\"[{cohort_label}] TMT-B: removed {n_total - n_used} (incl. zeros), retained {n_used}\")\n",
    "    df[\"TM_z\"] = tm_z\n",
    "\n",
    "    # Fluid intelligence\n",
    "    fi = pd.to_numeric(df[FI_COL], errors=\"coerce\")\n",
    "    invalid = fi.isna() | (fi == -1)\n",
    "    n_total = (~fi.isna()).sum()\n",
    "    fi_clean = fi.mask(invalid)\n",
    "    fi_z = standardize(fi_clean)\n",
    "    n_used = fi_z.notna().sum()\n",
    "    print(f\"[{cohort_label}] FI: removed {n_total - n_used} (incl. -1), retained {n_used}\")\n",
    "    df[\"FI_z\"] = fi_z\n",
    "\n",
    "    # Digit span\n",
    "    mem = pd.to_numeric(df[MEM_COL], errors=\"coerce\")\n",
    "    invalid = mem.isna() | (mem == -1)\n",
    "    n_total = (~mem.isna()).sum()\n",
    "    mem_clean = mem.mask(invalid)\n",
    "    mem_z = standardize(mem_clean)\n",
    "    n_used = mem_z.notna().sum()\n",
    "    print(f\"[{cohort_label}] Digit span: removed {n_total - n_used} (incl. -1), retained {n_used}\")\n",
    "    df[\"MEM_z\"] = mem_z\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_formula(outcome, covariates, categoricals, group_var=\"group\"):\n",
    "    \"\"\"Construct Patsy formula with categorical expansion for specified vars.\"\"\"\n",
    "    terms = [f\"C({group_var})\"]\n",
    "    for v in covariates:\n",
    "        if v == group_var:\n",
    "            continue\n",
    "        if v in categoricals:\n",
    "            terms.append(f\"C({v})\")\n",
    "        else:\n",
    "            terms.append(v)\n",
    "    return f\"{outcome} ~ \" + \" + \".join(terms)\n",
    "\n",
    "def find_group_coef(params_index, group_var=\"group\", level=\"Study\"):\n",
    "    \"\"\"Find the coefficient name for Study vs Control (may vary by backend).\"\"\"\n",
    "    prefix = f\"C({group_var})[T.\"\n",
    "    for name in params_index:\n",
    "        if name.startswith(prefix) and name.endswith(f\"{level}]\"):\n",
    "            return name\n",
    "    # fallback: first group contrast if naming differs\n",
    "    for name in params_index:\n",
    "        if name.startswith(prefix):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def fit_cluster_ols(formula, data, cluster_var):\n",
    "    \"\"\"Fit OLS with cluster-robust SE by cluster_var where available; else HC3.\"\"\"\n",
    "    y, X = dmatrices(formula, data, return_type=\"dataframe\", NA_action=\"drop\")\n",
    "    if y.empty:\n",
    "        return None, None, 0, 0\n",
    "    if cluster_var not in data.columns:\n",
    "        model = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "        return model, \"HC3 (no cluster var)\", 0, len(y)\n",
    "    groups = data.loc[y.index, cluster_var]\n",
    "    if groups.notna().sum() < 2 or groups.nunique() < 2:\n",
    "        model = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "        return model, \"HC3 (<=1 cluster)\", int(groups.nunique()), len(y)\n",
    "    ok = groups.notna()\n",
    "    y2, X2, g2 = y.loc[ok], X.loc[ok], groups.loc[ok]\n",
    "    model = sm.OLS(y2, X2).fit(cov_type=\"cluster\", cov_kwds={\"groups\": g2})\n",
    "    return model, f\"cluster (n_clusters={g2.nunique()})\", int(g2.nunique()), len(y2)\n",
    "\n",
    "def fit_cluster_logit(formula, data, cluster_var):\n",
    "    \"\"\"\n",
    "    Fit logistic regression (Binomial GLM) with robust covariance.\n",
    "\n",
    "    Outcome in `formula` must be 0/1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : GLMResults\n",
    "        Fitted model with robust covariance.\n",
    "    se_info : str\n",
    "        Description of SE type.\n",
    "    n_clusters : int\n",
    "        Number of clusters used (0 if none).\n",
    "    n_obs : int\n",
    "        Number of observations used.\n",
    "    \"\"\"\n",
    "    # Build design matrices\n",
    "    y, X = dmatrices(formula, data, return_type=\"dataframe\", NA_action=\"drop\")\n",
    "    if y.empty:\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    n_obs = int(y.shape[0])\n",
    "\n",
    "    # Default: HC3 robust SE\n",
    "    def _fit_hc3():\n",
    "        model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "        try:\n",
    "            res = model.fit(cov_type=\"HC3\")\n",
    "        except TypeError:\n",
    "            # older statsmodels without cov_type in fit\n",
    "            res = model.fit()\n",
    "        return res, \"HC3 (no/insufficient clusters)\", 0\n",
    "\n",
    "    # If we have a cluster var, try cluster-robust\n",
    "    if cluster_var in data.columns:\n",
    "        groups = data.loc[y.index, cluster_var]\n",
    "\n",
    "        if groups.notna().sum() >= 2 and groups.nunique() >= 2:\n",
    "            model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "            try:\n",
    "                res = model.fit(\n",
    "                    cov_type=\"cluster\",\n",
    "                    cov_kwds={\"groups\": groups}\n",
    "                )\n",
    "                se_info = f\"cluster (n_clusters={groups.nunique()})\"\n",
    "                n_clusters = int(groups.nunique())\n",
    "                return res, se_info, n_clusters, n_obs\n",
    "            except TypeError:\n",
    "                # if this statsmodels doesn't support cov_type here, fall back\n",
    "                pass\n",
    "\n",
    "        # if too few clusters or cov_type not supported → HC3 fallback\n",
    "        res, se_info, n_clusters = _fit_hc3()\n",
    "        return res, se_info, n_clusters, n_obs\n",
    "\n",
    "    # No cluster var → HC3\n",
    "    res, se_info, n_clusters = _fit_hc3()\n",
    "    return res, se_info, n_clusters, n_obs\n",
    "\n",
    "\n",
    "\n",
    "# ========= Word table helpers =========\n",
    "\n",
    "def _set_cell_border(cell, **kwargs):\n",
    "    \"\"\"Apply thin borders selectively to a table cell (used for three-line style).\"\"\"\n",
    "    tc = cell._tc\n",
    "    tcPr = tc.get_or_add_tcPr()\n",
    "    tcBorders = tcPr.find(qn('w:tcBorders'))\n",
    "    if tcBorders is None:\n",
    "        tcBorders = OxmlElement('w:tcBorders')\n",
    "        tcPr.append(tcBorders)\n",
    "    for edge in ('left','right','top','bottom','insideH','insideV'):\n",
    "        if edge in kwargs:\n",
    "            edge_data = kwargs.get(edge)\n",
    "            tag = OxmlElement(f'w:{edge}')\n",
    "            for key in (\"val\",\"sz\",\"color\",\"space\"):\n",
    "                if key in edge_data:\n",
    "                    tag.set(qn(f\"w:{key}\"), str(edge_data[key]))\n",
    "            tcBorders.append(tag)\n",
    "\n",
    "def _apply_three_line_table(table, header_row_idx=0):\n",
    "    \"\"\"Make a three-line academic table: header top+bottom lines, final bottom line.\"\"\"\n",
    "    # clear all\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            _set_cell_border(cell,\n",
    "                             left={\"val\":\"nil\"}, right={\"val\":\"nil\"},\n",
    "                             top={\"val\":\"nil\"}, bottom={\"val\":\"nil\"})\n",
    "    # header top+bottom\n",
    "    for cell in table.rows[header_row_idx].cells:\n",
    "        _set_cell_border(cell,\n",
    "                         top={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"},\n",
    "                         bottom={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "    # bottom line\n",
    "    for cell in table.rows[-1].cells:\n",
    "        _set_cell_border(cell,\n",
    "                         bottom={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "\n",
    "def _right_align_numeric(table, idx_list):\n",
    "    \"\"\"Right-align numeric-looking columns by index list (0-based).\"\"\"\n",
    "    for row in table.rows[1:]:\n",
    "        for j in idx_list:\n",
    "            if j < len(row.cells):\n",
    "                for p in row.cells[j].paragraphs:\n",
    "                    p.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "\n",
    "def export_cog_table_word(df_all: pd.DataFrame,\n",
    "                          out_path: str,\n",
    "                          title: str,\n",
    "                          legend: str):\n",
    "    \"\"\"\n",
    "    Word table (no vertical merges), columns:\n",
    "      Analytic Cohort | Outcome | N | Models | β (95% CI) | p\n",
    "\n",
    "    Row layout per (Cohort, Outcome):\n",
    "      - Model 1 (Base)\n",
    "      - Model 2 (+CMC)                      <-- source = \"Model 3 (+CMC)\"\n",
    "      - Model 3 (+Total WMH)                <-- source = \"Model 2 (+WMH)\" w/ WMH_family=Total WMH\n",
    "      - Model 3 (+PWMH)                     <-- source = \"Model 2 (+WMH)\" w/ WMH_family=PWMH\n",
    "      - Model 3 (+DWMH)                     <-- source = \"Model 2 (+WMH)\" w/ WMH_family=DWMH\n",
    "      - Model 4 (+Total WMH + CMC)          <-- source = \"Model 4 (+WMH+CMC)\" per family\n",
    "      - Model 4 (+PWMH + CMC)\n",
    "      - Model 4 (+DWMH + CMC)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Expects df_all columns: Cohort, WMH_family, Endpoint, N, Model, Beta, CI_lower, CI_upper, p\n",
    "    - N 已是 \"SA/Control\" 形式（脚本前面已生成）\n",
    "    - 不输出 q\n",
    "    \"\"\"\n",
    "    if Document is None or df_all.empty:\n",
    "        return None\n",
    "\n",
    "    needed = {\"Cohort\",\"WMH_family\",\"Endpoint\",\"N\",\"Model\",\n",
    "              \"Beta\",\"CI_lower\",\"CI_upper\",\"p\"}\n",
    "    miss = [c for c in needed if c not in df_all.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Missing columns for export: {miss}\")\n",
    "\n",
    "    # 规范顺序\n",
    "    df = df_all.copy()\n",
    "    wmh_order = [\"Total WMH\", \"PWMH\", \"DWMH\"]\n",
    "\n",
    "    # 小工具：按条件取一行，取不到则返回 None\n",
    "    def _get_row(cohort, endpoint, model, wmh=None):\n",
    "        q = (df[\"Cohort\"]==cohort) & (df[\"Endpoint\"]==endpoint) & (df[\"Model\"]==model)\n",
    "        if wmh is not None:\n",
    "            q &= (df[\"WMH_family\"]==wmh)\n",
    "        sub = df[q]\n",
    "        if sub.empty:\n",
    "            return None\n",
    "        return sub.iloc[0]\n",
    "\n",
    "    def _fmt_beta_ci(beta, lo, hi):\n",
    "        if any(pd.isna([beta, lo, hi])):\n",
    "            return \"—\"\n",
    "        return f\"{beta:.3f} ({lo:.3f}, {hi:.3f})\"\n",
    "\n",
    "    def _fmt_p(pv):\n",
    "        if pd.isna(pv): return \"—\"\n",
    "        pv = float(pv)\n",
    "        return \"<0.001\" if pv < 1e-3 else f\"{pv:.3f}\"\n",
    "\n",
    "    # 文档与表头\n",
    "    doc = Document()\n",
    "    try:\n",
    "        doc.styles[\"Normal\"].font.name = \"Times New Roman\"\n",
    "        doc.styles[\"Normal\"].font.size = Pt(10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    p = doc.add_paragraph(); r = p.add_run(title); r.bold = True\n",
    "\n",
    "    headers = [\"Analytic Cohort\", \"Outcome\", \"N\",\n",
    "               \"Models\", \"β (95% CI)\", \"p\"]\n",
    "    tbl = doc.add_table(rows=1, cols=len(headers))\n",
    "    for j,h in enumerate(headers):\n",
    "        tbl.rows[0].cells[j].text = h\n",
    "\n",
    "    # 以 (Cohort, Endpoint) 为块输出\n",
    "    for (cohort, endpoint), blk in df.groupby([\"Cohort\",\"Endpoint\"], sort=False):\n",
    "        # 这个 endpoint 的 N（任意行的 N 都相同）\n",
    "        n_str = str(blk[\"N\"].dropna().iloc[0]) if not blk[\"N\"].dropna().empty else \"\"\n",
    "\n",
    "        # ---- Model 1 (Base)\n",
    "        r1 = _get_row(cohort, endpoint, \"Model 1 (Base)\")\n",
    "        row = tbl.add_row().cells\n",
    "        row[0].text = str(cohort)\n",
    "        row[1].text = str(endpoint)\n",
    "        row[2].text = n_str\n",
    "        row[3].text = \"Model 1 (Base)\"\n",
    "        if r1 is not None:\n",
    "            row[4].text = _fmt_beta_ci(r1[\"Beta\"], r1[\"CI_lower\"], r1[\"CI_upper\"])\n",
    "            row[5].text = _fmt_p(r1[\"p\"])\n",
    "        else:\n",
    "            row[4].text = \"—\"; row[5].text = \"—\"\n",
    "\n",
    "        # ---- Model 2 (+CMC)  ← 源自你结果里的 “Model 3 (+CMC)”\n",
    "        r2 = _get_row(cohort, endpoint, \"Model 3 (+CMC)\")\n",
    "        row = tbl.add_row().cells\n",
    "        row[0].text = str(cohort)\n",
    "        row[1].text = str(endpoint)\n",
    "        row[2].text = n_str\n",
    "        row[3].text = \"Model 2 (+CMC)\"\n",
    "        if r2 is not None:\n",
    "            row[4].text = _fmt_beta_ci(r2[\"Beta\"], r2[\"CI_lower\"], r2[\"CI_upper\"])\n",
    "            row[5].text = _fmt_p(r2[\"p\"])\n",
    "        else:\n",
    "            row[4].text = \"—\"; row[5].text = \"—\"\n",
    "\n",
    "        # ---- Model 3：分别 + 三种 WMH（来自 “Model 2 (+WMH)”）\n",
    "        for fam in wmh_order:\n",
    "            r3 = _get_row(cohort, endpoint, \"Model 2 (+WMH)\", fam)\n",
    "            row = tbl.add_row().cells\n",
    "            row[0].text = str(cohort)\n",
    "            row[1].text = str(endpoint)\n",
    "            row[2].text = n_str\n",
    "            row[3].text = f\"Model 3 (+{fam})\"\n",
    "            if r3 is not None:\n",
    "                row[4].text = _fmt_beta_ci(r3[\"Beta\"], r3[\"CI_lower\"], r3[\"CI_upper\"])\n",
    "                row[5].text = _fmt_p(r3[\"p\"])\n",
    "            else:\n",
    "                row[4].text = \"—\"; row[5].text = \"—\"\n",
    "\n",
    "        # ---- Model 4：分别 + 三种 WMH + CMC（来自 “Model 4 (+WMH+CMC)”）\n",
    "        for fam in wmh_order:\n",
    "            r4 = _get_row(cohort, endpoint, \"Model 4 (+WMH+CMC)\", fam)\n",
    "            row = tbl.add_row().cells\n",
    "            row[0].text = str(cohort)\n",
    "            row[1].text = str(endpoint)\n",
    "            row[2].text = n_str\n",
    "            row[3].text = f\"Model 4 (+{fam} + CMC)\"\n",
    "            if r4 is not None:\n",
    "                row[4].text = _fmt_beta_ci(r4[\"Beta\"], r4[\"CI_lower\"], r4[\"CI_upper\"])\n",
    "                row[5].text = _fmt_p(r4[\"p\"])\n",
    "            else:\n",
    "                row[4].text = \"—\"; row[5].text = \"—\"\n",
    "\n",
    "    # 右对齐数值列\n",
    "    _right_align_numeric(tbl, [2,4,5])\n",
    "    # 三线样式（表头上下 + 全表底线）\n",
    "    _apply_three_line_table(tbl, header_row_idx=0)\n",
    "\n",
    "    # 备注\n",
    "    leg = doc.add_paragraph()\n",
    "    leg.add_run(legend).italic = True\n",
    "\n",
    "    doc.save(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_tmtb_noncompletion(df: pd.DataFrame,\n",
    "                               cohort_label: str,\n",
    "                               subdir: str,\n",
    "                               cmc_var: str,\n",
    "                               cmc_is_cat: bool):\n",
    "    \"\"\"\n",
    "    For a given cohort dataframe, run logistic models for TMT-B non-completion.\n",
    "\n",
    "    Outcome:\n",
    "        TMTB_noncomp = 1 if Duration_to_complete_alphanumeric_path_trail_2 == 0\n",
    "                       0 if > 0\n",
    "        Rows with missing TMT-B duration are excluded.\n",
    "\n",
    "    Models per WMH family:\n",
    "        Model 1 (Base)              : group + base covariates\n",
    "        Model 2 (+WMH)              : Model 1 + that WMH family\n",
    "        Model 3 (+CMC)              : Model 1 + CMC\n",
    "        Model 4 (+WMH+CMC)          : Model 1 + that WMH family + CMC\n",
    "\n",
    "    Effect:\n",
    "        Study vs Control association with non-completion:\n",
    "        log(OR), OR, 95% CI, p-value.\n",
    "    \"\"\"\n",
    "    if TM_COL not in df.columns:\n",
    "        print(f\"[{cohort_label}] TMT-B column missing; skip non-completion analysis.\")\n",
    "        return None\n",
    "\n",
    "    d = df.copy()\n",
    "    tm_raw = pd.to_numeric(d[TM_COL], errors=\"coerce\")\n",
    "\n",
    "    # Define binary outcome: 1 = not completed (0 sec), 0 = completed (>0)\n",
    "    d[\"TMTB_noncomp\"] = np.where(tm_raw == 0, 1,\n",
    "                          np.where(tm_raw > 0, 0, np.nan))\n",
    "\n",
    "    # Keep rows with defined outcome and group\n",
    "    d = d[~d[\"TMTB_noncomp\"].isna()]\n",
    "    d = d.dropna(subset=[GROUP_COL])\n",
    "    if d.empty:\n",
    "        print(f\"[{cohort_label}] No TMT-B non-completion data.\")\n",
    "        return None\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for wmh_var, wmh_label, _tag in WMH_PANELS:\n",
    "        if wmh_var not in d.columns:\n",
    "            print(f\"[{cohort_label}] Missing {wmh_var}; skip {wmh_label} in TMT-B non-completion.\")\n",
    "            continue\n",
    "\n",
    "        for model_label, model_id in MODEL_SPECS:\n",
    "            # Start from base covariates\n",
    "            covs = [v for v in BASE_COVARIATES if v in d.columns]\n",
    "            cats = set(CATEGORICAL_VARS) & set(covs)\n",
    "\n",
    "            # +WMH terms for models 2 & 4\n",
    "            if model_id in (2, 4):\n",
    "                covs = covs + [wmh_var]\n",
    "\n",
    "            # +CMC terms for models 3 & 4\n",
    "            if model_id in (3, 4) and cmc_var is not None:\n",
    "                covs = covs + [cmc_var]\n",
    "                if cmc_is_cat:\n",
    "                    cats.add(cmc_var)\n",
    "\n",
    "            # Impute covariates (outcome and group are not imputed here)\n",
    "            d_model = impute_covariates(d, covs, cats)\n",
    "\n",
    "            # Build logistic formula\n",
    "            formula = build_formula(\"TMTB_noncomp\", covs, cats)\n",
    "\n",
    "            # Fit clustered logistic\n",
    "            res, se_info, n_clu, n_obs = fit_cluster_logit(formula, d_model, CLUSTER_VAR)\n",
    "            if res is None:\n",
    "                continue\n",
    "\n",
    "            # Extract Study vs Control effect\n",
    "            coef_name = find_group_coef(res.params.index, GROUP_COL, \"Study\")\n",
    "            if coef_name is None or coef_name not in res.params.index:\n",
    "                continue\n",
    "\n",
    "            beta = float(res.params[coef_name])  # log(OR)\n",
    "            ci_l, ci_u = [float(x) for x in res.conf_int().loc[coef_name]]\n",
    "            pval = float(res.pvalues[coef_name])\n",
    "\n",
    "            rows.append({\n",
    "                \"Cohort\": cohort_label,\n",
    "                \"WMH_family\": wmh_label,\n",
    "                \"Model\": model_label,\n",
    "                \"log_OR\": beta,\n",
    "                \"log_OR_CI_lower\": ci_l,\n",
    "                \"log_OR_CI_upper\": ci_u,\n",
    "                \"OR\": np.exp(beta),\n",
    "                \"OR_CI_lower\": np.exp(ci_l),\n",
    "                \"OR_CI_upper\": np.exp(ci_u),\n",
    "                \"p\": pval,\n",
    "                \"SE_type\": se_info,\n",
    "                \"n_obs\": n_obs,\n",
    "                \"n_clusters\": n_clu,\n",
    "            })\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    if res_df.empty:\n",
    "        print(f\"[{cohort_label}] No TMT-B non-completion logistic results.\")\n",
    "        return None\n",
    "\n",
    "    out_path = os.path.join(subdir, \"TMTB_NonCompletion_Logistic_Results.csv\")\n",
    "    res_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[{cohort_label}] TMT-B non-completion logistic results saved: {out_path}\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "# ========= Forest plot (3 panels; A/B/C) =========\n",
    "\n",
    "def plot_cog_three_panel(df: pd.DataFrame,\n",
    "                         cohort_label: str,\n",
    "                         outdir: str):\n",
    "    \"\"\"\n",
    "    Three-panel forest plot for cognitive outcomes by WMH family.\n",
    "\n",
    "    Panels:\n",
    "        A = Total WMH\n",
    "        B = PWMH\n",
    "        C = DWMH\n",
    "\n",
    "    Endpoint display order (top → bottom), restricted to those present:\n",
    "        Reaction time, Trail making test-B, Fluid intelligence, Digit span\n",
    "\n",
    "    Legend is placed at the bottom center.\n",
    "    Panel letters are fixed at the top-left corner of each subplot.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Restrict to the selected cohort\n",
    "    df = df[df[\"Cohort\"] == cohort_label].copy()\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Full desired endpoint order (labels)\n",
    "    endpoint_order_full = [label for _, label in COG_ENDPOINTS]\n",
    "\n",
    "    # Keep only endpoints that actually appear in the results for this cohort\n",
    "    endpoint_order = [\n",
    "        ep for ep in endpoint_order_full\n",
    "        if (df[\"Endpoint\"] == ep).any()\n",
    "    ]\n",
    "    if not endpoint_order:\n",
    "        return\n",
    "\n",
    "    model_order = [m for m, _ in MODEL_SPECS]\n",
    "    wmh_order = [fam for _, fam, _ in WMH_PANELS]\n",
    "    panel_tag = {fam: tag for _, fam, tag in WMH_PANELS}\n",
    "\n",
    "    # --------- Build vertical positions (y) ----------\n",
    "    # Each endpoint forms a block of len(model_order) rows, with a small gap between blocks.\n",
    "    n_models = len(model_order)\n",
    "    gap = 1.0  # vertical gap between endpoint blocks\n",
    "\n",
    "    n_blocks = len(endpoint_order)\n",
    "    rows_per_block = n_models\n",
    "    total_rows = n_blocks * rows_per_block + (n_blocks - 1) * gap\n",
    "\n",
    "    y_map = {}      # (endpoint, model) -> y\n",
    "    tick_pos = {}   # endpoint -> y center\n",
    "\n",
    "    current = total_rows\n",
    "    for ep_label in endpoint_order:\n",
    "        ys = []\n",
    "        for model_label in model_order:\n",
    "            current -= 1.0\n",
    "            y_map[(ep_label, model_label)] = current\n",
    "            ys.append(current)\n",
    "        tick_pos[ep_label] = float(np.mean(ys))\n",
    "        current -= gap\n",
    "\n",
    "    # --------- X-axis limits (from all CIs) ----------\n",
    "    xmin = float(df[\"CI_lower\"].min())\n",
    "    xmax = float(df[\"CI_upper\"].max())\n",
    "    xmin = min(xmin, 0.0)\n",
    "    xmax = max(xmax, 0.0)\n",
    "    span = xmax - xmin if xmax > xmin else 0.2\n",
    "    pad = 0.12 * span\n",
    "    xmin -= pad\n",
    "    xmax += pad\n",
    "\n",
    "    # Markers for models\n",
    "    markers = {\n",
    "        \"Model 1 (Base)\": \"o\",\n",
    "        \"Model 2 (+WMH)\": \"s\",\n",
    "        \"Model 3 (+CMC)\": \"^\",\n",
    "        \"Model 4 (+WMH+CMC)\": \"D\",\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(11, 4.0), sharey=True)\n",
    "\n",
    "    # --------- Draw each panel ----------\n",
    "    for ax, fam in zip(axes, wmh_order):\n",
    "        sub = df[df[\"WMH_family\"] == fam].copy()\n",
    "        if sub.empty:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # Draw in fixed endpoint/model order to guarantee alignment\n",
    "        for ep_label in endpoint_order:\n",
    "            for model_label in model_order:\n",
    "                row = sub[\n",
    "                    (sub[\"Endpoint\"] == ep_label) &\n",
    "                    (sub[\"Model\"] == model_label)\n",
    "                ]\n",
    "                if row.empty:\n",
    "                    continue\n",
    "\n",
    "                r = row.iloc[0]\n",
    "                beta = r[\"Beta\"]\n",
    "                lo = r[\"CI_lower\"]\n",
    "                hi = r[\"CI_upper\"]\n",
    "                y = y_map[(ep_label, model_label)]\n",
    "                m = markers[model_label]\n",
    "\n",
    "                ax.errorbar(\n",
    "                    beta, y,\n",
    "                    xerr=[[beta - lo], [hi - beta]],\n",
    "                    fmt=m,\n",
    "                    mfc=\"#1f3b4d\",\n",
    "                    mec=\"#1f3b4d\",\n",
    "                    ecolor=\"#1f3b4d\",\n",
    "                    elinewidth=1.6,\n",
    "                    capsize=4,\n",
    "                    markersize=5.8,\n",
    "                    linestyle=\"none\",\n",
    "                    zorder=3,\n",
    "                )\n",
    "\n",
    "        # Zero line\n",
    "        ax.axvline(0, color=\"#9e9e9e\", linestyle=\"--\", linewidth=1.0)\n",
    "\n",
    "        # Axes style\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(0, total_rows + 0.5)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.grid(False)\n",
    "\n",
    "        # WMH family title (respecting global rcParams sizes)\n",
    "        ax.set_title(fam, pad=8)\n",
    "\n",
    "        # Panel letter (A/B/C) at top-left (axes coordinates)\n",
    "        tag = panel_tag.get(fam, \"\")\n",
    "        ax.text(\n",
    "            0.02, 0.98,\n",
    "            tag,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "            ha=\"left\",\n",
    "        )\n",
    "\n",
    "    # --------- Y-axis ticks & labels ----------\n",
    "    # Only on the leftmost panel; positions match block centers.\n",
    "    axes[0].set_yticks([tick_pos[ep] for ep in endpoint_order])\n",
    "    axes[0].set_yticklabels(endpoint_order)\n",
    "    axes[0].set_ylabel(\"Cognitive endpoints\", labelpad=8)\n",
    "\n",
    "    # Hide y tick labels on other panels, but keep shared scale\n",
    "    for ax in axes[1:]:\n",
    "        ax.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # --------- Global X label & Legend (bottom) ----------\n",
    "    try:\n",
    "        fig.supxlabel(\"β (SD units)  Study vs Control\", y=0.10)\n",
    "    except Exception:\n",
    "        fig.text(\n",
    "            0.5, 0.10,\n",
    "            \"β (SD units)  Study vs Control\",\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "    handles = [\n",
    "        plt.Line2D(\n",
    "            [0], [0],\n",
    "            marker=markers[m],\n",
    "            linestyle=\"none\",\n",
    "            markerfacecolor=\"#1f3b4d\",\n",
    "            markeredgecolor=\"#1f3b4d\",\n",
    "            markersize=6,\n",
    "            label=m,\n",
    "        )\n",
    "        for m in model_order\n",
    "    ]\n",
    "\n",
    "    fig.legend(\n",
    "        handles=handles,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, 0.03),\n",
    "        ncol=4,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    # Layout: leave space for left labels and bottom legend\n",
    "    plt.subplots_adjust(\n",
    "        left=0.18,\n",
    "        right=0.98,\n",
    "        top=0.82,\n",
    "        bottom=0.22,\n",
    "        wspace=0.22,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", cohort_label).strip(\"_\")\n",
    "    base = os.path.join(outdir, f\"{safe}_Cognitive_Forest_3Panel\")\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_cog_four_panel_by_endpoint(df: pd.DataFrame,\n",
    "                                    cohort_label: str,\n",
    "                                    outdir: str):\n",
    "    \"\"\"\n",
    "    Four-panel forest plot by cognitive endpoint, single cohort.\n",
    "\n",
    "    For each endpoint (one panel):\n",
    "        Y-axis rows (top -> bottom):\n",
    "            Base          : Model 1 (Base)\n",
    "            Base + CMC    : Model 3 (+CMC)\n",
    "            Total WMH     : Model 2 (+WMH) and Model 4 (+WMH+CMC) for Total WMH\n",
    "            PWMH          : Model 2 (+WMH) and Model 4 (+WMH+CMC) for PWMH\n",
    "            DWMH          : Model 2 (+WMH) and Model 4 (+WMH+CMC) for DWMH\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Restrict to this cohort\n",
    "    df = df[df[\"Cohort\"] == cohort_label].copy()\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Endpoint order (labels from config)\n",
    "    endpoint_order = [label for _, label in COG_ENDPOINTS]\n",
    "    endpoint_order = [ep for ep in endpoint_order if (df[\"Endpoint\"] == ep).any()]\n",
    "    if not endpoint_order:\n",
    "        return\n",
    "\n",
    "    # WMH families\n",
    "    wmh_order = [fam for _, fam, _ in WMH_PANELS]\n",
    "\n",
    "    # X-axis limits across all models for this cohort\n",
    "    xmin = float(df[\"CI_lower\"].min())\n",
    "    xmax = float(df[\"CI_upper\"].max())\n",
    "    xmin = min(xmin, 0.0)\n",
    "    xmax = max(xmax, 0.0)\n",
    "    span = xmax - xmin if xmax > xmin else 0.2\n",
    "    pad = 0.12 * span\n",
    "    xmin -= pad\n",
    "    xmax += pad\n",
    "\n",
    "    # Marker shapes by underlying model label\n",
    "    markers = {\n",
    "        \"Model 1 (Base)\": \"o\",\n",
    "        \"Model 3 (+CMC)\": \"^\",  # displayed as Model 2 (Base + CMC)\n",
    "        \"Model 2 (+WMH)\": \"s\",  # displayed as Model 3 (Base + WMH)\n",
    "        \"Model 4 (+WMH+CMC)\": \"D\",\n",
    "    }\n",
    "\n",
    "    # Vertical offsets for WMH rows so two models don't overlap\n",
    "    offsets = {\n",
    "        \"Model 1 (Base)\": 0.0,\n",
    "        \"Model 3 (+CMC)\": 0.0,\n",
    "        \"Model 2 (+WMH)\": +0.10,\n",
    "        \"Model 4 (+WMH+CMC)\": -0.10,\n",
    "    }\n",
    "\n",
    "    # Fixed row layout\n",
    "    row_labels = [\"Base\", \"Base + CMC\", \"Total WMH\", \"PWMH\", \"DWMH\"]\n",
    "    y_positions = {\n",
    "        \"Base\": 5.0,\n",
    "        \"Base + CMC\": 4.0,\n",
    "        \"Total WMH\": 3.0,\n",
    "        \"PWMH\": 2.0,\n",
    "        \"DWMH\": 1.0,\n",
    "    }\n",
    "\n",
    "    # Figure and axes (2x2 panels)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 7), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    panel_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    for ax, ep_label, panel_letter in zip(axes, endpoint_order, panel_letters):\n",
    "        sub_ep = df[df[\"Endpoint\"] == ep_label].copy()\n",
    "        if sub_ep.empty:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # ----- Base row: Model 1 (Base) -----\n",
    "        m1 = sub_ep[sub_ep[\"Model\"] == \"Model 1 (Base)\"]\n",
    "        if not m1.empty:\n",
    "            r = m1.iloc[0]\n",
    "            beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "            y = y_positions[\"Base\"] + offsets[\"Model 1 (Base)\"]\n",
    "            ax.errorbar(\n",
    "                beta, y,\n",
    "                xerr=[[beta - lo], [hi - beta]],\n",
    "                fmt=markers[\"Model 1 (Base)\"],\n",
    "                mfc=\"#1f3b4d\", mec=\"#1f3b4d\",\n",
    "                ecolor=\"#1f3b4d\",\n",
    "                elinewidth=1.5, capsize=4,\n",
    "                markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "            )\n",
    "\n",
    "        # ----- Base + CMC row: underlying \"Model 3 (+CMC)\" -----\n",
    "        m3 = sub_ep[sub_ep[\"Model\"] == \"Model 3 (+CMC)\"]\n",
    "        if not m3.empty:\n",
    "            r = m3.iloc[0]\n",
    "            beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "            y = y_positions[\"Base + CMC\"] + offsets[\"Model 3 (+CMC)\"]\n",
    "            ax.errorbar(\n",
    "                beta, y,\n",
    "                xerr=[[beta - lo], [hi - beta]],\n",
    "                fmt=markers[\"Model 3 (+CMC)\"],\n",
    "                mfc=\"#1f3b4d\", mec=\"#1f3b4d\",\n",
    "                ecolor=\"#1f3b4d\",\n",
    "                elinewidth=1.5, capsize=4,\n",
    "                markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "            )\n",
    "\n",
    "\n",
    "        # ----- WMH rows: underlying Model 2 (+WMH) and Model 4 (+WMH+CMC) -----\n",
    "        for wmh_label in wmh_order:\n",
    "            base_y = y_positions[wmh_label]\n",
    "\n",
    "            # +WMH\n",
    "            m2 = sub_ep[\n",
    "                (sub_ep[\"WMH_family\"] == wmh_label) &\n",
    "                (sub_ep[\"Model\"] == \"Model 2 (+WMH)\")\n",
    "            ]\n",
    "            if not m2.empty:\n",
    "                r = m2.iloc[0]\n",
    "                beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "                y = base_y + offsets[\"Model 2 (+WMH)\"]\n",
    "                ax.errorbar(\n",
    "                    beta, y,\n",
    "                    xerr=[[beta - lo], [hi - beta]],\n",
    "                    fmt=markers[\"Model 2 (+WMH)\"],\n",
    "                    mfc=\"#1f3b4d\", mec=\"#1f3b4d\",\n",
    "                    ecolor=\"#1f3b4d\",\n",
    "                    elinewidth=1.5, capsize=4,\n",
    "                    markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "                )\n",
    "\n",
    "            # +WMH+CMC\n",
    "            m4 = sub_ep[\n",
    "                (sub_ep[\"WMH_family\"] == wmh_label) &\n",
    "                (sub_ep[\"Model\"] == \"Model 4 (+WMH+CMC)\")\n",
    "            ]\n",
    "            if not m4.empty:\n",
    "                r = m4.iloc[0]\n",
    "                beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "                y = base_y + offsets[\"Model 4 (+WMH+CMC)\"]\n",
    "                ax.errorbar(\n",
    "                    beta, y,\n",
    "                    xerr=[[beta - lo], [hi - beta]],\n",
    "                    fmt=markers[\"Model 4 (+WMH+CMC)\"],\n",
    "                    mfc=\"#1f3b4d\", mec=\"#1f3b4d\",\n",
    "                    ecolor=\"#1f3b4d\",\n",
    "                    elinewidth=1.5, capsize=4,\n",
    "                    markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "                )\n",
    "\n",
    "        # Zero line\n",
    "        ax.axvline(0, color=\"#9e9e9e\", linestyle=\"--\", linewidth=1.0, zorder=1)\n",
    "\n",
    "        # Y-axis labels\n",
    "        ax.set_yticks([y_positions[lbl] for lbl in row_labels])\n",
    "        ax.set_yticklabels(row_labels)\n",
    "\n",
    "        # Axes style\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(0.5, 5.5)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.grid(False)\n",
    "\n",
    "        # Panel title and letter\n",
    "        ax.set_title(ep_label, pad=6, fontweight=\"bold\")\n",
    "        ax.text(\n",
    "            -0.15, 0.99,\n",
    "            panel_letter,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "            ha=\"right\",\n",
    "            clip_on=False,\n",
    "        )\n",
    "\n",
    "    # Hide unused axes if <4 endpoints\n",
    "    if len(endpoint_order) < 4:\n",
    "        for j in range(len(endpoint_order), 4):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "    # Bottom x-label\n",
    "    try:\n",
    "        fig.supxlabel(\"β (SD units)  SA vs Control\", y= -0.045)\n",
    "    except Exception:\n",
    "        fig.text(0.5, -0.045, \"β (SD units)  SA vs Control\", ha=\"center\")\n",
    "\n",
    "    # Legend mapping (display only)\n",
    "    display_label_map = {\n",
    "        \"Model 1 (Base)\": \"Model 1 (Base)\",\n",
    "        \"Model 3 (+CMC)\": \"Model 2 (Base + CMC)\",\n",
    "        \"Model 2 (+WMH)\": \"Model 3 (Base + WMH)\",\n",
    "        \"Model 4 (+WMH+CMC)\": \"Model 4 (Base + WMH + CMC)\",\n",
    "    }\n",
    "\n",
    "    legend_order = [\n",
    "        \"Model 1 (Base)\",\n",
    "        \"Model 3 (+CMC)\",\n",
    "        \"Model 2 (+WMH)\",\n",
    "        \"Model 4 (+WMH+CMC)\",\n",
    "    ]\n",
    "\n",
    "    handles = []\n",
    "    for key in legend_order:\n",
    "        handles.append(\n",
    "            plt.Line2D(\n",
    "                [0], [0],\n",
    "                marker=markers[key],\n",
    "                linestyle=\"none\",\n",
    "                markerfacecolor=\"#1f3b4d\",\n",
    "                markeredgecolor=\"#1f3b4d\",\n",
    "                markersize=5.5,\n",
    "                label=display_label_map[key],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.legend(\n",
    "        handles=handles,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.01),\n",
    "        ncol=4,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left=0.10,\n",
    "        right=0.98,\n",
    "        top=0.90,\n",
    "        bottom=0.24,\n",
    "        wspace=0.30,\n",
    "        hspace=0.30,\n",
    "    )\n",
    "\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", cohort_label).strip(\"_\")\n",
    "    base = os.path.join(outdir, f\"{safe}_Cognitive_Forest_4Panel_ByEndpoint\")\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_cog_four_panel_by_endpoint_combined(df: pd.DataFrame,\n",
    "                                             primary_cohort: str,\n",
    "                                             sensitivity_cohort: str,\n",
    "                                             outdir: str):\n",
    "    \"\"\"\n",
    "    Four-panel forest plot by cognitive endpoint, combining\n",
    "    primary and sensitivity cohorts.\n",
    "\n",
    "    - One panel per cognitive endpoint.\n",
    "    - Y-axis rows (top -> bottom):\n",
    "        Base\n",
    "        Base + CMC\n",
    "        Total WMH\n",
    "        PWMH\n",
    "        DWMH\n",
    "    - Primary cohort: deep blue markers.\n",
    "    - Sensitivity cohort: grey markers.\n",
    "    - Same marker shapes for the same model.\n",
    "    - Legend layout:\n",
    "        Row 1 (blue markers): model definitions.\n",
    "        Row 2 (colored squares): cohort definitions.\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    df_primary = df[df[\"Cohort\"] == primary_cohort].copy()\n",
    "    df_sens = df[df[\"Cohort\"] == sensitivity_cohort].copy()\n",
    "\n",
    "    if df_primary.empty or df_sens.empty:\n",
    "        print(\"[Combined plot] Missing one of the cohorts; skip combined figure.\")\n",
    "        return\n",
    "\n",
    "    # Endpoint order\n",
    "    endpoint_order = [label for _, label in COG_ENDPOINTS]\n",
    "    endpoint_order = [ep for ep in endpoint_order if (df[\"Endpoint\"] == ep).any()]\n",
    "    if not endpoint_order:\n",
    "        return\n",
    "\n",
    "    # WMH families\n",
    "    wmh_order = [fam for _, fam, _ in WMH_PANELS]\n",
    "\n",
    "    # X-axis limits across both cohorts\n",
    "    xmin = float(df[\"CI_lower\"].min())\n",
    "    xmax = float(df[\"CI_upper\"].max())\n",
    "    xmin = min(xmin, 0.0)\n",
    "    xmax = max(xmax, 0.0)\n",
    "    span = xmax - xmin if xmax > xmin else 0.2\n",
    "    pad = 0.12 * span\n",
    "    xmin -= pad\n",
    "    xmax += pad\n",
    "\n",
    "    # Colors\n",
    "    primary_color = \"#1f3b4d\"\n",
    "    sens_color = \"#b3b3b3\"\n",
    "\n",
    "    # Marker shapes (by underlying model label)\n",
    "    markers = {\n",
    "        \"Model 1 (Base)\": \"o\",\n",
    "        \"Model 3 (+CMC)\": \"^\",\n",
    "        \"Model 2 (+WMH)\": \"s\",\n",
    "        \"Model 4 (+WMH+CMC)\": \"D\",\n",
    "    }\n",
    "\n",
    "    # Vertical offsets to separate +WMH and +WMH+CMC within each WMH row\n",
    "    offsets = {\n",
    "        \"Model 1 (Base)\": 0.0,\n",
    "        \"Model 3 (+CMC)\": 0.0,\n",
    "        \"Model 2 (+WMH)\": +0.10,\n",
    "        \"Model 4 (+WMH+CMC)\": -0.10,\n",
    "    }\n",
    "\n",
    "    # Y rows\n",
    "    row_labels = [\"Base\", \"Base + CMC\", \"Total WMH\", \"PWMH\", \"DWMH\"]\n",
    "    y_positions = {\n",
    "        \"Base\": 5.0,\n",
    "        \"Base + CMC\": 4.0,\n",
    "        \"Total WMH\": 3.0,\n",
    "        \"PWMH\": 2.0,\n",
    "        \"DWMH\": 1.0,\n",
    "    }\n",
    "\n",
    "    def get_row(sub_df, model_label, endpoint_label, wmh_label=None):\n",
    "        q = (sub_df[\"Endpoint\"] == endpoint_label) & (sub_df[\"Model\"] == model_label)\n",
    "        if wmh_label is not None:\n",
    "            q &= (sub_df[\"WMH_family\"] == wmh_label)\n",
    "        rows = sub_df[q]\n",
    "        if rows.empty:\n",
    "            return None\n",
    "        return rows.iloc[0]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 7), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    panel_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    for ax, ep_label, panel_letter in zip(axes, endpoint_order, panel_letters):\n",
    "        sub_p = df_primary[df_primary[\"Endpoint\"] == ep_label].copy()\n",
    "        sub_s = df_sens[df_sens[\"Endpoint\"] == ep_label].copy()\n",
    "\n",
    "        if sub_p.empty and sub_s.empty:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # ---- Base (Model 1) ----\n",
    "        for sub_df, color in ((df_sens, sens_color), (df_primary, primary_color)):\n",
    "            r = get_row(sub_df, \"Model 1 (Base)\", ep_label)\n",
    "            if r is not None:\n",
    "                beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "                y = y_positions[\"Base\"] + offsets[\"Model 1 (Base)\"]\n",
    "                ax.errorbar(\n",
    "                    beta, y,\n",
    "                    xerr=[[beta - lo], [hi - beta]],\n",
    "                    fmt=markers[\"Model 1 (Base)\"],\n",
    "                    mfc=color, mec=color,\n",
    "                    ecolor=color,\n",
    "                    elinewidth=1.5, capsize=4,\n",
    "                    markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "                )\n",
    "\n",
    "        # ---- Base + CMC (Model 3 (+CMC)) ----\n",
    "        for sub_df, color in ((df_sens, sens_color), (df_primary, primary_color)):\n",
    "            r = get_row(sub_df, \"Model 3 (+CMC)\", ep_label)\n",
    "            if r is not None:\n",
    "                beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "                y = y_positions[\"Base + CMC\"] + offsets[\"Model 3 (+CMC)\"]\n",
    "                ax.errorbar(\n",
    "                    beta, y,\n",
    "                    xerr=[[beta - lo], [hi - beta]],\n",
    "                    fmt=markers[\"Model 3 (+CMC)\"],\n",
    "                    mfc=color, mec=color,\n",
    "                    ecolor=color,\n",
    "                    elinewidth=1.5, capsize=4,\n",
    "                    markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "                )\n",
    "\n",
    "        # ---- WMH rows: +WMH & +WMH+CMC ----\n",
    "        for wmh_label in wmh_order:\n",
    "            base_y = y_positions[wmh_label]\n",
    "\n",
    "            # +WMH (Model 2)\n",
    "            for sub_df, color in ((df_sens, sens_color), (df_primary, primary_color)):\n",
    "                r = get_row(sub_df, \"Model 2 (+WMH)\", ep_label, wmh_label)\n",
    "                if r is not None:\n",
    "                    beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "                    y = base_y + offsets[\"Model 2 (+WMH)\"]\n",
    "                    ax.errorbar(\n",
    "                        beta, y,\n",
    "                        xerr=[[beta - lo], [hi - beta]],\n",
    "                        fmt=markers[\"Model 2 (+WMH)\"],\n",
    "                        mfc=color, mec=color,\n",
    "                        ecolor=color,\n",
    "                        elinewidth=1.5, capsize=4,\n",
    "                        markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "                    )\n",
    "\n",
    "            # +WMH + CMC (Model 4)\n",
    "            for sub_df, color in ((df_sens, sens_color), (df_primary, primary_color)):\n",
    "                r = get_row(sub_df, \"Model 4 (+WMH+CMC)\", ep_label, wmh_label)\n",
    "                if r is not None:\n",
    "                    beta, lo, hi = r[\"Beta\"], r[\"CI_lower\"], r[\"CI_upper\"]\n",
    "                    y = base_y + offsets[\"Model 4 (+WMH+CMC)\"]\n",
    "                    ax.errorbar(\n",
    "                        beta, y,\n",
    "                        xerr=[[beta - lo], [hi - beta]],\n",
    "                        fmt=markers[\"Model 4 (+WMH+CMC)\"],\n",
    "                        mfc=color, mec=color,\n",
    "                        ecolor=color,\n",
    "                        elinewidth=1.5, capsize=4,\n",
    "                        markersize=5.5, linestyle=\"none\", zorder=3,\n",
    "                    )\n",
    "\n",
    "        # Axis style\n",
    "        ax.axvline(0, color=\"#9e9e9e\", linestyle=\"--\", linewidth=1.0, zorder=1)\n",
    "        ax.set_yticks([y_positions[lbl] for lbl in row_labels])\n",
    "        ax.set_yticklabels(row_labels)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(0.5, 5.5)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.grid(False)\n",
    "\n",
    "        ax.set_title(ep_label, pad=6, fontweight=\"bold\")\n",
    "        ax.text(\n",
    "            -0.15, 0.99,\n",
    "            panel_letter,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "            ha=\"right\",\n",
    "            clip_on=False,\n",
    "        )\n",
    "\n",
    "    # Hide unused axes if <4 endpoints\n",
    "    if len(endpoint_order) < 4:\n",
    "        for j in range(len(endpoint_order), 4):\n",
    "            axes[j].axis(\"off\")\n",
    "\n",
    "    # ----- Global x-label + legends (fixed vertical layout) -----\n",
    "\n",
    "    plt.subplots_adjust(\n",
    "        left=0.10,\n",
    "        right=0.98,\n",
    "        top=0.90,\n",
    "        bottom=0.25,\n",
    "        wspace=0.30,\n",
    "        hspace=0.30,\n",
    "    )\n",
    "\n",
    "    # β label\n",
    "    fig.text(\n",
    "        0.5, -0.005,\n",
    "        \"β (SD units)  SA vs Control\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "    # First legend: models (blue markers)\n",
    "    model_handles = [\n",
    "        plt.Line2D([0], [0], marker=markers[\"Model 1 (Base)\"], linestyle=\"none\",\n",
    "                   markerfacecolor=primary_color, markeredgecolor=primary_color,\n",
    "                   markersize=5.5, label=\"Model 1 (Base)\"),\n",
    "        plt.Line2D([0], [0], marker=markers[\"Model 3 (+CMC)\"], linestyle=\"none\",\n",
    "                   markerfacecolor=primary_color, markeredgecolor=primary_color,\n",
    "                   markersize=5.5, label=\"Model 2 (+ CMC)\"),\n",
    "        plt.Line2D([0], [0], marker=markers[\"Model 2 (+WMH)\"], linestyle=\"none\",\n",
    "                   markerfacecolor=primary_color, markeredgecolor=primary_color,\n",
    "                   markersize=5.5, label=\"Model 3 (+ WMH)\"),\n",
    "        plt.Line2D([0], [0], marker=markers[\"Model 4 (+WMH+CMC)\"], linestyle=\"none\",\n",
    "                   markerfacecolor=primary_color, markeredgecolor=primary_color,\n",
    "                   markersize=5.5, label=\"Model 4 (+ WMH + CMC)\"),\n",
    "    ]\n",
    "    leg1 = fig.legend(\n",
    "        handles=model_handles,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.075),\n",
    "        ncol=4,\n",
    "        frameon=False,\n",
    "    )\n",
    "    fig.add_artist(leg1)\n",
    "\n",
    "    # Second legend: cohorts (color blocks)\n",
    "    color_handles = [\n",
    "        plt.Line2D([0], [0], marker=\"o\", linestyle=\"none\",\n",
    "                   markerfacecolor=primary_color, markeredgecolor=primary_color,\n",
    "                   markersize=5.5, label=\"Primary PSM cohort\"),\n",
    "        plt.Line2D([0], [0], marker=\"o\", linestyle=\"none\",\n",
    "                   markerfacecolor=\"#b3b3b3\", markeredgecolor=\"#b3b3b3\",\n",
    "                   markersize=5.5, label=\"PSM–sensitivity cohort\"),\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles=color_handles,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.11),\n",
    "        ncol=2,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    base = os.path.join(\n",
    "        outdir,\n",
    "        \"Primary_Sensitivity_Combined_Cognitive_Forest_4Panel_ByEndpoint\"\n",
    "    )\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "# ======================== MAIN PIPELINE ========================\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for cohort_label, fp in COHORT_FILES.items():\n",
    "    if not os.path.exists(fp):\n",
    "        print(f\"[Warning] File not found: {fp} (skip {cohort_label})\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {cohort_label} ===\")\n",
    "    subdir = os.path.join(ROOT_DIR, re.sub(r'[^0-9A-Za-z]+', '_', cohort_label).strip(\"_\"))\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    df = clean_columns(df)\n",
    "    df = ensure_group(df)\n",
    "    df = build_wmh_covariates(df)\n",
    "    df = prepare_cognitive_scores(df, cohort_label)\n",
    "\n",
    "    cmc_var, cmc_is_cat = choose_cmc(df)\n",
    "\n",
    "    # Logistic analysis for TMT-B non-completion (0 sec vs >0 sec)\n",
    "    analyze_tmtb_noncompletion(df, cohort_label, subdir, cmc_var, cmc_is_cat)\n",
    "\n",
    "    # Loop over WMH panels & models & endpoints\n",
    "    rows = []\n",
    "    for wmh_var, wmh_label, _tag in WMH_PANELS:\n",
    "        if wmh_var not in df.columns:\n",
    "            print(f\"[{cohort_label}] Missing {wmh_var}; skip {wmh_label} panel.\")\n",
    "            continue\n",
    "\n",
    "        for ep_col, ep_label in COG_ENDPOINTS:\n",
    "            # Subset to non-missing endpoint and group\n",
    "            d = df.copy()\n",
    "            d = d[~d[ep_col].isna()]\n",
    "            d = d.dropna(subset=[GROUP_COL])\n",
    "            if d.empty:\n",
    "                continue\n",
    "\n",
    "            # ---------- NEW: N as \"SA/Control\" (Study/Control) per endpoint ----------\n",
    "            # Counts are taken *after* endpoint filtering and group availability.\n",
    "            n_sa = int((d[GROUP_COL] == \"Study\").sum())\n",
    "            n_ctrl = int((d[GROUP_COL] == \"Control\").sum())\n",
    "            n_str = f\"{n_sa}/{n_ctrl}\"\n",
    "\n",
    "            for model_label, model_id in MODEL_SPECS:\n",
    "                covs = [v for v in BASE_COVARIATES if v in d.columns]\n",
    "                cats = set(CATEGORICAL_VARS) & set(covs)\n",
    "\n",
    "                if model_id in (2, 4):\n",
    "                    covs = covs + [wmh_var]\n",
    "                if model_id in (3, 4) and cmc_var is not None:\n",
    "                    covs = covs + [cmc_var]\n",
    "                    if cmc_is_cat:\n",
    "                        cats.add(cmc_var)\n",
    "\n",
    "                d_model = impute_covariates(d, covs, cats)\n",
    "\n",
    "                formula = build_formula(ep_col, covs, cats)\n",
    "                model, se_info, n_clu, n_obs = fit_cluster_ols(formula, d_model, CLUSTER_VAR)\n",
    "                if model is None:\n",
    "                    continue\n",
    "\n",
    "                coef_name = find_group_coef(model.params.index, GROUP_COL, \"Study\")\n",
    "                if coef_name is None or coef_name not in model.params.index:\n",
    "                    continue\n",
    "\n",
    "                beta = float(model.params[coef_name])\n",
    "                ci_l, ci_u = [float(x) for x in model.conf_int().loc[coef_name]]\n",
    "                pval = float(model.pvalues[coef_name])\n",
    "\n",
    "                rows.append({\n",
    "                    \"Cohort\": cohort_label,\n",
    "                    \"WMH_family\": wmh_label,\n",
    "                    \"Endpoint\": ep_label,\n",
    "                    \"Model\": model_label,\n",
    "                    \"N\": n_str,                 # <-- keep SA/Control here\n",
    "                    \"Beta\": beta,\n",
    "                    \"CI_lower\": ci_l,\n",
    "                    \"CI_upper\": ci_u,\n",
    "                    \"p\": pval,\n",
    "                    \"SE_type\": se_info,\n",
    "                    \"n_obs\": n_obs,\n",
    "                    \"n_clusters\": n_clu,\n",
    "                })\n",
    "\n",
    "    # ---- Build per-cohort results and apply FDR (PWMH/DWMH only) ----\n",
    "# After collecting results for one cohort:\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    if res_df.empty:\n",
    "        print(f\"[{cohort_label}] No cognitive results.\")\n",
    "        continue\n",
    "\n",
    "    # ---- Save per-cohort results ----\n",
    "    res_path = os.path.join(subdir, \"Cognitive_Results.csv\")\n",
    "    res_df.to_csv(res_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[{cohort_label}] CSV saved: {res_path}\")\n",
    "\n",
    "    # ---- Word table ----\n",
    "    legend_text = (\n",
    "        \"Legend: β coefficients represent standardized mean differences in cognitive performance \"\n",
    "        \"(Study vs Control) in SD units. Reaction time and Trail making test-B were log-transformed \"\n",
    "        \"and multiplied by −1 so that higher values indicate better performance. \"\n",
    "        \"Fluid intelligence and Digit span were standardized after excluding invalid codes \"\n",
    "        \"(-1 for Fluid intelligence and Digit span; 0 for Trail making). \"\n",
    "        \"WMH covariates are head-size–normalized and log1p-transformed (no z-score). \"\n",
    "        \"CMC covariate uses the categorical comorbidity score when available, otherwise the continuous score. \"\n",
    "        \"Models use cluster-robust standard errors by matched set (match_id) where feasible. \"\n",
    "        \"N is reported as SA/Control counts after endpoint-specific exclusions.\"\n",
    "    )\n",
    "    word_path = os.path.join(subdir, \"Cognitive_Results.docx\")\n",
    "    export_cog_table_word(res_df, word_path,\n",
    "                        title=f\"Table. Cognitive outcomes — {cohort_label}\",\n",
    "                        legend=legend_text)\n",
    "    print(f\"[{cohort_label}] Word table saved: {word_path}\")\n",
    "\n",
    "    # ---- Figures ----\n",
    "    plot_cog_three_panel(res_df, cohort_label, subdir)\n",
    "    plot_cog_four_panel_by_endpoint(res_df, cohort_label, subdir)\n",
    "\n",
    "\n",
    "    all_results.append(res_df)\n",
    "\n",
    "# ===== Combined outputs =====\n",
    "if all_results:\n",
    "    comb = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    comb_csv = os.path.join(ROOT_DIR, \"Cognitive_Results_AllCohorts_Combined.csv\")\n",
    "    comb.to_csv(comb_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Combined CSV saved: {comb_csv}\")\n",
    "\n",
    "    # Combined Word\n",
    "    comb_word = os.path.join(ROOT_DIR, \"Cognitive_Results_AllCohorts_Combined.docx\")\n",
    "    export_cog_table_word(\n",
    "        comb,\n",
    "        comb_word,\n",
    "        title=\"Table. Cognitive outcomes — Primary PSM and PSM–sensitivity cohorts\",\n",
    "        legend=legend_text\n",
    "    )\n",
    "    print(f\"Combined Word table saved: {comb_word}\")\n",
    "\n",
    "    # Combined Figure\n",
    "    plot_cog_four_panel_by_endpoint_combined(\n",
    "        comb,\n",
    "        primary_cohort=\"Primary PSM cohort\",\n",
    "        sensitivity_cohort=\"PSM–sensitivity cohort\",\n",
    "        outdir=ROOT_DIR,\n",
    "    )\n",
    "    print(\"Combined 4-panel cognitive figure saved.\")\n",
    "\n",
    "\n",
    "print(\"\\nCognitive secondary endpoint analysis completed.\")\n",
    "\n",
    "\n",
    "# ===== Cognitive Figure 3 legend (4-panel by endpoint, combined cohorts) =====\n",
    "\n",
    "cog_fig3_legend = (\n",
    "    \"Figure 3. Association of SA with cognitive outcomes in propensity-matched cohorts after additional adjustment for WMH burden and cardiovascular–metabolic conditions\\n\\n\"\n",
    "    \"Forest plots show adjusted differences in standardized cognitive outcomes (β, SD units; SA vs control) for four cognitive measures in the Primary PSM cohort (dark blue) \"\n",
    "    \"and the PSM–sensitivity cohort (light grey). Panels A–D display reaction time, Trail making test-B, fluid intelligence, and digit span, respectively. Within each panel, \"\n",
    "    \"rows correspond to the base model (Base), the base model additionally adjusted for cardiovascular and metabolic conditions (Base + CMC), and models including total WMH, \"\n",
    "    \"periventricular WMH (PWMH), or deep WMH (DWMH), with and without additional CMC adjustment. Circles, triangles, squares, and diamonds represent Model 1 (Base), \"\n",
    "    \"Model 2 (+ CMC), Model 3 (+ WMH), and Model 4 (+ WMH + CMC), respectively. All cognitive outcomes were standardized using z-scores; reaction time and Trail making test-B \"\n",
    "    \"were log-transformed before standardization, and all measures were coded so that higher values indicate better cognitive performance. The Base model was adjusted for \"\n",
    "    \"age at Instance 2, sex, Townsend deprivation index at recruitment, body mass index at baseline, genetic ethnic grouping, smoking status, alcohol intake frequency, \"\n",
    "    \"educational attainment (degree vs no degree), and APOE ε4 allele count. Positive β values therefore indicate better cognitive performance in participants with SA relative \"\n",
    "    \"to matched controls. Models including CMC further adjusted for cardiovascular and metabolic conditions, and WMH models additionally included the corresponding WMH measures. \"\n",
    "    \"All models used cluster-robust standard errors by matched set.\"\n",
    ")\n",
    "\n",
    "# TXT version\n",
    "cog_fig3_txt_path = os.path.join(ROOT_DIR, \"Cognitive_Figure3_Legend.txt\")\n",
    "with open(cog_fig3_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cog_fig3_legend)\n",
    "print(f\"Cognitive Figure 3 legend (TXT) saved: {cog_fig3_txt_path}\")\n",
    "\n",
    "# Word version (if python-docx is available)\n",
    "if Document is not None:\n",
    "    doc = Document()\n",
    "    try:\n",
    "        doc.styles[\"Normal\"].font.name = \"Times New Roman\"\n",
    "        doc.styles[\"Normal\"].font.size = Pt(10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Title line in bold\n",
    "    p_title = doc.add_paragraph()\n",
    "    r_title = p_title.add_run(\n",
    "        \"Figure 3. Association of SA with cognitive outcomes in propensity-matched cohorts after additional adjustment for WMH burden and cardiovascular–metabolic conditions\"\n",
    "    )\n",
    "    r_title.bold = True\n",
    "\n",
    "    # Main legend text\n",
    "    doc.add_paragraph(\n",
    "        \"Forest plots show adjusted differences in standardized cognitive outcomes (β, SD units; SA vs control) for four cognitive measures in the Primary PSM cohort (dark blue) \"\n",
    "        \"and the PSM–sensitivity cohort (light grey). Panels A–D display reaction time, Trail making test-B, fluid intelligence, and digit span, respectively. Within each panel, \"\n",
    "        \"rows correspond to the base model (Base), the base model additionally adjusted for cardiovascular and metabolic conditions (Base + CMC), and models including total WMH, \"\n",
    "        \"periventricular WMH (PWMH), or deep WMH (DWMH), with and without additional CMC adjustment. Circles, triangles, squares, and diamonds represent Model 1 (Base), \"\n",
    "        \"Model 2 (+ CMC), Model 3 (+ WMH), and Model 4 (+ CMC + WMH), respectively. All cognitive outcomes were standardized using z-scores; reaction time and Trail making test-B \"\n",
    "        \"were log-transformed before standardization, and all measures were coded so that higher values indicate better cognitive performance. The Base model was adjusted for \"\n",
    "        \"age at Instance 2, sex, Townsend deprivation index at recruitment, body mass index at baseline, genetic ethnic grouping, smoking status, alcohol intake frequency, \"\n",
    "        \"educational attainment (degree vs no degree), and APOE ε4 allele count. Positive β values therefore indicate better cognitive performance in participants with SA relative \"\n",
    "        \"to matched controls. Models including CMC further adjusted for cardiovascular and metabolic conditions, and WMH models additionally included the corresponding WMH measures. \"\n",
    "        \"All models used cluster-robust standard errors by matched set.\"\n",
    "    )\n",
    "\n",
    "    # Abbreviations line\n",
    "    doc.add_paragraph(\n",
    "        \"SA = sleep apnea; WMH = white matter hyperintensities; PWMH = periventricular WMH; \"\n",
    "        \"DWMH = deep WMH; PSM = propensity score matching; CMC = cardiovascular and metabolic conditions.\"\n",
    "    )\n",
    "\n",
    "    cog_fig3_docx_path = os.path.join(ROOT_DIR, \"Cognitive_Figure3_Legend.docx\")\n",
    "    doc.save(cog_fig3_docx_path)\n",
    "    print(f\"Cognitive Figure 3 legend (Word) saved: {cog_fig3_docx_path}\")\n",
    "else:\n",
    "    print(\"Cognitive Figure 3 legend (Word) not saved: python-docx not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d4ca514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4145030010.py:216: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = ensure_outcomes(clean_cols(pd.read_csv(fp)))\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4145030010.py:216: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = ensure_outcomes(clean_cols(pd.read_csv(fp)))\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4145030010.py:216: DtypeWarning: Columns (23,29,30,38,45,48,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = ensure_outcomes(clean_cols(pd.read_csv(fp)))\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4145030010.py:216: DtypeWarning: Columns (29,30,38,45,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = ensure_outcomes(clean_cols(pd.read_csv(fp)))\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4145030010.py:216: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = ensure_outcomes(clean_cols(pd.read_csv(fp)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostics tables saved under C:\\Users\\M328449\\OneDrive - Mayo Clinic\\Documents\\Python\\SA_WMH_Pub\\Diagnostics_Summary\\Supplement\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics Tables for 5 Cohorts (with ATO weights support)\n",
    "# Output: eTable 1 (fit + tests), eTable 2 (VIF)\n",
    "# Format: landscape, three-line table, Times New Roman 10pt\n",
    "# ============================================================\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- diagnostics ----\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence, variance_inflation_factor\n",
    "\n",
    "# ---- Word export ----\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.section import WD_ORIENTATION\n",
    "\n",
    "# ============================================================\n",
    "# Paths & Settings\n",
    "# ============================================================\n",
    "\n",
    "OUT_ROOT = Path(\"Diagnostics_Summary\")\n",
    "SUPP_DIR = OUT_ROOT / \"Supplement\"\n",
    "SUPP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "COHORTS = {\n",
    "    \"Primary\": \"primary_cohort.csv\",\n",
    "    \"PSM–sensitivity\": \"sensitivity_cohort.csv\",\n",
    "    \"ATO–restricted\": \"ato_sensitivity_sym.csv\",\n",
    "    \"ATO–full\": \"ato_sensitivity_noexclusion.csv\",\n",
    "    \"Fully adjusted\": \"data_processed.csv\"\n",
    "}\n",
    "\n",
    "COHORT_ORDER = list(COHORTS.keys())\n",
    "\n",
    "GROUP_COL = \"group\"\n",
    "MATCH_ID  = \"match_id\"\n",
    "\n",
    "SCALE_I2 = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "RAW_TOT  = \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"\n",
    "RAW_PV   = \"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\"\n",
    "RAW_DEEP = \"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\"\n",
    "\n",
    "OUTCOME_BUILD = [\n",
    "    (\"Log_HSNorm_Total_WMH_T1_T2\",     RAW_TOT,  \"Total WMH\"),\n",
    "    (\"Log_HSNorm_PeriVentricular_WMH\", RAW_PV,   \"PWMH\"),\n",
    "    (\"Log_HSNorm_Deep_WMH\",            RAW_DEEP, \"DWMH\"),\n",
    "]\n",
    "OUTCOME_ORDER = [\"Total WMH\",\"PWMH\",\"DWMH\"]\n",
    "\n",
    "BASE_COVARS = [\n",
    "    \"Sex\",\"Age_at_Instance_2\",\"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\"Alcohol_intake_frequency_ordinal\"\n",
    "]\n",
    "CATEGORICAL = {\"Sex\",\"Genetic_ethnic_grouping\",\"Smoking_Ever\"}\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns=lambda c: re.sub(r\"_{2,}\", \"_\",\n",
    "           re.sub(r\"[^0-9a-zA-Z]+\",\"_\", str(c))).strip(\"_\"))\n",
    "\n",
    "def ensure_outcomes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    if GROUP_COL in d.columns:\n",
    "        d[GROUP_COL] = pd.Categorical(d[GROUP_COL], [\"Control\",\"Study\"])\n",
    "    scale = pd.to_numeric(d.get(SCALE_I2), errors=\"coerce\").fillna(1.0)\n",
    "    for y, src, _lab in OUTCOME_BUILD:\n",
    "        v = pd.to_numeric(d.get(src), errors=\"coerce\")\n",
    "        d[y] = np.log1p(v * scale)\n",
    "    return d\n",
    "\n",
    "def build_formula(y_col: str, df: pd.DataFrame) -> str:\n",
    "    rhs = [\"C(group)\"]\n",
    "    for v in BASE_COVARS:\n",
    "        if v not in df.columns: continue\n",
    "        rhs.append(f\"C({v})\" if v in CATEGORICAL else v)\n",
    "    return f\"{y_col} ~ \" + \" + \".join(rhs)\n",
    "\n",
    "def fit_cluster_or_hc3(formula: str, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit OLS with cluster-robust SE or WLS if weight column is present.\n",
    "    \"\"\"\n",
    "    y, X = dmatrices(formula, data, return_type=\"dataframe\", NA_action=\"drop\")\n",
    "    weights = data.loc[y.index, \"ato_weight\"] if \"ato_weight\" in data.columns else None\n",
    "    if weights is not None:\n",
    "        base = sm.WLS(y, X, weights=weights).fit(cov_type=\"HC3\")\n",
    "    else:\n",
    "        base = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "\n",
    "    if MATCH_ID not in data.columns: \n",
    "        return base, y, X, 0\n",
    "    g = data.loc[y.index, MATCH_ID].dropna()\n",
    "    if g.nunique() <= 1: \n",
    "        return base, y, X, int(g.nunique())\n",
    "\n",
    "    if weights is not None:\n",
    "        fit = sm.WLS(y.loc[g.index], X.loc[g.index], weights=weights.loc[g.index]).fit(\n",
    "            cov_type=\"cluster\", cov_kwds={\"groups\": g})\n",
    "    else:\n",
    "        fit = sm.OLS(y.loc[g.index], X.loc[g.index]).fit(\n",
    "            cov_type=\"cluster\", cov_kwds={\"groups\": g})\n",
    "    return fit, y.loc[g.index], X.loc[g.index], int(g.nunique())\n",
    "\n",
    "def tests_bp_white(fit):\n",
    "    resid, exog = fit.resid, fit.model.exog\n",
    "    try: bp_stat, bp_p, _, _ = het_breuschpagan(resid, exog)\n",
    "    except: bp_stat, bp_p = np.nan, np.nan\n",
    "    try: wh_stat, wh_p, _, _ = het_white(resid, exog)\n",
    "    except: wh_stat, wh_p = np.nan, np.nan\n",
    "    return bp_stat, bp_p, wh_stat, wh_p\n",
    "\n",
    "def test_jb(fit):\n",
    "    jb_stat, jb_p, _, _ = jarque_bera(fit.resid)\n",
    "    return jb_stat, jb_p\n",
    "\n",
    "def compute_vif_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [c for c in X.columns if c.lower() != \"intercept\"]\n",
    "    Xv = X[cols].copy()\n",
    "    out=[]\n",
    "    for i,c in enumerate(Xv.columns):\n",
    "        try: v = variance_inflation_factor(Xv.values, i)\n",
    "        except: v = np.nan\n",
    "        out.append({\"Term\": c, \"VIF\": float(v) if np.isfinite(v) else np.nan})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def condition_number(X: pd.DataFrame) -> float:\n",
    "    cols = [c for c in X.columns if c.lower() != \"intercept\"]\n",
    "    Xm = X[cols].to_numpy(dtype=float)\n",
    "    try:\n",
    "        s = np.linalg.svd(Xm, compute_uv=False)\n",
    "        s = s[s>0]\n",
    "        return float((s.max() / s.min())) if s.size>0 else np.nan\n",
    "    except: return np.nan\n",
    "\n",
    "# ============================================================\n",
    "# Word export (three-line, landscape)\n",
    "# ============================================================\n",
    "\n",
    "def _apply_normal_style(doc):\n",
    "    style = doc.styles[\"Normal\"]; style.font.name = \"Times New Roman\"\n",
    "    style._element.rPr.rFonts.set(qn(\"w:eastAsia\"), \"Times New Roman\")\n",
    "    style.font.size = Pt(10)\n",
    "\n",
    "def _set_cell_border(cell, **kwargs):\n",
    "    tc = cell._tc; tcPr = tc.get_or_add_tcPr()\n",
    "    tcBorders = tcPr.first_child_found_in(\"w:tcBorders\")\n",
    "    if tcBorders is None:\n",
    "        tcBorders = OxmlElement('w:tcBorders'); tcPr.append(tcBorders)\n",
    "    for edge, attrs in kwargs.items():\n",
    "        tag = 'w:' + edge\n",
    "        el = tcBorders.find(qn(tag))\n",
    "        if el is None:\n",
    "            el = OxmlElement(tag); tcBorders.append(el)\n",
    "        for k in [\"val\",\"sz\",\"space\",\"color\"]:\n",
    "            if k in attrs: el.set(qn('w:'+k), str(attrs[k]))\n",
    "\n",
    "def apply_three_line_table(table):\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            _set_cell_border(cell, top={\"val\":\"none\"}, bottom={\"val\":\"none\"},\n",
    "                             left={\"val\":\"none\"}, right={\"val\":\"none\"})\n",
    "    for cell in table.rows[0].cells:\n",
    "        _set_cell_border(cell, top={\"val\":\"single\",\"sz\":\"20\",\"color\":\"000000\"},\n",
    "                               bottom={\"val\":\"single\",\"sz\":\"15\",\"color\":\"000000\"})\n",
    "    for cell in table.rows[-1].cells:\n",
    "        _set_cell_border(cell, bottom={\"val\":\"single\",\"sz\":\"20\",\"color\":\"000000\"})\n",
    "\n",
    "def write_docx_three_line(path_docx, title, df, legend_text):\n",
    "    doc = Document(); _apply_normal_style(doc)\n",
    "    # landscape\n",
    "    section = doc.sections[-1]\n",
    "    new_width, new_height = section.page_height, section.page_width\n",
    "    section.orientation = WD_ORIENTATION.LANDSCAPE\n",
    "    section.page_width, section.page_height = new_width, new_height\n",
    "\n",
    "    p = doc.add_paragraph(); run=p.add_run(title); run.bold=True\n",
    "    table = doc.add_table(rows=1, cols=len(df.columns))\n",
    "    for j,c in enumerate(df.columns): \n",
    "        header = \"Analytic cohort\" if c==\"Cohort\" else str(c)\n",
    "        table.rows[0].cells[j].text = header\n",
    "    for _, row in df.iterrows():\n",
    "        cells = table.add_row().cells\n",
    "        for j,c in enumerate(df.columns):\n",
    "            val = row[c]\n",
    "            if isinstance(val, float) and not np.isnan(val):\n",
    "                cells[j].text = f\"{val:.4g}\"\n",
    "            else:\n",
    "                cells[j].text = \"\" if pd.isna(val) else str(val)\n",
    "    apply_three_line_table(table)\n",
    "    doc.add_paragraph(legend_text).italic = True\n",
    "    doc.save(path_docx)\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "all_fit_rows = []\n",
    "all_vif_rows = []\n",
    "\n",
    "for label, fp in COHORTS.items():\n",
    "    if not os.path.exists(fp):\n",
    "        print(f\"[Skip] {label}: not found\")\n",
    "        continue\n",
    "    df = ensure_outcomes(clean_cols(pd.read_csv(fp)))\n",
    "\n",
    "    if GROUP_COL not in df.columns:\n",
    "        if \"treatment_var\" in df.columns:\n",
    "            df[GROUP_COL] = df[\"treatment_var\"].map({0:\"Control\",1:\"Study\"})\n",
    "        elif \"SA\" in df.columns:\n",
    "            df[GROUP_COL] = df[\"SA\"].map({0:\"Control\",1:\"Study\"})\n",
    "        else:\n",
    "            raise ValueError(f\"{label}: no group col\")\n",
    "\n",
    "    for v in BASE_COVARS:\n",
    "        if v not in df.columns: continue\n",
    "        if pd.api.types.is_numeric_dtype(df[v]):\n",
    "            df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "            if df[v].notna().any():\n",
    "                df[v] = df[v].fillna(df[v].median())\n",
    "        else:\n",
    "            m = df[v].mode(dropna=True)\n",
    "            if not m.empty: df[v] = df[v].fillna(m.iloc[0])\n",
    "\n",
    "    for y_col, _src, lab in OUTCOME_BUILD:\n",
    "        if y_col not in df.columns: continue\n",
    "        formula = build_formula(y_col, df)\n",
    "        fit, y, X, n_clusters = fit_cluster_or_hc3(formula, df)\n",
    "\n",
    "        jb_stat, jb_p = test_jb(fit)\n",
    "        bp_stat, bp_p, wh_stat, wh_p = tests_bp_white(fit)\n",
    "        cooks = OLSInfluence(fit).cooks_distance[0]\n",
    "\n",
    "        # Table 1\n",
    "        all_fit_rows.append({\n",
    "            \"Cohort\": label, \"Outcome\": lab,\n",
    "            \"N\": int(fit.nobs), \"Clusters\": n_clusters,\n",
    "            \"R^2\": float(fit.rsquared), \"Adj R^2\": float(fit.rsquared_adj),\n",
    "            \"AIC\": float(fit.aic), \"BIC\": float(fit.bic),\n",
    "            \"SE type\": fit.cov_type + (f\" (clusters={n_clusters})\" if fit.cov_type==\"cluster\" else \"\"),\n",
    "            \"JB χ²\": jb_stat, \"JB p\": jb_p,\n",
    "            \"BP χ²\": bp_stat, \"BP p\": bp_p,\n",
    "            \"White χ²\": wh_stat, \"White p\": wh_p,\n",
    "            \"κ (condition number)\": condition_number(X),\n",
    "            \"Max Cook’s D\": float(np.nanmax(cooks))\n",
    "        })\n",
    "\n",
    "        # Table 2\n",
    "        vif_df = compute_vif_df(X)\n",
    "        if label == \"Primary\":\n",
    "            vif_df.insert(0, \"Cohort\", label)\n",
    "            vif_df.insert(1, \"Outcome\", lab)\n",
    "            vif_df[\"κ (condition number)\"] = np.where(\n",
    "                vif_df.index==0, condition_number(X), np.nan)\n",
    "            all_vif_rows.append(vif_df)\n",
    "        else:\n",
    "            all_vif_rows.append(pd.DataFrame([{\n",
    "                \"Cohort\": label, \"Outcome\": lab,\n",
    "                \"Term\": \"Summary\", \n",
    "                \"VIF\": vif_df[\"VIF\"].max(),\n",
    "                \"κ (condition number)\": condition_number(X)\n",
    "            }]))\n",
    "\n",
    "# ============================================================\n",
    "# Export\n",
    "# ============================================================\n",
    "\n",
    "df_fit = pd.DataFrame(all_fit_rows)\n",
    "df_fit[\"Outcome\"] = pd.Categorical(df_fit[\"Outcome\"], categories=OUTCOME_ORDER, ordered=True)\n",
    "df_fit[\"Cohort\"] = pd.Categorical(df_fit[\"Cohort\"], categories=COHORT_ORDER, ordered=True)\n",
    "df_fit = df_fit.sort_values([\"Cohort\",\"Outcome\"])\n",
    "write_docx_three_line(\n",
    "    SUPP_DIR / \"eTable_1_ModelFit_AssumptionTests.docx\",\n",
    "    \"eTable 1. Model fit and assumption tests across analytic cohorts\",\n",
    "    df_fit,\n",
    "    (\"Reported metrics include N, clusters, R², adjusted R², AIC, BIC, \"\n",
    "     \"standard error type, Jarque–Bera normality test, Breusch–Pagan and White heteroscedasticity tests, \"\n",
    "     \"condition number κ, and maximum Cook’s distance.\")\n",
    ")\n",
    "\n",
    "df_vif = pd.concat(all_vif_rows, ignore_index=True)\n",
    "df_vif[\"Outcome\"] = pd.Categorical(df_vif[\"Outcome\"], categories=OUTCOME_ORDER, ordered=True)\n",
    "df_vif[\"Cohort\"] = pd.Categorical(df_vif[\"Cohort\"], categories=COHORT_ORDER, ordered=True)\n",
    "df_vif = df_vif.sort_values([\"Cohort\",\"Outcome\"])\n",
    "write_docx_three_line(\n",
    "    SUPP_DIR / \"eTable_2_VIF_and_Kappa.docx\",\n",
    "    \"eTable 2. Multicollinearity diagnostics (VIF and condition number κ)\",\n",
    "    df_vif,\n",
    "    (\"For the Primary cohort, VIFs are reported for each covariate; for other cohorts, \"\n",
    "     \"only the maximum VIF and condition number κ are shown. Values >5–10 suggest potential collinearity, \"\n",
    "     \"and κ>30 indicates serious collinearity.\")\n",
    ")\n",
    "\n",
    "print(\"Diagnostics tables saved under\", SUPP_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f3ee13d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATO Symmetric exclusion ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3794241081.py:344: DtypeWarning: Columns (23,29,30,38,45,48,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ATO No exclusion ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3794241081.py:344: DtypeWarning: Columns (29,30,38,45,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Full-sample adjusted ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3794241081.py:344: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Five-cohort CSV saved: Additional_Sensitivity_Analysis\\Combined_FiveCohort_Table.csv\n",
      "Five-cohort combined table failed: apply_three_line_table() got an unexpected keyword argument 'header_row_idx'\n",
      "\n",
      "=== Baseline table for ATO–restricted cohort ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3794241081.py:546: DtypeWarning: Columns (23,29,30,38,45,48,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved CSV: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\ATO_restricted_cohort_Table1_Baseline.csv\n",
      "[OK] Saved Word: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\Table1_Baseline_ATO_restricted_cohort_20251112-084154.docx\n",
      "\n",
      "=== Baseline table for ATO–full cohort ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3794241081.py:546: DtypeWarning: Columns (29,30,38,45,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved CSV: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\ATO_full_cohort_Table1_Baseline.csv\n",
      "[OK] Saved Word: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\Table1_Baseline_ATO_full_cohort_20251112-084155.docx\n",
      "\n",
      "=== Baseline table for Regression-adjusted ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\3794241081.py:546: DtypeWarning: Columns (24,25,33,40,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved CSV: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\Regression_adjusted_Table1_Baseline.csv\n",
      "[OK] Saved Word: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\Table1_Baseline_Regression_adjusted_20251112-084156.docx\n",
      "[OK] Combined Word table saved: Additional_Sensitivity_Analysis\\Supplementary_Baseline\\TableSx_Baseline_Supplementary.docx\n"
     ]
    }
   ],
   "source": [
    "# Additional Sensitivity Analysis Pipeline\n",
    "\"\"\"\n",
    "This pipeline performs additional sensitivity analyses of WMH outcomes\n",
    "using three alternative cohorts:\n",
    "1. ATO Symmetric exclusion (Instance 2 MRI-anchored neuro exclusion)\n",
    "2. ATO No exclusion (full sample with ATO weighting, no neuro exclusion)\n",
    "3. Full-sample adjusted (no weighting, covariate-adjusted regression)\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "- Cohort-specific OLS/WLS results (CSV)\n",
    "- Forest plots (single cohort + overlay)\n",
    "- Manuscript-ready Word tables (three-line format)\n",
    "- Figure legends (TXT + Word)\n",
    "- Supplementary combined table (3 cohorts together, with q)\n",
    "\"\"\"\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from datetime import datetime\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docx.oxml import OxmlElement\n",
    "    from docx.oxml.ns import qn\n",
    "except:\n",
    "    Document=None\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "OUT_DIR = \"Additional_Sensitivity_Analysis\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "file_sets = {\n",
    "    \"ATO Symmetric exclusion\": \"ato_sensitivity_sym.csv\",\n",
    "    \"ATO No exclusion\":        \"ato_sensitivity_noexclusion.csv\",\n",
    "    \"Full-sample adjusted\":    \"data_processed.csv\"\n",
    "}\n",
    "\n",
    "SHORT = {\n",
    "    \"Log_HSNorm_Total_WMH_T1_T2\":\"Total WMH\",\n",
    "    \"Log_HSNorm_PeriVentricular_WMH\":\"Periventricular WMH\",\n",
    "    \"Log_HSNorm_Deep_WMH\":\"Deep WMH\"\n",
    "}\n",
    "PLOT_ORDER=[\"Total WMH\",\"PWMH\",\"DWMH\"]\n",
    "\n",
    "BASE_ADJ=[\n",
    "    \"Sex\",\"Age_at_Instance_2\",\"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\"Genetic_ethnic_grouping\",\"Smoking_Ever\",\n",
    "    \"Alcohol_intake_frequency_ordinal\"\n",
    "]\n",
    "CATEGORICAL={\"Sex\",\"Smoking_Ever\",\"Genetic_ethnic_grouping\"}\n",
    "\n",
    "# === Publication style & multi-format export (match main analysis) ===\n",
    "# Fonts & sizes consistent with the main analysis\n",
    "PUB_RC = {\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"figure.autolayout\": True,\n",
    "}\n",
    "\n",
    "# --- High-quality TIFF export defaults ---\n",
    "SAVEFIG_DPI = 600\n",
    "PNG_KW  = {\"dpi\": SAVEFIG_DPI, \"bbox_inches\": \"tight\", \"facecolor\": \"white\"}\n",
    "PDF_KW  = {\"bbox_inches\": \"tight\"}\n",
    "SVG_KW  = {\"bbox_inches\": \"tight\"}\n",
    "TIFF_KW = {\n",
    "    \"dpi\": SAVEFIG_DPI,\n",
    "    \"bbox_inches\": \"tight\",\n",
    "    \"facecolor\": \"white\",\n",
    "    \"format\": \"tiff\",\n",
    "    \"pil_kwargs\": {\"compression\": \"tiff_lzw\"}  \n",
    "}\n",
    "\n",
    "# -Cohort Name----\n",
    "alias_to_final = {\n",
    "    \"ATO Symmetric exclusion\": \"ATO–restricted cohort\",\n",
    "    \"ATO No exclusion\":        \"ATO–full cohort\",\n",
    "    \"Full-sample adjusted\":    \"Regression-adjusted\",\n",
    "    \"Primary 1:10 (NoNeuro)\":  \"Primary 1:10 (NoNeuro)\",\n",
    "    \"Sensitivity 1:10 (WithNeuro)\": \"Sensitivity 1:10 (WithNeuro)\",\n",
    "    # passthroughs (if upstream already standardized)\n",
    "    \"Regression-adjusted\":     \"Regression-adjusted\",\n",
    "}\n",
    "\n",
    "# ---------------- UTILITIES ----------------\n",
    "def build_formula(outcome, adjust_vars, categorical_vars, group_var=\"group\"):\n",
    "    terms=[f\"C({group_var})\"]\n",
    "    for v in adjust_vars:\n",
    "        if v==group_var: continue\n",
    "        terms.append(f\"C({v})\" if v in categorical_vars else v)\n",
    "    return f\"{outcome} ~ \" + \" + \".join(terms)\n",
    "\n",
    "def find_group_term(params_index, group_var=\"group\", target_level=\"Study\"):\n",
    "    cands=[p for p in params_index if p.startswith(f\"C({group_var})[T.\")]\n",
    "    for p in cands:\n",
    "        if p.endswith(f\"[T.{target_level}]\"): return p\n",
    "    return None\n",
    "\n",
    "def fit_hc3(formula, data, weight_col=None):\n",
    "    \"\"\"\n",
    "    Fit OLS (HC3) or WLS (HC3 if weights provided).\n",
    "    \"\"\"\n",
    "    y,X=dmatrices(formula,data,return_type=\"dataframe\",NA_action=\"drop\")\n",
    "    if weight_col and weight_col in data.columns:\n",
    "        w = data.loc[y.index, weight_col]\n",
    "        model=sm.WLS(y,X,weights=w).fit(cov_type=\"HC3\")\n",
    "    else:\n",
    "        model=sm.OLS(y,X).fit(cov_type=\"HC3\")\n",
    "    return model,y,X\n",
    "\n",
    "def plot_overlay_forest(res_dict, outdir, basename):\n",
    "    \"\"\"\n",
    "    Overlay forest plot across three cohorts with publication-ready styling.\n",
    "\n",
    "    Changes in this version:\n",
    "    - Unify display names to the finalized style:\n",
    "        \"ATO–restricted cohort\", \"ATO–full cohort\", \"Regression-adjusted\"\n",
    "      (An alias map keeps backward-compatibility for old keys like\n",
    "       \"ATO Symmetric exclusion\", \"ATO No exclusion\", \"Full-sample adjusted\".)\n",
    "    - Remove top/right spines; legend on the right center without frame.\n",
    "    - Apply global PUB_RC for consistent typography.\n",
    "    \"\"\"\n",
    "    if not res_dict:\n",
    "        return\n",
    "\n",
    "    # Apply global publication style (Arial, sizes, dpi, etc.)\n",
    "    plt.rcParams.update(PUB_RC)\n",
    "\n",
    "    # ---- (1) Normalize incoming keys to the finalized display names ----\n",
    "    # Backward-compatible alias mapping from OLD -> NEW (display) names\n",
    "    alias_to_final = {\n",
    "        \"ATO Symmetric exclusion\": \"ATO–restricted cohort\",\n",
    "        \"ATO No exclusion\":        \"ATO–full cohort\",\n",
    "        \"Full-sample adjusted\":    \"Regression-adjusted\",\n",
    "        # passthroughs if already standardized:\n",
    "        \"ATO–restricted cohort\":   \"ATO–restricted cohort\",\n",
    "        \"ATO–full cohort\":         \"ATO–full cohort\",\n",
    "        \"Regression-adjusted\":     \"Regression-adjusted\",\n",
    "    }\n",
    "    # Build a standardized copy so downstream uses only the NEW names\n",
    "    std_res = {}\n",
    "    for k, df in res_dict.items():\n",
    "        std_name = alias_to_final.get(k, k)  # default: passthrough if unknown\n",
    "        std_res[std_name] = df\n",
    "\n",
    "    # Finalized order for plotting and legend\n",
    "    labels_order = [\"ATO–restricted cohort\", \"ATO–full cohort\", \"Regression-adjusted\"]\n",
    "\n",
    "    # ---- (2) Visual mappings (colors/markers) keyed by NEW names ----\n",
    "    colors = {\n",
    "        \"ATO–restricted cohort\": \"#1f3b4d\",  # deep blue (primary)\n",
    "        \"ATO–full cohort\":       \"#6e6e6e\",  # neutral gray\n",
    "        \"Regression-adjusted\":   \"#d95f02\"   # accent orange\n",
    "    }\n",
    "    markers = {\n",
    "        \"ATO–restricted cohort\": \"o\",\n",
    "        \"ATO–full cohort\":       \"s\",\n",
    "        \"Regression-adjusted\":   \"^\"\n",
    "    }\n",
    "\n",
    "    # ---- (3) Figure + axis ----\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    # Map outcomes to positions using your global PLOT_ORDER\n",
    "    order_map = {v: i for i, v in enumerate(PLOT_ORDER)}\n",
    "    y = np.arange(len(PLOT_ORDER))[::-1]\n",
    "    pad_y = 0.45\n",
    "    jitter = 0.18  # horizontal stacking offset\n",
    "\n",
    "    # ---- (4) Draw each cohort as a jittered series ----\n",
    "    for i, lbl in enumerate(labels_order):\n",
    "        if lbl not in std_res:\n",
    "            continue\n",
    "        res = std_res[lbl].sort_values(\"Outcome\", key=lambda s: s.map(order_map))\n",
    "        # Map raw region names to display labels for y-axis\n",
    "        region_map = {\n",
    "            \"Total\": \"Total WMH\",\n",
    "            \"Periventricular\": \"PWMH\",\n",
    "            \"Deep\": \"DWMH\"\n",
    "        }\n",
    "        res[\"Outcome\"] = res[\"Outcome\"].replace(region_map)\n",
    "        y_shift = y - jitter * (i - 1)\n",
    "\n",
    "        ax.errorbar(\n",
    "            res[\"% Change\"], y_shift,\n",
    "            xerr=[res[\"% Change\"] - res[\"% CI Lower\"],\n",
    "                  res[\"% CI Upper\"] - res[\"% Change\"]],\n",
    "            fmt=markers[lbl], ms=5.0,\n",
    "            # Fill only the primary series; others hollow for contrast\n",
    "            mfc=(\"white\" if lbl != \"ATO–restricted cohort\" else colors[lbl]),\n",
    "            mec=colors[lbl], ecolor=colors[lbl],\n",
    "            elinewidth=1.4, capsize=4, label=lbl\n",
    "        )\n",
    "\n",
    "    # Reference line at zero\n",
    "    ax.axvline(0, color=\"grey\", ls=\"--\", lw=1)\n",
    "\n",
    "    # Y axis: categories and padding\n",
    "    ax.set_yticks(y, PLOT_ORDER)\n",
    "    ax.set_ylim(y.min() - pad_y, y.max() + pad_y)\n",
    "\n",
    "    # X axis label (kept as previously used text)\n",
    "    ax.set_xlabel(\"% change (SA - Control)\")\n",
    "\n",
    "    # Cosmetics: remove top/right spines for a modern look\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # ---- (5) Legend on the right, vertically centered, no frame ----\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ordered = [by_label[l] for l in labels_order if l in by_label]\n",
    "\n",
    "    ax.legend(\n",
    "        ordered, labels_order,\n",
    "        frameon=False,\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        fontsize=8,\n",
    "        handlelength=1.5\n",
    "    )\n",
    "\n",
    "    # Tight layout + safe filename\n",
    "    plt.tight_layout()\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", basename).strip(\"_\")\n",
    "    base = os.path.join(outdir, safe)\n",
    "\n",
    "    # Save high-resolution outputs\n",
    "    plt.savefig(base + \".png\",  **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\",  **PDF_KW)\n",
    "    plt.savefig(base + \".svg\",  **SVG_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)   # 新增高品质 TIFF (600 dpi, LZW)\n",
    "    plt.close(fig)\n",
    "\n",
    "def export_table1_word(df: pd.DataFrame, analysis_label: str, outdir: str, timestamp: str = None):\n",
    "    \"\"\"\n",
    "    Export a 'Table 1' style baseline table to Word (.docx).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Aggregated baseline table produced by make_table1(...).\n",
    "        Typical columns: [\"Analysis\",\"Characteristic\",\"Study\",\"Control\", ... optionally p_value / SMD]\n",
    "    analysis_label : str\n",
    "        Label used in the table title and file name.\n",
    "    outdir : str\n",
    "        Output directory.\n",
    "    timestamp : str, optional\n",
    "        Timestamp for the file name. If None, a timestamp will be generated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        Path to the generated .docx file, or None if python-docx is unavailable.\n",
    "    \"\"\"\n",
    "    if Document is None:\n",
    "        print(\"[Warn] python-docx not available; skipping Word export.\")\n",
    "        return None\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    safe_label = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", analysis_label).strip(\"_\")\n",
    "    word_path = os.path.join(outdir, f\"Table1_Baseline_{safe_label}_{timestamp}.docx\")\n",
    "\n",
    "    doc = Document()\n",
    "\n",
    "    # Title\n",
    "    title = doc.add_paragraph()\n",
    "    run = title.add_run(f\"Table Sx. Baseline characteristics — {analysis_label}\")\n",
    "    run.bold = True\n",
    "    title.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    # Table header\n",
    "    cols = list(df.columns)\n",
    "    table = doc.add_table(rows=1, cols=len(cols))\n",
    "    hdr = table.rows[0].cells\n",
    "    for i, c in enumerate(cols):\n",
    "        hdr[i].text = str(c)\n",
    "\n",
    "    # Table rows\n",
    "    for _, row in df.iterrows():\n",
    "        cells = table.add_row().cells\n",
    "        for j, c in enumerate(cols):\n",
    "            val = row[c] if c in row else \"\"\n",
    "            if isinstance(val, (int, float, np.floating)):\n",
    "                cl = str(c).lower()\n",
    "                if cl in {\"p\", \"p_value\", \"p-value\"}:\n",
    "                    cells[j].text = \"<0.001\" if float(val) < 0.001 else f\"{float(val):.3f}\"\n",
    "                elif cl in {\"smd\"}:\n",
    "                    cells[j].text = f\"{float(val):.3f}\"\n",
    "                else:\n",
    "                    cells[j].text = f\"{float(val):.2f}\"\n",
    "            else:\n",
    "                cells[j].text = \"\" if pd.isna(val) else str(val)\n",
    "\n",
    "    # Right-align numeric columns\n",
    "    numeric_idx = [j for j, c in enumerate(cols) if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    for row in table.rows[1:]:\n",
    "        for j in numeric_idx:\n",
    "            for p in row.cells[j].paragraphs:\n",
    "                p.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "\n",
    "    # Optional styling hooks (safe if helpers are absent)\n",
    "    try:\n",
    "        apply_three_line_table(table, header_row_idx=0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        set_table_font(table, font_name=\"Times New Roman\", font_size=10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Legend\n",
    "    legend = doc.add_paragraph()\n",
    "    note = legend.add_run(\n",
    "        \"Legend: Values are mean (SD), median [IQR], or n (%). \"\n",
    "        \"p-values from Welch’s t test (continuous), Mann–Whitney U (alcohol intake), \"\n",
    "        \"and χ² or Fisher’s exact test (categorical). \"\n",
    "        \"Study = sleep apnea; Control = matched controls.\"\n",
    "    )\n",
    "    note.italic = True\n",
    "\n",
    "    doc.save(word_path)\n",
    "    return word_path\n",
    "\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "timestamp=datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "res_by_label={}\n",
    "\n",
    "for label,file in file_sets.items():\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    df=pd.read_csv(file)\n",
    "    df.columns=df.columns.str.replace(r\"[^0-9a-zA-Z]+\",\"_\",regex=True)\n",
    "\n",
    "    if \"group\" not in df.columns:\n",
    "        if \"treatment_var\" in df.columns:\n",
    "            df[\"group\"]=df[\"treatment_var\"].map({0:\"Control\",1:\"Study\"})\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    scale=pd.to_numeric(df.get(\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"),errors=\"coerce\")\n",
    "    df[\"HSNorm_Deep_WMH\"]=pd.to_numeric(df.get(\"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\"),errors=\"coerce\")*scale\n",
    "    df[\"HSNorm_PeriVentricular_WMH\"]=pd.to_numeric(df.get(\"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\"),errors=\"coerce\")*scale\n",
    "    df[\"HSNorm_Total_WMH_T1_T2\"]=pd.to_numeric(df.get(\"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"),errors=\"coerce\")*scale\n",
    "    df[\"Log_HSNorm_Deep_WMH\"]=np.log1p(df[\"HSNorm_Deep_WMH\"])\n",
    "    df[\"Log_HSNorm_PeriVentricular_WMH\"]=np.log1p(df[\"HSNorm_PeriVentricular_WMH\"])\n",
    "    df[\"Log_HSNorm_Total_WMH_T1_T2\"]=np.log1p(df[\"HSNorm_Total_WMH_T1_T2\"])\n",
    "\n",
    "    present_adj=[v for v in BASE_ADJ if v in df.columns]\n",
    "    categorical_in=set([v for v in present_adj if v in CATEGORICAL])\n",
    "\n",
    "    dmod=df.copy()\n",
    "    for v in present_adj:\n",
    "        if pd.api.types.is_numeric_dtype(dmod[v]):\n",
    "            dmod[v]=pd.to_numeric(dmod[v],errors=\"coerce\").fillna(dmod[v].median())\n",
    "        else:\n",
    "            mode=dmod[v].mode(dropna=True)\n",
    "            dmod[v]=dmod[v].fillna(mode.iloc[0] if not mode.empty else dmod[v])\n",
    "\n",
    "    outcomes=[\"Log_HSNorm_Total_WMH_T1_T2\",\"Log_HSNorm_PeriVentricular_WMH\",\"Log_HSNorm_Deep_WMH\"]\n",
    "\n",
    "    rows=[]\n",
    "    for oc in outcomes:\n",
    "        if oc not in dmod.columns: continue\n",
    "        fml=build_formula(oc,BASE_ADJ,categorical_in)\n",
    "\n",
    "        # Use ATO weights if available\n",
    "        weight_col=None\n",
    "        if label in [\"ATO Symmetric exclusion\",\"ATO No exclusion\"]:\n",
    "            weight_col=\"ato_weight_norm\"\n",
    "\n",
    "        model,y,X=fit_hc3(fml,dmod,weight_col=weight_col)\n",
    "        key=find_group_term(model.params.index,\"group\",\"Study\")\n",
    "        if key is None: continue\n",
    "        beta=float(model.params[key])\n",
    "        ci_l,ci_u=[float(x) for x in model.conf_int().loc[key]]\n",
    "        pval=float(model.pvalues[key])\n",
    "        pct,lo,hi=100*(np.exp(beta)-1),100*(np.exp(ci_l)-1),100*(np.exp(ci_u)-1)\n",
    "        rows.append({\"Outcome\":SHORT[oc],\"% Change\":pct,\"% CI Lower\":lo,\"% CI Upper\":hi,\"p_value\":pval})\n",
    "\n",
    "    res=pd.DataFrame(rows)\n",
    "    if not res.empty:\n",
    "        mask=res[\"Outcome\"].isin([\"Periventricular WMH\",\"Deep WMH\"])\n",
    "        if mask.any():\n",
    "            rej,qv,_,_=multipletests(res.loc[mask,\"p_value\"],method=\"fdr_bh\")\n",
    "            res.loc[mask,\"q_value\"]=qv\n",
    "        res.loc[res[\"Outcome\"]==\"Total WMH\",[\"q_value\"]]=np.nan\n",
    "    new_label = alias_to_final.get(label, label)\n",
    "    res_by_label[new_label] = res\n",
    "    plot_single_forest(res,label,OUT_DIR)\n",
    "    safe=re.sub(r\"[^0-9a-zA-Z]+\",\"_\",label)\n",
    "    res.to_csv(os.path.join(OUT_DIR,f\"{safe}_Results.csv\"),index=False,encoding=\"utf-8-sig\")\n",
    "\n",
    "# Overlay figure\n",
    "plot_overlay_forest(res_by_label,OUT_DIR,f\"Overlay_Forest_Sensitivity\")\n",
    "\n",
    "# -------- Combine 5 cohorts into one publication-ready table --------\n",
    "def make_five_cohort_table(all_results: dict, outdir: str) -> pd.DataFrame:\n",
    "    cohort_order = [\n",
    "        \"Primary 1:10 (NoNeuro)\",\n",
    "        \"Sensitivity 1:10 (WithNeuro)\",\n",
    "        \"ATO–restricted cohort\",\n",
    "        \"ATO–full cohort\",\n",
    "        \"Regression-adjusted\"\n",
    "    ]\n",
    "\n",
    "    outcome_order = [\"Total WMH\", \"Periventricular WMH\", \"Deep WMH\"]\n",
    "\n",
    "    rows = []\n",
    "    for outcome in outcome_order:\n",
    "        for c in cohort_order:\n",
    "            if c not in all_results or all_results[c].empty:\n",
    "                continue\n",
    "            df = all_results[c]\n",
    "\n",
    "            row = df[df[\"Outcome\"] == outcome]\n",
    "            if row.empty:\n",
    "                continue\n",
    "\n",
    "            r = row.iloc[0]\n",
    "            rows.append({\n",
    "                \"Outcome\": outcome,\n",
    "                \"Cohort\": c,\n",
    "                \"%Δ (95% CI)\": f\"{r['% Change']:.1f}% ({r['% CI Lower']:.1f}, {r['% CI Upper']:.1f})\",\n",
    "                \"p\": \"<0.001\" if r[\"p_value\"] < 0.001 else f\"{r['p_value']:.3f}\",\n",
    "                \"q\": \"—\" if pd.isna(r.get(\"q_value\")) else (\n",
    "                    \"<0.001\" if r[\"q_value\"] < 0.001 else f\"{r['q_value']:.3f}\"\n",
    "                )\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "\n",
    "    # ---- 排序 ----\n",
    "    if not df_out.empty:\n",
    "        df_out[\"Outcome\"] = pd.Categorical(df_out[\"Outcome\"], categories=outcome_order, ordered=True)\n",
    "        df_out[\"Cohort\"] = pd.Categorical(df_out[\"Cohort\"], categories=cohort_order, ordered=True)\n",
    "        df_out = df_out.sort_values([\"Outcome\", \"Cohort\"]).reset_index(drop=True)\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(outdir, \"Combined_FiveCohort_Table.csv\")\n",
    "    df_out.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] Five-cohort CSV saved: {csv_path}\")\n",
    "\n",
    "    # Save Word\n",
    "    if Document is not None and not df_out.empty:\n",
    "        doc = Document()\n",
    "        title = doc.add_paragraph()\n",
    "        r = title.add_run(\"Table Sx. Combined sensitivity analyses of WMH (five cohorts)\")\n",
    "        r.bold = True\n",
    "        title.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "        cols = [\"Outcome\", \"Cohort\", \"%Δ (95% CI)\", \"p\", \"q\"]\n",
    "        t = doc.add_table(rows=1, cols=len(cols))\n",
    "        hdr = t.rows[0].cells\n",
    "        for i, c in enumerate(cols):\n",
    "            hdr[i].text = c\n",
    "\n",
    "        for _, row in df_out.iterrows():\n",
    "            cells = t.add_row().cells\n",
    "            for j, c in enumerate(cols):\n",
    "                cells[j].text = str(row[c])\n",
    "\n",
    "        # numeric right align\n",
    "        for row in t.rows[1:]:\n",
    "            for j in [2, 3, 4]:\n",
    "                for p in row.cells[j].paragraphs:\n",
    "                    p.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "\n",
    "        apply_three_line_table(t, header_row_idx=0)\n",
    "\n",
    "        legend = doc.add_paragraph()\n",
    "        legend.add_run(\n",
    "            \"Legend: Adjusted between-group differences in log1p WMH expressed as percent change \"\n",
    "            \"(Study vs Control) with 95% confidence intervals. Cohorts include Primary (main analysis), \"\n",
    "            \"Matched Sensitivity (with neuro), ATO Symmetric exclusion, ATO No exclusion, and Regression-adjusted. \"\n",
    "            \"Models adjusted for prespecified covariates; cluster-robust or HC3 SEs applied. \"\n",
    "            \"FDR (q) is applied only within secondary outcomes (Periventricular, Deep); \"\n",
    "            \"Total WMH is primary outcome and not FDR-adjusted.\"\n",
    "        ).italic = True\n",
    "\n",
    "        word_path = os.path.join(outdir, \"Combined_FiveCohort_Table.docx\")\n",
    "        doc.save(word_path)\n",
    "        print(f\"[OK] Five-cohort Word saved: {word_path}\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "# ---------------- RUN COMBINED 5-COHORT TABLE ----------------\n",
    "try:\n",
    "    # === Load already computed Primary & Sensitivity results ===\n",
    "    primary_file = \"Primary_1_10_NoNeuro_LOG1P_results_gatekept.csv\"\n",
    "    sensitivity_file = \"Sensitivity_1_10_WithNeuro_LOG1P_results_gatekept.csv\"\n",
    "\n",
    "    if os.path.exists(primary_file):\n",
    "        df_p = pd.read_csv(primary_file)\n",
    "        res_by_label[\"Primary 1:10 (NoNeuro)\"] = df_p\n",
    "\n",
    "    if os.path.exists(sensitivity_file):\n",
    "        df_s = pd.read_csv(sensitivity_file)\n",
    "        res_by_label[\"Sensitivity 1:10 (WithNeuro)\"] = df_s\n",
    "\n",
    "    # Run five-cohort table\n",
    "    combined5 = make_five_cohort_table(res_by_label, OUT_DIR)\n",
    "    if not combined5.empty:\n",
    "        print(combined5.head(10))  \n",
    "except Exception as e:\n",
    "    print(f\"Five-cohort combined table failed: {e}\")\n",
    "\n",
    "# === Baseline Table Export for Supplementary Sensitivity Cohorts ===\n",
    "\"\"\"\n",
    "This block generates baseline descriptive tables for the three supplementary\n",
    "sensitivity cohorts (ATO–restricted, ATO–full, Regression-adjusted).\n",
    "Output style matches the main Table 1 (three-line table, right-aligned numeric).\n",
    "Exports: CSV + Word (Table Sx).\n",
    "\"\"\"\n",
    "\n",
    "SUPP_OUT_DIR = os.path.join(OUT_DIR, \"Supplementary_Baseline\")\n",
    "os.makedirs(SUPP_OUT_DIR, exist_ok=True)\n",
    "\n",
    "supp_file_sets = {\n",
    "    \"ATO–restricted cohort\": \"ato_sensitivity_sym.csv\",\n",
    "    \"ATO–full cohort\":       \"ato_sensitivity_noexclusion.csv\",\n",
    "    \"Regression-adjusted\":   \"data_processed.csv\"\n",
    "}\n",
    "\n",
    "all_supp_tbls = []\n",
    "\n",
    "for label, file in supp_file_sets.items():\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"[Skip] File not found: {file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Baseline table for {label} ===\")\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = df.columns.str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "\n",
    "    # Ensure group variable\n",
    "    if \"group\" not in df.columns:\n",
    "        if \"treatment_var\" in df.columns:\n",
    "            df[\"group\"] = df[\"treatment_var\"].map({0: \"Control\", 1: \"Study\"})\n",
    "        else:\n",
    "            print(f\"[Skip] Missing group variable in {file}\")\n",
    "            continue\n",
    "\n",
    "    # Impute covariates (same rules as main)\n",
    "    present_adj = [v for v in BASE_ADJ if v in df.columns]\n",
    "    dmod = df.copy()\n",
    "    for v in present_adj:\n",
    "        if pd.api.types.is_numeric_dtype(dmod[v]):\n",
    "            dmod[v] = pd.to_numeric(dmod[v], errors=\"coerce\").fillna(dmod[v].median())\n",
    "        else:\n",
    "            mode_vals = dmod[v].mode(dropna=True)\n",
    "            dmod[v] = dmod[v].fillna(mode_vals.iloc[0] if not mode_vals.empty else dmod[v])\n",
    "\n",
    "    dmod = dmod.dropna(subset=[\"group\"])\n",
    "\n",
    "    # --- Build Table 1 using the same helper ---\n",
    "    tbl = make_table1(dmod, analysis_label=label, id_col=\"Participant_ID\")\n",
    "\n",
    "    # Save CSV\n",
    "    safe = re.sub(r\"[^0-9a-zA-Z]+\", \"_\", label).strip(\"_\")\n",
    "    csv_path = os.path.join(SUPP_OUT_DIR, f\"{safe}_Table1_Baseline.csv\")\n",
    "    tbl.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] Saved CSV: {csv_path}\")\n",
    "\n",
    "    # Save Word\n",
    "    docx_path = export_table1_word(tbl, analysis_label=label,outdir=SUPP_OUT_DIR)\n",
    "    if docx_path:\n",
    "        print(f\"[OK] Saved Word: {docx_path}\")\n",
    "\n",
    "    all_supp_tbls.append(tbl)\n",
    "\n",
    "# --- Combine all three supplementary cohorts into one Word table ---\n",
    "if all_supp_tbls and Document is not None:\n",
    "    df_combined = pd.concat(all_supp_tbls, ignore_index=True)\n",
    "\n",
    "    doc = Document()\n",
    "    r = doc.add_paragraph().add_run(\"Table Sx. Baseline characteristics — Supplementary sensitivity cohorts\")\n",
    "    r.bold = True\n",
    "    doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "\n",
    "    cols = [\"Analysis\", \"Characteristic\", \"Study\", \"Control\"]\n",
    "    if TABLE1_USE_PVALUE: cols.append(\"p_value\")\n",
    "    if TABLE1_INCLUDE_SMD: cols.append(\"SMD\")\n",
    "\n",
    "    t = doc.add_table(rows=1, cols=len(cols))\n",
    "    hdr = t.rows[0].cells\n",
    "    for i, c in enumerate(cols):\n",
    "        hdr[i].text = c\n",
    "\n",
    "    for _, row in df_combined[cols].iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        for j, c in enumerate(cols):\n",
    "            val = row[c] if c in row else \"\"\n",
    "            if c == \"p_value\" and isinstance(val, float):\n",
    "                cells[j].text = \"<0.001\" if val < 0.001 else f\"{val:.3f}\"\n",
    "            else:\n",
    "                cells[j].text = \"\" if pd.isna(val) else str(val)\n",
    "\n",
    "    # Right-align numeric\n",
    "    num_idx = list(range(2, len(cols)))\n",
    "    _right_align_numeric(t, num_idx)\n",
    "\n",
    "    # Apply three-line + font\n",
    "    if THREELINE_TABLES:\n",
    "        apply_three_line_table(t)\n",
    "    set_table_font(t, font_name=\"Times New Roman\", font_size=10)\n",
    "\n",
    "    legend = doc.add_paragraph()\n",
    "    legend.add_run(\n",
    "        \"Legend: Values are mean (SD), median [IQR], or n (%). \"\n",
    "        \"p-values from Welch’s t test (continuous), Mann–Whitney U (alcohol intake), \"\n",
    "        \"and χ² or Fisher’s exact test (categorical). \"\n",
    "        \"Study = sleep apnea; Control = matched controls.\"\n",
    "    ).italic = True\n",
    "\n",
    "    combined_docx = os.path.join(SUPP_OUT_DIR, \"TableSx_Baseline_Supplementary.docx\")\n",
    "    doc.save(combined_docx)\n",
    "    print(f\"[OK] Combined Word table saved: {combined_docx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "562c410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: primary_cohort.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\46474624.py:102: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(infile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved Study-only–stratified file: primary_cohort_stratified.csv\n",
      "\n",
      "[Study-only summaries]\n",
      "SA_ascertain_group:\n",
      " SA_ascertain_group\n",
      "Hospital_Secondary    394\n",
      "Hospital_Primary      313\n",
      "Self_Report_Only      148\n",
      "Name: count, dtype: int64\n",
      "Sex:\n",
      " Sex\n",
      "1    626\n",
      "0    229\n",
      "Name: count, dtype: int64\n",
      "SA_MRI_interval_stratum:\n",
      " SA_MRI_interval_stratum\n",
      "LE10_years    452\n",
      "GT10_years    401\n",
      "NaN             2\n",
      "Name: count, dtype: int64\n",
      "Snoring_Stratum:\n",
      " Snoring_Stratum\n",
      "1    609\n",
      "0    246\n",
      "Name: count, dtype: Int64\n",
      "Sleepy_SA_Stratum:\n",
      " Sleepy_SA_Stratum\n",
      "0    500\n",
      "1    355\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "[Info] Self_Report_Only participants: 148 / 855 (17.3%) within Study group\n",
      "\n",
      "=== Processing: sensitivity_cohort.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\46474624.py:102: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(infile)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved Study-only–stratified file: sensitivity_cohort_stratified.csv\n",
      "\n",
      "[Study-only summaries]\n",
      "SA_ascertain_group:\n",
      " SA_ascertain_group\n",
      "Hospital_Secondary    409\n",
      "Hospital_Primary      321\n",
      "Self_Report_Only      150\n",
      "Name: count, dtype: int64\n",
      "Sex:\n",
      " Sex\n",
      "1    644\n",
      "0    236\n",
      "Name: count, dtype: int64\n",
      "SA_MRI_interval_stratum:\n",
      " SA_MRI_interval_stratum\n",
      "LE10_years    473\n",
      "GT10_years    405\n",
      "NaN             2\n",
      "Name: count, dtype: int64\n",
      "Snoring_Stratum:\n",
      " Snoring_Stratum\n",
      "1    631\n",
      "0    249\n",
      "Name: count, dtype: Int64\n",
      "Sleepy_SA_Stratum:\n",
      " Sleepy_SA_Stratum\n",
      "0    515\n",
      "1    365\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "[Info] Self_Report_Only participants: 150 / 880 (17.0%) within Study group\n"
     ]
    }
   ],
   "source": [
    "# Stratify process\n",
    "# ==============================================================================================\n",
    "# This script:\n",
    "#   • Reuses cleaning outputs (SA_ascertain_group, Years_since_sleep_disorder) or derives them if missing.\n",
    "#   • Creates Study-only stratification columns (Controls = NaN):\n",
    "#       - SA_MRI_interval_stratum   (LE10_years / GT10_years)\n",
    "#       - Snoring_Stratum\n",
    "#       - Sleepy_SA_Stratum\n",
    "#   • Additionally prints summary counts for:\n",
    "#       - SA_ascertain_group (source of SA diagnosis)\n",
    "#       - Sex (sex distribution)\n",
    "#       - Self_Report_Only proportion within Study group\n",
    "#   • Saves *_stratified.csv for downstream analyses.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ---------- Input & Output ----------\n",
    "cohorts = {\n",
    "    \"primary_cohort.csv\": \"primary_cohort_stratified.csv\",\n",
    "    \"sensitivity_cohort.csv\": \"sensitivity_cohort_stratified.csv\"\n",
    "}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = (df.columns\n",
    "              .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "              .str.replace(r\"_{2,}\", \"_\", regex=True)\n",
    "              .str.strip(\"_\"))\n",
    "    out = df.copy(); out.columns = cols\n",
    "    return out\n",
    "\n",
    "def _derive_sa_flags_and_groups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Derive group, treatment_var, and SA_ascertain_group (same as cleaning).\"\"\"\n",
    "    out = df.copy()\n",
    "    col_any, col_main, col_sec = \"Diagnoses_ICD10\", \"Diagnoses_main_ICD10\", \"Diagnoses_secondary_ICD10\"\n",
    "\n",
    "    sr_pat = re.compile(r\"^Non_cancer_illness_code_self_reported_Instance_[0-2]_Array_([0-9]|[1-2][0-9]|3[0-3])$\")\n",
    "    self_report_cols = [c for c in out.columns if sr_pat.match(c)]\n",
    "    if self_report_cols:\n",
    "        sr_block = out[self_report_cols].astype(str)\n",
    "        sr_has_1123 = sr_block.apply(lambda s: s.eq(\"1123\")).any(axis=1)\n",
    "    else:\n",
    "        sr_has_1123 = pd.Series(False, index=out.index)\n",
    "\n",
    "    has_any  = out.get(col_any,  pd.Series(\"\", index=out.index)).fillna(\"\").str.contains(\"G473\", regex=False)\n",
    "    has_main = out.get(col_main, pd.Series(\"\", index=out.index)).fillna(\"\").str.contains(\"G473\", regex=False)\n",
    "    has_sec  = out.get(col_sec,  pd.Series(\"\", index=out.index)).fillna(\"\").str.contains(\"G473\", regex=False)\n",
    "\n",
    "    out[\"treatment_var\"] = np.where(has_any | sr_has_1123, 1, 0)\n",
    "    out[\"group\"] = np.where(out[\"treatment_var\"].eq(1), \"Study\", \"Control\")\n",
    "\n",
    "    out[\"SA_ascertain_group\"] = pd.NA\n",
    "    out.loc[out[\"group\"] == \"Control\", \"SA_ascertain_group\"] = \"No_SA\"\n",
    "    out.loc[(out[\"group\"] == \"Study\") & has_main,              \"SA_ascertain_group\"] = \"Hospital_Primary\"\n",
    "    out.loc[(out[\"group\"] == \"Study\") & (~has_main) & has_sec, \"SA_ascertain_group\"] = \"Hospital_Secondary\"\n",
    "    out.loc[(out[\"group\"] == \"Study\") & (~has_any),            \"SA_ascertain_group\"] = \"Self_Report_Only\"\n",
    "    return out\n",
    "\n",
    "def _ensure_years_since_sa(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure Years_since_sleep_disorder exists (same exclusion rules as cleaning).\"\"\"\n",
    "    out = df.copy()\n",
    "    mri_col = \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "    sa_col  = \"Sleep_Disorder_Diagnosis_Date\"\n",
    "\n",
    "    if \"Years_since_sleep_disorder\" not in out.columns:\n",
    "        if mri_col in out.columns:\n",
    "            out[mri_col] = pd.to_datetime(out[mri_col], errors=\"coerce\")\n",
    "        if sa_col in out.columns:\n",
    "            out[sa_col] = pd.to_datetime(out[sa_col], errors=\"coerce\")\n",
    "\n",
    "        if (mri_col in out.columns) and (sa_col in out.columns):\n",
    "            delta_days = (out[mri_col] - out[sa_col]).dt.days\n",
    "            out[\"Years_since_sleep_disorder\"] = np.where(\n",
    "                (~out[mri_col].isna()) & (~out[sa_col].isna()),\n",
    "                delta_days / 365.25,\n",
    "                np.nan\n",
    "            )\n",
    "        else:\n",
    "            out[\"Years_since_sleep_disorder\"] = np.nan\n",
    "\n",
    "    if \"group\" not in out.columns or \"treatment_var\" not in out.columns:\n",
    "        out = _derive_sa_flags_and_groups(out)\n",
    "\n",
    "    neg_mask     = out[\"Years_since_sleep_disorder\"] < 0\n",
    "    study_mask   = out.get(\"treatment_var\", pd.Series(dtype=int)) == 1\n",
    "    control_mask = out.get(\"treatment_var\", pd.Series(dtype=int)) == 0\n",
    "\n",
    "    out.loc[control_mask & neg_mask, \"Years_since_sleep_disorder\"] = np.nan\n",
    "    excl_n = int((study_mask & neg_mask).sum())\n",
    "    if excl_n > 0:\n",
    "        out = out[~(study_mask & neg_mask)]\n",
    "        print(f\"[info] Dropped {excl_n} Study participants with negative Years_since_sleep_disorder (<0).\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- Main ----------\n",
    "for infile, outfile in cohorts.items():\n",
    "    print(f\"\\n=== Processing: {infile} ===\")\n",
    "    df = pd.read_csv(infile)\n",
    "    df = _clean_cols(df)\n",
    "\n",
    "    # Reuse or derive SA grouping\n",
    "    if (\"SA_ascertain_group\" not in df.columns) or (\"group\" not in df.columns) or (\"treatment_var\" not in df.columns):\n",
    "        df = _derive_sa_flags_and_groups(df)\n",
    "\n",
    "    # Ensure Years_since_sleep_disorder consistent with cleaning\n",
    "    df = _ensure_years_since_sa(df)\n",
    "\n",
    "    is_study = df[\"group\"].eq(\"Study\")\n",
    "\n",
    "    # --- 1) SA_MRI_interval_stratum (Study only) ---\n",
    "    df[\"SA_MRI_interval_stratum\"] = pd.Series(pd.array([pd.NA] * len(df), dtype=\"object\"))\n",
    "    if \"Years_since_sleep_disorder\" in df.columns:\n",
    "        bins = [-float(\"inf\"), 10, float(\"inf\")]\n",
    "        labels = [\"LE10_years\", \"GT10_years\"]\n",
    "        df.loc[is_study, \"SA_MRI_interval_stratum\"] = pd.cut(\n",
    "            df.loc[is_study, \"Years_since_sleep_disorder\"],\n",
    "            bins=bins, labels=labels\n",
    "        )\n",
    "\n",
    "    # --- 2) Snoring_Stratum (Study only) ---\n",
    "    df[\"Snoring_Stratum\"] = pd.Series(pd.array([pd.NA] * len(df), dtype=\"Int64\"))\n",
    "    if \"Snoring_Group\" in df.columns:\n",
    "        df.loc[is_study, \"Snoring_Stratum\"] = df.loc[is_study, \"Snoring_Group\"].astype(\"Int64\")\n",
    "    elif \"Snoring_Instance_0\" in df.columns:\n",
    "        df.loc[is_study, \"Snoring_Stratum\"] = (df.loc[is_study, \"Snoring_Instance_0\"] == 1).astype(\"Int64\")\n",
    "\n",
    "    # --- 3) Sleepy_SA_Stratum (Study only) ---\n",
    "    df[\"Sleepy_SA_Stratum\"] = pd.Series(pd.array([pd.NA] * len(df), dtype=\"Int64\"))\n",
    "    if \"Daytime_dozing_sleeping_Instance_0\" in df.columns:\n",
    "        df.loc[is_study, \"Sleepy_SA_Stratum\"] = df.loc[is_study, \"Daytime_dozing_sleeping_Instance_0\"].isin([1, 2]).astype(\"Int64\")\n",
    "\n",
    "    # --- Save ---\n",
    "    df.to_csv(outfile, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] Saved Study-only–stratified file: {outfile}\")\n",
    "\n",
    "    # --- Summaries (Study only) ---\n",
    "    print(\"\\n[Study-only summaries]\")\n",
    "    for col in [\"SA_ascertain_group\", \"Sex\", \"SA_MRI_interval_stratum\", \"Snoring_Stratum\", \"Sleepy_SA_Stratum\"]:\n",
    "        if col in df.columns:\n",
    "            print(f\"{col}:\\n\", df.loc[is_study, col].value_counts(dropna=False))\n",
    "        else:\n",
    "            print(f\"{col}: (not present)\")\n",
    "\n",
    "    # --- Extra: Self-report proportion within Study group ---\n",
    "    if \"SA_ascertain_group\" in df.columns:\n",
    "        total_study = is_study.sum()\n",
    "        self_report_n = (df.loc[is_study, \"SA_ascertain_group\"] == \"Self_Report_Only\").sum()\n",
    "        prop = (self_report_n / total_study * 100) if total_study > 0 else np.nan\n",
    "        print(f\"\\n[Info] Self_Report_Only participants: {self_report_n} / {total_study} ({prop:.1f}%) within Study group\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c3d1a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Primary (primary_cohort_stratified.csv) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4162120285.py:441: DtypeWarning: Columns (18,24,25,33,40,43,44,187) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Sex --\n",
      "Stratification Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "           Sex  Female           Total WMH   12.5176   -0.3369    27.0301 0.0567            NaN                NaN 229/855\n",
      "           Sex    Male           Total WMH   10.6022    2.8590    18.9282 0.0065            NaN                NaN 626/855\n",
      "           Sex  Female Periventricular WMH   10.2908   -1.6082    23.6289 0.0926         0.0926                NaN 229/855\n",
      "           Sex    Male Periventricular WMH   10.2983    2.8333    18.3051 0.0061         0.0122                NaN 626/855\n",
      "           Sex  Female            Deep WMH   26.1362    5.5543    50.7313 0.0106         0.0213                NaN 229/855\n",
      "           Sex    Male            Deep WMH   12.0851    0.5634    24.9268 0.0393         0.0393                NaN 626/855\n",
      "\n",
      "-- Daytime Sleepiness --\n",
      "    Stratification    Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "Daytime Sleepiness Not Sleepy           Total WMH   11.8178    2.9843    21.4089 0.0078            NaN                NaN 500/855\n",
      "Daytime Sleepiness     Sleepy           Total WMH   10.5981    1.0438    21.0557 0.0289            NaN                NaN 355/855\n",
      "Daytime Sleepiness Not Sleepy Periventricular WMH   10.7140    2.3479    19.7639 0.0111         0.0111                NaN 500/855\n",
      "Daytime Sleepiness     Sleepy Periventricular WMH   10.5953    1.3935    20.6321 0.0231         0.0461                NaN 355/855\n",
      "Daytime Sleepiness Not Sleepy            Deep WMH   17.9239    4.5204    33.0461 0.0074         0.0111                NaN 500/855\n",
      "Daytime Sleepiness     Sleepy            Deep WMH   11.6234   -2.5226    27.8221 0.1117         0.1117                NaN 355/855\n",
      "\n",
      "-- Snoring Status --\n",
      "Stratification  Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "Snoring Status No Snore           Total WMH   14.8265    2.3302    28.8488 0.0187            NaN                NaN 246/855\n",
      "Snoring Status  Snoring           Total WMH   10.0447    2.3908    18.2707 0.0093            NaN                NaN 609/855\n",
      "Snoring Status No Snore Periventricular WMH   12.2998    0.7492    25.1747 0.0362         0.0362                NaN 246/855\n",
      "Snoring Status  Snoring Periventricular WMH   10.0673    2.6801    17.9860 0.0068         0.0136                NaN 609/855\n",
      "Snoring Status No Snore            Deep WMH   25.0591    4.7223    49.3454 0.0135         0.0271                NaN 246/855\n",
      "Snoring Status  Snoring            Deep WMH   11.8573    0.7458    24.1943 0.0358         0.0358                NaN 609/855\n",
      "\n",
      "-- Diagnosis–MRI Interval --\n",
      "        Stratification Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "Diagnosis–MRI Interval   >10 y           Total WMH    7.6154   -1.8385    17.9798 0.1177            NaN                NaN 401/853\n",
      "Diagnosis–MRI Interval   ≤10 y           Total WMH   15.2897    6.3038    25.0351 0.0006            NaN                NaN 452/853\n",
      "Diagnosis–MRI Interval   >10 y Periventricular WMH    6.0379   -2.8728    15.7660 0.1905         0.1905                NaN 401/853\n",
      "Diagnosis–MRI Interval   ≤10 y Periventricular WMH   15.2987    6.6326    24.6690 0.0004         0.0007                NaN 452/853\n",
      "Diagnosis–MRI Interval   >10 y            Deep WMH   13.2073   -1.1599    29.6628 0.0732         0.1464                NaN 401/853\n",
      "Diagnosis–MRI Interval   ≤10 y            Deep WMH   18.5090    5.1455    33.5708 0.0054         0.0054                NaN 452/853\n",
      "\n",
      "-- SA Diagnosis Method --\n",
      "     Stratification        Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "SA Diagnosis Method   Hosp-Primary           Total WMH   10.9051    0.2026    22.7507 0.0456            NaN                NaN 313/855\n",
      "SA Diagnosis Method Hosp-Secondary           Total WMH    9.9546    0.4665    20.3386 0.0393            NaN                NaN 394/855\n",
      "SA Diagnosis Method    Self-Report           Total WMH   17.5090    2.0311    35.3349 0.0252            NaN                NaN 148/855\n",
      "SA Diagnosis Method   Hosp-Primary Periventricular WMH    9.8264   -0.4483    21.1616 0.0614         0.0614                NaN 313/855\n",
      "SA Diagnosis Method Hosp-Secondary Periventricular WMH    9.2914    0.2838    19.1081 0.0429         0.0452                NaN 394/855\n",
      "SA Diagnosis Method    Self-Report Periventricular WMH   17.4948    2.7590    34.3436 0.0184         0.0367                NaN 148/855\n",
      "SA Diagnosis Method   Hosp-Primary            Deep WMH   15.6118   -0.3817    34.1730 0.0562         0.0614                NaN 313/855\n",
      "SA Diagnosis Method Hosp-Secondary            Deep WMH   14.6366    0.2936    31.0308 0.0452         0.0452                NaN 394/855\n",
      "SA Diagnosis Method    Self-Report            Deep WMH   19.0187   -3.7254    47.1360 0.1076         0.1076                NaN 148/855\n",
      "\n",
      "===== Sensitivity (sensitivity_cohort_stratified.csv) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\4162120285.py:441: DtypeWarning: Columns (24,25,33,43,187) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Sex --\n",
      "Stratification Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "           Sex  Female           Total WMH   19.0661    6.0370    33.6962 0.0032            NaN                NaN 236/880\n",
      "           Sex    Male           Total WMH   10.3895    2.5732    18.8014 0.0083            NaN                NaN 644/880\n",
      "           Sex  Female Periventricular WMH   15.0309    3.0713    28.3782 0.0124            NaN             0.0124 236/880\n",
      "           Sex    Male Periventricular WMH    9.8467    2.3673    17.8725 0.0090            NaN             0.0181 644/880\n",
      "           Sex  Female            Deep WMH   39.2812   17.5012    65.0984 0.0001            NaN             0.0003 236/880\n",
      "           Sex    Male            Deep WMH   12.5186    0.8794    25.5007 0.0342            NaN             0.0342 644/880\n",
      "\n",
      "-- Daytime Sleepiness --\n",
      "    Stratification    Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "Daytime Sleepiness Not Sleepy           Total WMH   11.4648    2.7437    20.9262 0.0090            NaN                NaN 515/880\n",
      "Daytime Sleepiness     Sleepy           Total WMH   12.5401    2.8384    23.1569 0.0102            NaN                NaN 365/880\n",
      "Daytime Sleepiness Not Sleepy Periventricular WMH   10.3607    2.1382    19.2451 0.0126            NaN             0.0126 515/880\n",
      "Daytime Sleepiness     Sleepy Periventricular WMH   11.2890    2.0500    21.3643 0.0156            NaN             0.0156 365/880\n",
      "Daytime Sleepiness Not Sleepy            Deep WMH   17.5984    4.2473    32.6594 0.0084            NaN             0.0126 515/880\n",
      "Daytime Sleepiness     Sleepy            Deep WMH   18.4015    3.3969    35.5834 0.0146            NaN             0.0156 365/880\n",
      "\n",
      "-- Snoring Status --\n",
      "Stratification  Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "Snoring Status No Snore           Total WMH   15.1640    2.9216    28.8627 0.0138            NaN                NaN 249/880\n",
      "Snoring Status  Snoring           Total WMH   10.3175    2.6531    18.5542 0.0075            NaN                NaN 631/880\n",
      "Snoring Status No Snore Periventricular WMH   12.6364    1.3061    25.2338 0.0278            NaN             0.0278 249/880\n",
      "Snoring Status  Snoring Periventricular WMH    9.7625    2.4475    17.5998 0.0081            NaN             0.0134 631/880\n",
      "Snoring Status No Snore            Deep WMH   25.6297    5.6623    49.3705 0.0098            NaN             0.0196 249/880\n",
      "Snoring Status  Snoring            Deep WMH   14.2736    2.8061    27.0202 0.0134            NaN             0.0134 631/880\n",
      "\n",
      "-- Diagnosis–MRI Interval --\n",
      "        Stratification Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "Diagnosis–MRI Interval   >10 y           Total WMH    9.0681   -0.5259    19.5874 0.0646            NaN                NaN 405/878\n",
      "Diagnosis–MRI Interval   ≤10 y           Total WMH   14.4344    5.5308    24.0891 0.0011            NaN                NaN 473/878\n",
      "Diagnosis–MRI Interval   >10 y Periventricular WMH    7.5832   -1.4524    17.4474 0.1025            NaN             0.1025 405/878\n",
      "Diagnosis–MRI Interval   ≤10 y Periventricular WMH   13.5891    5.1378    22.7198 0.0012            NaN             0.0024 473/878\n",
      "Diagnosis–MRI Interval   >10 y            Deep WMH   14.6103    0.0475    31.2927 0.0492            NaN             0.0984 405/878\n",
      "Diagnosis–MRI Interval   ≤10 y            Deep WMH   20.7340    6.8937    36.3663 0.0024            NaN             0.0024 473/878\n",
      "\n",
      "-- SA Diagnosis Method --\n",
      "     Stratification        Stratum             Outcome  % change  % CI Low  % CI High      p  q_FDR_primary  q_FDR_sensitivity       N\n",
      "SA Diagnosis Method   Hosp-Primary           Total WMH   12.7124    1.5213    25.1371 0.0249            NaN                NaN 321/880\n",
      "SA Diagnosis Method Hosp-Secondary           Total WMH    9.4671    0.1744    19.6219 0.0457            NaN                NaN 409/880\n",
      "SA Diagnosis Method    Self-Report           Total WMH   18.0703    3.2880    34.9683 0.0149            NaN                NaN 150/880\n",
      "SA Diagnosis Method   Hosp-Primary Periventricular WMH   10.9365    0.4158    22.5595 0.0412            NaN             0.0412 321/880\n",
      "SA Diagnosis Method Hosp-Secondary Periventricular WMH    8.6310   -0.1985    18.2417 0.0556            NaN             0.0556 409/880\n",
      "SA Diagnosis Method    Self-Report Periventricular WMH   17.6667    3.5762    33.6739 0.0124            NaN             0.0248 150/880\n",
      "SA Diagnosis Method   Hosp-Primary            Deep WMH   21.3574    4.0660    41.5218 0.0136            NaN             0.0272 321/880\n",
      "SA Diagnosis Method Hosp-Secondary            Deep WMH   14.9034    0.5569    31.2967 0.0412            NaN             0.0556 409/880\n",
      "SA Diagnosis Method    Self-Report            Deep WMH   18.3245   -3.2023    44.6386 0.1005            NaN             0.1005 150/880\n",
      "Combined CSV saved: Stratify\\Stratified_AllCohorts_Combined.csv\n",
      "Word table saved: Stratify\\Stratified_AllCohorts_Supplement.docx\n",
      "\n",
      "All stratified outputs saved under: c:\\Users\\M328449\\OneDrive - Mayo Clinic\\Documents\\Python\\SA_WMH_Pub\\Stratify\n"
     ]
    }
   ],
   "source": [
    "# Stratified OLS of HSN-WMH (log1p) with in-stratum BH-FDR\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Updates per request:\n",
    "#   1) Keep the stratification formerly called \"SA severity\" but RENAME it to\n",
    "#      \"SA diagnosis method\" using the column SA_ascertain_group with levels:\n",
    "#         [\"Self_Report_Only\", \"Hospital_Secondary\", \"Hospital_Primary\"].\n",
    "#      As with other strata, only matched sets where the Study arm belongs to\n",
    "#      a given level are included for that level.\n",
    "#   2) Do NOT add CMC as an adjustment covariate; legends and models have no CMC.\n",
    "#\n",
    "# Analysis notes:\n",
    "#   • Within each stratum level, apply BH-FDR to {Periventricular WMH, Deep WMH};\n",
    "#     Total WMH is excluded from FDR (q = NaN).\n",
    "#   • Cohorts: primary_cohort_stratified.csv & sensitivity_cohort_stratified.csv.\n",
    "#   • Outputs under Stratify/<Cohort>; a combined CSV & Word table under Stratify/.\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ---------- Optional Word export ----------\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docx.oxml import OxmlElement\n",
    "    from docx.oxml.ns import qn\n",
    "    from docx.shared import Pt\n",
    "except Exception:\n",
    "    Document = None\n",
    "\n",
    "# ---------------- Paths & cohorts ----------------\n",
    "COHORTS = {\n",
    "    \"Primary\":     \"primary_cohort_stratified.csv\",\n",
    "    \"Sensitivity\": \"sensitivity_cohort_stratified.csv\",\n",
    "}\n",
    "OUTDIR = \"Stratify\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---- Export defaults ----\n",
    "SAVEFIG_DPI = 600\n",
    "PNG_KW  = {\"dpi\": SAVEFIG_DPI, \"bbox_inches\": \"tight\", \"facecolor\": \"white\"}\n",
    "PDF_KW  = {\"bbox_inches\": \"tight\"}\n",
    "TIFF_KW = {\"format\": \"tiff\", \"dpi\": SAVEFIG_DPI, \"bbox_inches\": \"tight\",\n",
    "           \"facecolor\": \"white\", \"pil_kwargs\": {\"compression\": \"tiff_lzw\"}}\n",
    "\n",
    "# ---------------- Columns & settings ----------------\n",
    "GROUP_COL, MATCH_ID = \"group\", \"match_id\"\n",
    "SCALE_COL = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "RAW_DEEP  = \"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\"\n",
    "RAW_PV    = \"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\"\n",
    "RAW_TOT   = (\"Total_volume_of_white_matter_hyperintensities_from_T1_\"\n",
    "             \"and_T2_FLAIR_images_Instance_2\")\n",
    "\n",
    "OUTCOME_BUILD = [\n",
    "    (\"Log_HSNorm_Total_WMH\",           RAW_TOT,  \"Total WMH\"),\n",
    "    (\"Log_HSNorm_PeriVentricular_WMH\", RAW_PV,   \"Periventricular WMH\"),\n",
    "    (\"Log_HSNorm_Deep_WMH\",            RAW_DEEP, \"Deep WMH\"),\n",
    "]\n",
    "OUTCOME_ORDER = [\"Total WMH\", \"Periventricular WMH\", \"Deep WMH\"]\n",
    "\n",
    "# Prespecified covariates (NO CMC here)\n",
    "ADJUST_ALL = [\n",
    "    \"Sex\",\"Age_at_Instance_2\",\"Townsend_deprivation_index_at_recruitment\",\n",
    "    \"Body_mass_index_BMI_Instance_0\",\"Genetic_ethnic_grouping\",\n",
    "    \"Smoking_Ever\",\"Alcohol_intake_frequency_ordinal\",\n",
    "]\n",
    "CATEGORICAL_ALL = {\"Sex\",\"Genetic_ethnic_grouping\",\"Smoking_Ever\"}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def safe_name(s: str) -> str:\n",
    "    return re.sub(r\"[^0-9A-Za-z._-]+\", \"_\", str(s))\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns=lambda c: pd.Series(c).str.replace(r\"[^0-9a-zA-Z]+\",\"_\", regex=True).iloc[0])\n",
    "\n",
    "def ensure_outcomes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if GROUP_COL in df.columns:\n",
    "        df[GROUP_COL] = pd.Categorical(df[GROUP_COL], [\"Control\",\"Study\"])\n",
    "    scale = pd.to_numeric(df.get(SCALE_COL, 1.0), errors=\"coerce\").fillna(1.0)\n",
    "    for out_col, src, _lab in OUTCOME_BUILD:\n",
    "        v = pd.to_numeric(df.get(src), errors=\"coerce\")\n",
    "        v = v * scale\n",
    "        df[out_col] = np.log1p(v)\n",
    "    return df\n",
    "\n",
    "def build_formula(outcome: str, data: pd.DataFrame) -> str:\n",
    "    rhs = [\"C(group)\"]\n",
    "    for v in ADJUST_ALL:\n",
    "        if v in data.columns:\n",
    "            rhs.append(f\"C({v})\" if v in CATEGORICAL_ALL else v)\n",
    "    return f\"{outcome} ~ \" + \" + \".join(rhs)\n",
    "\n",
    "def fit_cluster(formula: str, data: pd.DataFrame):\n",
    "    y, X = dmatrices(formula, data, return_type=\"dataframe\", NA_action=\"drop\")\n",
    "    base = sm.OLS(y, X).fit(cov_type=\"HC3\")\n",
    "    if MATCH_ID not in data.columns:\n",
    "        return base\n",
    "    groups = data.loc[y.index, MATCH_ID].dropna()\n",
    "    if groups.nunique() <= 1:\n",
    "        return base\n",
    "    return sm.OLS(y.loc[groups.index], X.loc[groups.index]).fit(\n",
    "        cov_type=\"cluster\", cov_kwds={\"groups\": groups}\n",
    "    )\n",
    "\n",
    "# ---------------- Forest plot ----------------\n",
    "def forest_plot(df_plot: pd.DataFrame, title: str, out_png: str, use_abbrev=True):\n",
    "    if df_plot is None or df_plot.empty: return\n",
    "    ORDER = [\"Deep WMH\", \"Periventricular WMH\", \"Total WMH\"]\n",
    "    ABBR  = {\"Periventricular WMH\": \"PWMH\", \"Deep WMH\": \"DWMH\"}\n",
    "\n",
    "    dfp = df_plot.copy()\n",
    "    dfp[\"Outcome\"] = pd.Categorical(dfp[\"Outcome\"], categories=ORDER, ordered=True)\n",
    "    dfp = dfp.sort_values([\"Outcome\",\"Stratum\"]).reset_index(drop=True)\n",
    "\n",
    "    def row_color(row):\n",
    "        qcols = [c for c in row.index if c.lower().startswith(\"q_fdr\")]\n",
    "        qvals = [row[c] for c in qcols if pd.notna(row[c])]\n",
    "        if qvals and np.nanmin(qvals) <= 0.05: return \"#1f3b4d\"\n",
    "        if pd.notna(row.get(\"p\")) and float(row[\"p\"]) <= 0.05: return \"#1f3b4d\"\n",
    "        return \"#7f7f7f\"\n",
    "\n",
    "    colors = [row_color(r) for _, r in dfp.iterrows()]\n",
    "    x  = pd.to_numeric(dfp[\"% change\"], errors=\"coerce\").to_numpy()\n",
    "    xl = pd.to_numeric(dfp[\"% CI Low\"], errors=\"coerce\").to_numpy()\n",
    "    xu = pd.to_numeric(dfp[\"% CI High\"], errors=\"coerce\").to_numpy()\n",
    "    n  = len(dfp); y = np.arange(n)\n",
    "\n",
    "    plt.rcParams.update({\"font.family\":\"Arial\",\"font.size\":12,\n",
    "                         \"axes.labelsize\":12,\"xtick.labelsize\":11,\"ytick.labelsize\":11,\n",
    "                         \"savefig.dpi\":SAVEFIG_DPI})\n",
    "    fig = plt.figure(figsize=(6.2, max(3.6, 0.55*n))); ax = plt.gca()\n",
    "\n",
    "    for i in range(n):\n",
    "        if np.isnan(x[i]) or np.isnan(xl[i]) or np.isnan(xu[i]): continue\n",
    "        ax.errorbar(x[i], y[i],\n",
    "                    xerr=[[x[i]-xl[i]], [xu[i]-x[i]]],\n",
    "                    fmt=\"o\", mfc=colors[i], mec=colors[i],\n",
    "                    ecolor=colors[i], elinewidth=1.8, capsize=4, markersize=6.0,\n",
    "                    linestyle=\"none\", zorder=3)\n",
    "\n",
    "    xmin = np.nanmin(xl) if np.isfinite(np.nanmin(xl)) else -1.0\n",
    "    xmax = np.nanmax(xu) if np.isfinite(np.nanmax(xu)) else  1.0\n",
    "    pad  = max(0.12*(xmax-xmin if xmax>xmin else 1.0), 2.0)\n",
    "    ax.set_xlim(xmin - pad, xmax + pad)\n",
    "\n",
    "    def fmt_outcome(o): return ABBR.get(o, o) if use_abbrev else o\n",
    "    ylabels = (dfp[\"Outcome\"].map(fmt_outcome).astype(str) + \" (\" + dfp[\"Stratum\"].astype(str) + \")\").tolist()\n",
    "    ax.set_yticks(y); ax.set_yticklabels(ylabels)\n",
    "\n",
    "    ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    ax.grid(False); ax.spines[\"top\"].set_visible(False); ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlabel(\"% change (SA - Control)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(out_png, **PNG_KW)\n",
    "    plt.savefig(os.path.splitext(out_png)[0] + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(os.path.splitext(out_png)[0] + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------------- Word helpers ----------------\n",
    "def _set_cell_border(cell, **kwargs):\n",
    "    tc = cell._tc\n",
    "    tcPr = tc.get_or_add_tcPr()\n",
    "    tcBorders = tcPr.find(qn('w:tcBorders'))\n",
    "    if tcBorders is None:\n",
    "        tcBorders = OxmlElement('w:tcBorders'); tcPr.append(tcBorders)\n",
    "    for edge in ('left','right','top','bottom','insideH','insideV'):\n",
    "        if edge in kwargs:\n",
    "            edge_data = kwargs.get(edge); tag = OxmlElement(f'w:{edge}')\n",
    "            for key in (\"val\",\"sz\",\"color\",\"space\"):\n",
    "                if key in edge_data: tag.set(qn(f'w:{key}'), str(edge_data[key]))\n",
    "            tcBorders.append(tag)\n",
    "\n",
    "def _apply_three_line_table(table, header_row_idx=0):\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            _set_cell_border(cell, left={\"val\":\"nil\"}, right={\"val\":\"nil\"},\n",
    "                                   top={\"val\":\"nil\"},  bottom={\"val\":\"nil\"})\n",
    "    for cell in table.rows[header_row_idx].cells:\n",
    "        _set_cell_border(cell, top={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"},\n",
    "                              bottom={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "    for cell in table.rows[-1].cells:\n",
    "        _set_cell_border(cell, bottom={\"val\":\"single\",\"sz\":8,\"color\":\"000000\"})\n",
    "\n",
    "def _right_align_numeric(table, numeric_col_idx):\n",
    "    for row in table.rows[1:]:\n",
    "        for j in numeric_col_idx:\n",
    "            if j < len(row.cells):\n",
    "                for p in row.cells[j].paragraphs:\n",
    "                    p.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "\n",
    "def export_stratified_word(df_all: pd.DataFrame, outdir: str,\n",
    "                           title: str = \"eTable. Stratified analyses of WMH\",\n",
    "                           legend: str | None = None,\n",
    "                           filename: str = \"Stratified_AllCohorts_Supplement.docx\"):\n",
    "    if Document is None or df_all is None or df_all.empty:\n",
    "        print(\"Word export skipped.\"); return None\n",
    "\n",
    "    # --- 兼容列名：内部统一使用 df[\"Cohort\"] 处理 ---\n",
    "    df = df_all.copy()\n",
    "    if \"Cohort\" not in df.columns and \"Analytic Cohort\" in df.columns:\n",
    "        df = df.rename(columns={\"Analytic Cohort\": \"Cohort\"})\n",
    "\n",
    "    def _fmt_num(x, nd=1):  return \"\" if pd.isna(x) else f\"{float(x):.{nd}f}\"\n",
    "    def _fmt_p(x):          return \"—\" if pd.isna(x) else (\"<0.001\" if float(x) < 1e-3 else f\"{float(x):.3f}\")\n",
    "    def _fmt_q(x):          return \"—\" if pd.isna(x) else (\"<0.001\" if float(x) < 1e-3 else f\"{float(x):.3f}\")\n",
    "\n",
    "    # 选择 q 列（若存在分队列的 q）\n",
    "    def _pick_q(row):\n",
    "        c = str(row.get(\"Cohort\",\"\")).lower()\n",
    "        if c.startswith(\"primary\"):     return row.get(\"q_FDR_primary\", np.nan)\n",
    "        if c.startswith(\"psm–sensitivity\") or c.startswith(\"sensitivity\"):\n",
    "            return row.get(\"q_FDR_sensitivity\", np.nan)\n",
    "        qa, qb = row.get(\"q_FDR_primary\", np.nan), row.get(\"q_FDR_sensitivity\", np.nan)\n",
    "        return qa if pd.notna(qa) else qb\n",
    "\n",
    "    df[\"q\"] = df.apply(_pick_q, axis=1)\n",
    "    df[\"95% CI\"] = df.apply(\n",
    "        lambda r: \"\" if any(pd.isna([r.get(\"% CI Low\"), r.get(\"% CI High\")]))\n",
    "        else f\"({_fmt_num(r['% CI Low'])}, {_fmt_num(r['% CI High'])})\", axis=1\n",
    "    )\n",
    "\n",
    "    # 输出列：注意第一列显示为“Analytic Cohort”\n",
    "    cols_out = [\"Cohort\",\"Stratification\",\"Stratum\",\"Outcome\",\"N\",\"% change\",\"95% CI\",\"p\",\"q\"]\n",
    "    df_out = pd.DataFrame({\n",
    "        \"Cohort\": df[\"Cohort\"],  # 内部 Cohort，下面表头改成 Analytic Cohort\n",
    "        \"Stratification\": df.get(\"Stratification\", \"\"),\n",
    "        \"Stratum\": df.get(\"Stratum\", \"\"),\n",
    "        \"Outcome\": df.get(\"Outcome\", \"\"),\n",
    "        \"N\": df.get(\"N\", \"\"),\n",
    "        \"% change\": df.get(\"% change\", np.nan).apply(lambda v: _fmt_num(v, 1)),\n",
    "        \"95% CI\": df[\"95% CI\"],\n",
    "        \"p\": df.get(\"p\", np.nan).apply(_fmt_p),\n",
    "        \"q\": df.get(\"q\", np.nan).apply(_fmt_q),\n",
    "    })[cols_out]\n",
    "\n",
    "    doc = Document()\n",
    "    try:\n",
    "        doc.styles[\"Normal\"].font.name = \"Times New Roman\"\n",
    "        doc.styles[\"Normal\"].font.size = Pt(10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    p = doc.add_paragraph(); p.add_run(title).bold = True\n",
    "\n",
    "    # 表头：第一列改为 Analytic Cohort\n",
    "    headers = [\"Analytic Cohort\",\"Stratification\",\"Stratum\",\"Outcome\",\n",
    "            \"N (stratum / total SA)\",\"%Δ\",\"95% CI (low, high)\",\"p\",\"q\"]\n",
    "    t = doc.add_table(rows=1, cols=len(headers))\n",
    "    for j, h in enumerate(headers):\n",
    "        t.rows[0].cells[j].text = h\n",
    "\n",
    "    for _, row in df_out.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        cells[0].text = \"\" if pd.isna(row[\"Cohort\"]) else str(row[\"Cohort\"])\n",
    "        cells[1].text = \"\" if pd.isna(row[\"Stratification\"]) else str(row[\"Stratification\"])\n",
    "        cells[2].text = \"\" if pd.isna(row[\"Stratum\"]) else str(row[\"Stratum\"])\n",
    "        cells[3].text = \"\" if pd.isna(row[\"Outcome\"]) else str(row[\"Outcome\"])\n",
    "        cells[4].text = \"\" if pd.isna(row[\"N\"]) else str(row[\"N\"])\n",
    "        cells[5].text = \"\" if pd.isna(row[\"% change\"]) else str(row[\"% change\"])\n",
    "        cells[6].text = \"\" if pd.isna(row[\"95% CI\"]) else str(row[\"95% CI\"])\n",
    "        cells[7].text = \"\" if pd.isna(row[\"p\"]) else str(row[\"p\"])\n",
    "        cells[8].text = \"\" if pd.isna(row[\"q\"]) else str(row[\"q\"])\n",
    "\n",
    "    _right_align_numeric(t, numeric_col_idx=[5,6,7,8])\n",
    "    _apply_three_line_table(t, header_row_idx=0)\n",
    "\n",
    "    if legend is None:\n",
    "        legend = (\"Legend: Adjusted between-group differences in log1p-transformed WMH expressed as percent change \"\n",
    "                  \"(Study vs Control) with 95% CIs. N is Study/Control per stratum. Benjamini–Hochberg FDR is applied \"\n",
    "                  \"within each stratum to {Periventricular, Deep}; Total WMH excluded. Models adjust for prespecified \"\n",
    "                  \"covariates; cluster-robust SEs by matched set (match_id) or HC3 when clustering is infeasible.\")\n",
    "    doc.add_paragraph().add_run(legend).italic = True\n",
    "\n",
    "    path = os.path.join(outdir, filename)\n",
    "    doc.save(path); print(f\"Word table saved: {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ---------------- One stratification block ----------------\n",
    "def run_one_block(df: pd.DataFrame,\n",
    "                  strat_col: str,\n",
    "                  levels: list,\n",
    "                  label_map: dict,\n",
    "                  title: str,\n",
    "                  cohort_label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run a full stratified analysis for a single stratification variable.\n",
    "\n",
    "    Key changes:\n",
    "      • N is reported as \"stratum SA / total SA participating in this stratification\".\n",
    "        - Numerator: number of SA (group == 'Study') whose value == current stratum level.\n",
    "        - Denominator: number of SA (group == 'Study') that have a non-missing value in this\n",
    "          stratification variable and whose value is one of `levels`.\n",
    "      • Matched sets are restricted so that the Study arm belongs to the current stratum level.\n",
    "      • Within each stratum, BH-FDR is applied to {Periventricular WMH, Deep WMH}; Total WMH excluded.\n",
    "    \"\"\"\n",
    "    # Skip if the stratification column is missing or entirely NA\n",
    "    if strat_col not in df.columns or df[strat_col].dropna().empty:\n",
    "        print(f\"[Skip] {strat_col} missing → {title}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Denominator for N: all SA (Study) that are eligible for THIS stratification (non-missing value in `levels`)\n",
    "    sa_total_in_strat = int(((df[GROUP_COL] == \"Study\") & df[strat_col].isin(levels)).sum())\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for lvl in levels:\n",
    "        lvl_label = label_map.get(lvl, str(lvl))\n",
    "\n",
    "        # Keep matched sets where the Study arm belongs to this level\n",
    "        study_ids_in_level = df[(df[strat_col] == lvl) & (df[GROUP_COL] == \"Study\")][MATCH_ID].dropna().unique()\n",
    "        if study_ids_in_level.size == 0:\n",
    "            continue\n",
    "        sub = df[df[MATCH_ID].isin(study_ids_in_level)].copy()\n",
    "\n",
    "        # Minimal pragmatic imputation for prespecified covariates\n",
    "        for v in ADJUST_ALL:\n",
    "            if v not in sub.columns:\n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(sub[v]):\n",
    "                sub[v] = pd.to_numeric(sub[v], errors=\"coerce\").fillna(sub[v].median())\n",
    "            else:\n",
    "                m = sub[v].mode(dropna=True)\n",
    "                sub[v] = sub[v].fillna(m.iloc[0] if not m.empty else sub[v])\n",
    "\n",
    "        # N formatting: \"stratum SA / total SA participating in this stratification\"\n",
    "        sa_in_level = int(((df[GROUP_COL] == \"Study\") & (df[strat_col] == lvl)).sum())\n",
    "        n_label = f\"{sa_in_level}/{sa_total_in_strat}\" if sa_total_in_strat > 0 else f\"{sa_in_level}/0\"\n",
    "\n",
    "        # Fit three WMH outcomes (log1p head-size normalized already prepared upstream)\n",
    "        for out_col, out_lab in {\n",
    "            \"Log_HSNorm_Total_WMH\": \"Total WMH\",\n",
    "            \"Log_HSNorm_PeriVentricular_WMH\": \"Periventricular WMH\",\n",
    "            \"Log_HSNorm_Deep_WMH\": \"Deep WMH\",\n",
    "        }.items():\n",
    "            if out_col not in sub.columns:\n",
    "                continue\n",
    "\n",
    "            formula = build_formula(out_col, sub)\n",
    "            model = fit_cluster(formula, sub)\n",
    "\n",
    "            coef_name = \"C(group)[T.Study]\"\n",
    "            if coef_name not in model.params.index:\n",
    "                continue\n",
    "\n",
    "            beta = float(model.params[coef_name])\n",
    "            ci_l, ci_u = [float(x) for x in model.conf_int().loc[coef_name]]\n",
    "            pval = float(model.pvalues[coef_name])\n",
    "\n",
    "            # Back-transform β to percent change on the original (non-log) scale\n",
    "            pct   = 100.0 * (np.exp(beta) - 1.0)\n",
    "            pct_l = 100.0 * (np.exp(ci_l) - 1.0)\n",
    "            pct_u = 100.0 * (np.exp(ci_u) - 1.0)\n",
    "\n",
    "            out_rows.append({\n",
    "                \"Stratification\": title,\n",
    "                \"Stratum\": lvl_label,\n",
    "                \"Outcome\": out_lab,\n",
    "                \"Beta\": beta,\n",
    "                \"CI Lower\": ci_l,\n",
    "                \"CI Upper\": ci_u,\n",
    "                \"p\": pval,\n",
    "                \"% change\": pct,\n",
    "                \"% CI Low\": pct_l,\n",
    "                \"% CI High\": pct_u,\n",
    "                \"q_FDR_primary\": np.nan,\n",
    "                \"q_FDR_sensitivity\": np.nan,\n",
    "                \"N\": n_label,\n",
    "            })\n",
    "\n",
    "        # Within-level BH-FDR for {Periventricular, Deep}; Total excluded\n",
    "        if out_rows:\n",
    "            idx_this_level = [i for i, r in enumerate(out_rows) if r[\"Stratum\"] == lvl_label]\n",
    "            idx_fdr = [i for i in idx_this_level if out_rows[i][\"Outcome\"] in (\"Periventricular WMH\", \"Deep WMH\")]\n",
    "            if idx_fdr:\n",
    "                pvals = np.array([out_rows[i][\"p\"] for i in idx_fdr], dtype=float)\n",
    "                qvals = multipletests(pvals, method=\"fdr_bh\")[1]\n",
    "                for i_row, qv in zip(idx_fdr, qvals):\n",
    "                    if cohort_label == \"Primary\":\n",
    "                        out_rows[i_row][\"q_FDR_primary\"] = float(qv)\n",
    "                    elif cohort_label == \"Sensitivity\":\n",
    "                        out_rows[i_row][\"q_FDR_sensitivity\"] = float(qv)\n",
    "\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "\n",
    "# ---------------- Build STRATA dynamically (includes SA_ascertain_group) ----------------\n",
    "def build_strata(df_example: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create the STRATA list with:\n",
    "      • Sex (adaptive mapping)\n",
    "      • Sleepy_SA_Stratum (1/0)\n",
    "      • Snoring_Stratum (1/0)\n",
    "      • SA_MRI_interval_stratum (LE10_years vs GT10_years)\n",
    "      • SA_ascertain_group (Self_Report_Only / Hospital_Secondary / Hospital_Primary)\n",
    "    \"\"\"\n",
    "    strata = []\n",
    "\n",
    "    # Sex (adaptive to \"Male/Female\" or 1/0)\n",
    "    if \"Sex\" in df_example.columns:\n",
    "        uniq = set(str(x) for x in pd.Series(df_example[\"Sex\"]).dropna().unique())\n",
    "        if {\"Male\",\"Female\"} & uniq:\n",
    "            levels = [\"Male\",\"Female\"]; lmap = {\"Male\":\"Male\",\"Female\":\"Female\"}\n",
    "        else:\n",
    "            levels = [1,0];            lmap = {1:\"Male\", 0:\"Female\"}\n",
    "        strata.append((\"Sex\", levels, lmap, \"Sex\", \"Sex\"))\n",
    "\n",
    "    if \"Sleepy_SA_Stratum\" in df_example.columns:\n",
    "        strata.append((\"Sleepy_SA_Stratum\", [1,0], {1:\"Sleepy\", 0:\"Not Sleepy\"}, \"Daytime Sleepiness\", \"Sleepiness\"))\n",
    "    if \"Snoring_Stratum\" in df_example.columns:\n",
    "        strata.append((\"Snoring_Stratum\", [1,0], {1:\"Snoring\", 0:\"No Snore\"}, \"Snoring Status\", \"Snoring\"))\n",
    "    if \"SA_MRI_interval_stratum\" in df_example.columns:\n",
    "        strata.append((\"SA_MRI_interval_stratum\",\n",
    "                       [\"LE10_years\",\"GT10_years\"],\n",
    "                       {\"LE10_years\":\"≤10 y\", \"GT10_years\":\">10 y\"},\n",
    "                       \"Diagnosis–MRI Interval\", \"Dx_MRI_Interval\"))\n",
    "\n",
    "    # NEW: SA diagnosis method stratification (renamed from “severity”)\n",
    "    if \"SA_ascertain_group\" in df_example.columns:\n",
    "        strata.append((\"SA_ascertain_group\",\n",
    "                       [\"Self_Report_Only\",\"Hospital_Secondary\",\"Hospital_Primary\"],\n",
    "                       {\"Self_Report_Only\":\"Self-Report\",\n",
    "                        \"Hospital_Secondary\":\"Hosp-Secondary\",\n",
    "                        \"Hospital_Primary\":\"Hosp-Primary\"},\n",
    "                       \"SA Diagnosis Method\", \"SA_Diagnosis\"))\n",
    "\n",
    "    return strata\n",
    "\n",
    "# ---------------- One cohort end-to-end ----------------\n",
    "def run_for_cohort(label: str, csv_path: str) -> pd.DataFrame:\n",
    "    print(f\"\\n===== {label} ({csv_path}) =====\")\n",
    "    subdir = os.path.join(OUTDIR, label); os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    df_raw = pd.read_csv(csv_path)\n",
    "    df = ensure_outcomes(clean_cols(df_raw))\n",
    "\n",
    "    STRATA = build_strata(df)\n",
    "\n",
    "    all_blocks = []\n",
    "    for strat_col, levels, lmap, panel_title, safe_key in STRATA:\n",
    "        block_df = run_one_block(df, strat_col, levels, lmap, panel_title, label)\n",
    "        if block_df is None or block_df.empty: continue\n",
    "\n",
    "        all_blocks.append(block_df)\n",
    "\n",
    "        block_csv = os.path.join(subdir, f\"Stratified_{safe_key}.csv\")\n",
    "        block_df.to_csv(block_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        out_png = os.path.join(subdir, f\"Forest_{safe_key}.png\")\n",
    "        forest_plot(block_df, panel_title, out_png)\n",
    "\n",
    "        # Console preview\n",
    "        show = block_df.copy()\n",
    "        show[\"Outcome\"] = pd.Categorical(show[\"Outcome\"], OUTCOME_ORDER, ordered=True)\n",
    "        show = show.sort_values([\"Outcome\",\"Stratum\"])\n",
    "        cols = [\"Stratification\",\"Stratum\",\"Outcome\",\"% change\",\"% CI Low\",\"% CI High\",\"p\",\n",
    "                \"q_FDR_primary\",\"q_FDR_sensitivity\",\"N\"]\n",
    "        print(\"\\n--\", panel_title, \"--\")\n",
    "        print(show[cols].round(4).to_string(index=False))\n",
    "\n",
    "    if all_blocks:\n",
    "        res_all = pd.concat(all_blocks, ignore_index=True)\n",
    "        res_all = res_all[[\n",
    "            \"Stratification\",\"Stratum\",\"Outcome\",\"N\",\n",
    "            \"Beta\",\"CI Lower\",\"CI Upper\",\"p\",\n",
    "            \"q_FDR_primary\",\"q_FDR_sensitivity\",\n",
    "            \"% change\",\"% CI Low\",\"% CI High\"\n",
    "        ]]\n",
    "        res_all.insert(0, \"Analytic Cohort\", label)\n",
    "        res_all[\"Analytic Cohort\"] = res_all[\"Analytic Cohort\"].replace({\n",
    "            \"Primary\": \"Primary PSM cohort\",\n",
    "            \"Sensitivity\": \"PSM–sensitivity cohort\"\n",
    "        })\n",
    "        res_csv = os.path.join(subdir, \"Stratified_AllResults.csv\")\n",
    "        res_all.to_csv(res_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    else:\n",
    "        res_all = pd.DataFrame()\n",
    "\n",
    "    # README\n",
    "    note = [\n",
    "        \"Head-size–normalized volumes with log1p transform.\",\n",
    "        \"Covariates: \" + \", \".join(ADJUST_ALL) + \" (no CMC).\",\n",
    "        \"Cluster-robust SEs by match_id where feasible; otherwise HC3.\",\n",
    "        \"Within each stratum: BH-FDR to {Periventricular, Deep}; Total excluded.\",\n",
    "        \"Forest figure outcome order: Total → Periventricular → Deep.\"\n",
    "    ]\n",
    "    with open(os.path.join(subdir, \"README_Strata.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(note))\n",
    "\n",
    "    return res_all\n",
    "\n",
    "# ---------------- Run both cohorts & export combined table ----------------\n",
    "all_res = []\n",
    "for lab, fp in COHORTS.items():\n",
    "    if not os.path.exists(fp):\n",
    "        print(f\"[Warning] File not found: {fp} → skip {lab}\")\n",
    "        continue\n",
    "    res = run_for_cohort(lab, fp)\n",
    "    if res is not None and not res.empty:\n",
    "        all_res.append(res)\n",
    "\n",
    "if all_res:\n",
    "    comb = pd.concat(all_res, ignore_index=True)\n",
    "\n",
    "    # 如果是旧列名，统一成“Analytic Cohort”\n",
    "    if \"Cohort\" in comb.columns and \"Analytic Cohort\" not in comb.columns:\n",
    "        comb = comb.rename(columns={\"Cohort\": \"Analytic Cohort\"})\n",
    "\n",
    "    # 统一显示文本为论文用语\n",
    "    comb[\"Analytic Cohort\"] = comb[\"Analytic Cohort\"].replace({\n",
    "        \"Primary\": \"Primary PSM cohort\",\n",
    "        \"Sensitivity\": \"PSM–sensitivity cohort\",\n",
    "        \"primary\": \"Primary PSM cohort\",\n",
    "        \"sensitivity\": \"PSM–sensitivity cohort\",\n",
    "    })\n",
    "\n",
    "    # 保存 CSV\n",
    "    out_csv = os.path.join(OUTDIR, \"Stratified_AllCohorts_Combined.csv\")\n",
    "    comb.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Combined CSV saved: {out_csv}\")\n",
    "\n",
    "    # Word：函数内部会兼容列名并把表头写为“Analytic Cohort”\n",
    "    export_stratified_word(comb, OUTDIR, filename=\"Stratified_AllCohorts_Supplement.docx\")\n",
    "\n",
    "\n",
    "print(f\"\\nAll stratified outputs saved under: {os.path.abspath(OUTDIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d2973959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Primary =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:941: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_cols(pd.read_csv(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Primary] Follow-up duration summary (years):\n",
      "  Overall: median 2.25 (IQR 2.17–2.47); N=636\n",
      "  Control: median 2.25 (IQR 2.17–2.47); N=589\n",
      "  Study: median 2.22 (IQR 2.12–2.36); N=47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:857: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".png\", **PNG_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:858: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".pdf\", **PDF_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:858: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".pdf\", **PDF_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:859: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".svg\", **SVG_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:860: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".tiff\", **TIFF_KW)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Sensitivity =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:941: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_cols(pd.read_csv(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sensitivity] Follow-up duration summary (years):\n",
      "  Overall: median 2.22 (IQR 2.15–2.41); N=654\n",
      "  Control: median 2.22 (IQR 2.15–2.41); N=606\n",
      "  Study: median 2.22 (IQR 2.13–2.39); N=48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:857: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".png\", **PNG_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:858: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".pdf\", **PDF_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:858: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".pdf\", **PDF_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:859: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".svg\", **SVG_KW)\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:860: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.savefig(base + \".tiff\", **TIFF_KW)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All outputs saved to: c:\\Users\\M328449\\OneDrive - Mayo Clinic\\Documents\\Python\\SA_WMH_Pub\\Annual_Change\n",
      "Combined CSV saved: Annual_Change\\Longitudinal_AllAnalyses_AllCohorts.csv\n",
      "Word eTable saved: Annual_Change\\Longitudinal_AllAnalyses_AllCohorts.docx\n",
      "CSV saved: Annual_Change\\IncreaseDecrease_AllCohorts.csv\n",
      "Word eTable saved: Annual_Change\\IncreaseDecrease_AllCohorts.docx\n",
      "Within-group Inc/Dec CSV saved: Annual_Change\\WithinGroup_IncDec_AllCohorts.csv\n",
      "Word eTable saved: Annual_Change\\WithinGroup_IncDec_AllCohorts.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:1729: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for (cohort_name, typ, grp), sub in df_all.groupby([\"Analytic cohort\", \"Metric\", \"Group\"]):\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:1802: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_cols(pd.read_csv(csv_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Primary] SA I2–Dx (years): N=853 | mean=10.50, SD=7.86 | median=9.57, IQR=4.33–15.18 | min=0.02, max=73.57 | positive=853, negative=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:1802: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_cols(pd.read_csv(csv_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sensitivity] SA I2–Dx (years): N=878 | mean=10.35, SD=7.83 | median=9.23, IQR=4.16–14.93 | min=0.02, max=73.57 | positive=878, negative=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:1932: DtypeWarning: Columns (18,24,25,33,40,43,44) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_cols(pd.read_csv(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SA Dx-duration (I2-Dx) table saved: Annual_Change\\SA_DxDuration_ByIncDec\\Primary\\Primary_SA_DxDuration_IncDec.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\426250346.py:1932: DtypeWarning: Columns (24,25,33,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = clean_cols(pd.read_csv(filepath))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SA Dx-duration (I2-Dx) table saved: Annual_Change\\SA_DxDuration_ByIncDec\\Sensitivity\\Sensitivity_SA_DxDuration_IncDec.csv\n"
     ]
    }
   ],
   "source": [
    "# Annual WMH Change (I3–I2) · Primary + Sensitivity Cohorts · Final (Revised & FDR Applied)\n",
    "# Key points:\n",
    "# - HSN log1p (la table): Benjamini–Hochberg FDR applied only to Periventricular & Deep; Total excluded (q=NaN).\n",
    "# - Inc/Dec (proportion tests): FDR applied only to Periventricular & Deep; Total excluded. In plots, Total has no q.\n",
    "# - 2-panel and single-region plots: plotting order Total → Periventricular → Deep; Total shows p only (as primary).\n",
    "\n",
    "import os, re, warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.proportion import proportions_ztest, confint_proportions_2indep\n",
    "\n",
    "# ---------------- Basic Configuration ----------------\n",
    "COHORTS = {\n",
    "    \"Primary\":     \"primary_cohort.csv\",\n",
    "    \"Sensitivity\": \"sensitivity_cohort.csv\",\n",
    "}\n",
    "OUTDIR = \"Annual_Change\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"figure.autolayout\": True,\n",
    "})\n",
    "\n",
    "# Publication style rcParams\n",
    "PUB_RC = {\n",
    "    \"font.family\": \"Arial\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"figure.autolayout\": True,\n",
    "}\n",
    "\n",
    "# File export kwargs\n",
    "SAVEFIG_DPI = 600\n",
    "PNG_KW  = {\"dpi\": SAVEFIG_DPI, \"bbox_inches\": \"tight\", \"facecolor\": \"white\"}\n",
    "PDF_KW  = {\"bbox_inches\": \"tight\"}\n",
    "SVG_KW  = {\"bbox_inches\": \"tight\"}\n",
    "TIFF_KW = {\n",
    "    \"dpi\": SAVEFIG_DPI,\n",
    "    \"bbox_inches\": \"tight\",\n",
    "    \"facecolor\": \"white\",\n",
    "    \"format\": \"tiff\",\n",
    "    \"pil_kwargs\": {\"compression\": \"tiff_lzw\"},\n",
    "}\n",
    "\n",
    "# Colors / markers consistent with main analysis\n",
    "COLOR_PRIMARY = \"#1f3b4d\"  # deep blue\n",
    "ELW, CAPSIZE, MS = 1.6, 4, 5.5\n",
    "\n",
    "# Abbreviated labels & plotting order\n",
    "DISPLAY_LABEL = {\"Total\": \"Total WMH\", \"Periventricular\": \"PWMH\", \"Deep\": \"DWMH\"}\n",
    "PLOT_ORDER = [\"Total\", \"Periventricular\", \"Deep\"]\n",
    "\n",
    "def safe_name(s):\n",
    "    return re.sub(r\"[^0-9A-Za-z._-]+\", \"_\", str(s))\n",
    "\n",
    "def fmt_p(p):\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "    return f\"{p:.1e}\" if p < 1e-3 else f\"{p:.3f}\"\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns=lambda c: pd.Series(c)\n",
    "                     .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "                     .iloc[0])\n",
    "\n",
    "# ---------------- Variable Definitions ----------------\n",
    "wmh_fields = {\n",
    "    \"Deep\": (\"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\",\n",
    "             \"Total_volume_of_deep_white_matter_hyperintensities_Instance_3\"),\n",
    "    \"Periventricular\": (\"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\",\n",
    "                        \"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_3\"),\n",
    "    \"Total\": (\"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\",\n",
    "              \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_3\"),\n",
    "}\n",
    "\n",
    "# ---------------- Annualized Change Definitions ----------------\n",
    "def annual_change_raw_ratio(d, f2, f3):\n",
    "    wm2 = d[\"Volume_of_white_matter_Instance_2\"].replace(0, np.nan)\n",
    "    wm3 = d[\"Volume_of_white_matter_Instance_3\"].replace(0, np.nan)\n",
    "    return ((d[f3] / wm3) - (d[f2] / wm2)) / d[\"Delta_years\"], \"WMH/WM\", \"raw\"\n",
    "\n",
    "def annual_change_raw_abs_hsn(d, f2, f3):\n",
    "    v2 = d[f2] * d[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"]\n",
    "    v3 = d[f3] * d[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_3\"]\n",
    "    return (v3 - v2) / d[\"Delta_years\"], \"HSN\", \"raw\"\n",
    "\n",
    "def annual_change_log1p_ratio(d, f2, f3):\n",
    "    wm2 = d[\"Volume_of_white_matter_Instance_2\"].replace(0, np.nan)\n",
    "    wm3 = d[\"Volume_of_white_matter_Instance_3\"].replace(0, np.nan)\n",
    "    return (np.log1p(d[f3] / wm3) - np.log1p(d[f2] / wm2)) / d[\"Delta_years\"], \"WMH/WM\", \"log\"\n",
    "\n",
    "def annual_change_log1p_abs_hsn(d, f2, f3):\n",
    "    v2 = d[f2] * d[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"]\n",
    "    v3 = d[f3] * d[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_3\"]\n",
    "    return (np.log1p(v3) - np.log1p(v2)) / d[\"Delta_years\"], \"HSN\", \"log\"\n",
    "\n",
    "# ---------------- Regression and Diagnostics ----------------\n",
    "def fit_cluster(formula, data, groups):\n",
    "    \"\"\"\n",
    "    OLS with cluster-robust SEs by `groups` if possible; otherwise HC3.\n",
    "    \"\"\"\n",
    "    if (groups in data.columns) and (data[groups].dropna().nunique() > 1):\n",
    "        return smf.ols(formula, data=data).fit(\n",
    "            cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": data[groups]}\n",
    "        )\n",
    "    return smf.ols(formula, data=data).fit(cov_type=\"HC3\")\n",
    "\n",
    "def diagnostic_compare(d, outcome_raw, outcome_log, rhs_terms, categorical_in):\n",
    "    \"\"\"\n",
    "    Compare raw vs log1p models via:\n",
    "      - Breusch–Pagan (heteroscedasticity)\n",
    "      - Jarque–Bera (normality)\n",
    "      - AIC\n",
    "    Recommend log1p if ≥2 criteria favor log1p.\n",
    "    \"\"\"\n",
    "    keep_cols = [outcome_raw, outcome_log, \"group\", \"match_id\"] + rhs_terms\n",
    "    tmp = d[keep_cols].dropna().copy()\n",
    "    if tmp.empty:\n",
    "        return None\n",
    "\n",
    "    rhs = \" + \".join(\n",
    "        [f\"C({v})\" if v in categorical_in else v for v in rhs_terms]\n",
    "    ) if rhs_terms else \"\"\n",
    "    f_raw = f\"{outcome_raw} ~ C(group)\" + (f\" + {rhs}\" if rhs else \"\")\n",
    "    f_log = f\"{outcome_log} ~ C(group)\" + (f\" + {rhs}\" if rhs else \"\")\n",
    "\n",
    "    m_raw = fit_cluster(f_raw, tmp, \"match_id\")\n",
    "    m_log = fit_cluster(f_log, tmp, \"match_id\")\n",
    "\n",
    "    bp_raw = het_breuschpagan(m_raw.resid, m_raw.model.exog)[1]\n",
    "    bp_log = het_breuschpagan(m_log.resid, m_log.model.exog)[1]\n",
    "    jb_raw = jarque_bera(m_raw.resid)[1]\n",
    "    jb_log = jarque_bera(m_log.resid)[1]\n",
    "    aic_raw, aic_log = m_raw.aic, m_log.aic\n",
    "\n",
    "    rec = (\n",
    "        (bp_log > bp_raw)\n",
    "        + (jb_log > jb_raw)\n",
    "        + (aic_log < aic_raw - 2)\n",
    "    ) >= 2\n",
    "\n",
    "    return {\n",
    "        \"n\": len(tmp),\n",
    "        \"BP_p_raw\": float(bp_raw),\n",
    "        \"BP_p_log\": float(bp_log),\n",
    "        \"JB_p_raw\": float(jb_raw),\n",
    "        \"JB_p_log\": float(jb_log),\n",
    "        \"AIC_raw\": float(aic_raw),\n",
    "        \"AIC_log\": float(aic_log),\n",
    "        \"Recommend_log1p\": bool(rec),\n",
    "    }\n",
    "\n",
    "def run_models(df, covars_in, categorical_in, change_kind=\"raw_abs\",\n",
    "               baseline_adjust=False, return_models=False):\n",
    "    \"\"\"\n",
    "    Run OLS models for all WMH regions.\n",
    "\n",
    "    change_kind: 'raw_abs', 'raw_ratio', 'log_abs', 'log_ratio'\n",
    "    baseline_adjust: if True, include appropriate baseline at Instance 2.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    fits = {}\n",
    "\n",
    "    for region, (f2, f3) in wmh_fields.items():\n",
    "        d = df.copy()\n",
    "\n",
    "        # outcome\n",
    "        if change_kind == \"raw_ratio\":\n",
    "            y, scale, scl = annual_change_raw_ratio(d, f2, f3)\n",
    "        elif change_kind == \"raw_abs\":\n",
    "            y, scale, scl = annual_change_raw_abs_hsn(d, f2, f3)\n",
    "        elif change_kind == \"log_ratio\":\n",
    "            y, scale, scl = annual_change_log1p_ratio(d, f2, f3)\n",
    "        else:  # \"log_abs\"\n",
    "            y, scale, scl = annual_change_log1p_abs_hsn(d, f2, f3)\n",
    "\n",
    "        d = d.assign(Annual_Change=y).dropna(\n",
    "            subset=[\"Annual_Change\", \"match_id\", \"group\"]\n",
    "        )\n",
    "\n",
    "        rhs_terms = list(covars_in)\n",
    "\n",
    "        if baseline_adjust:\n",
    "            d[\"Baseline_cov\"] = _baseline_column(d, change_kind, f2)\n",
    "            rhs_terms = [\"Baseline_cov\"] + rhs_terms\n",
    "\n",
    "        rhs = \" + \".join(\n",
    "            [f\"C({c})\" if c in categorical_in else c for c in rhs_terms]\n",
    "        ) if rhs_terms else \"\"\n",
    "        formula = \"Annual_Change ~ C(group)\" + (f\" + {rhs}\" if rhs else \"\")\n",
    "\n",
    "        fit = fit_cluster(formula, d, \"match_id\")\n",
    "        if return_models:\n",
    "            fits[region] = fit\n",
    "\n",
    "        key = \"C(group)[T.Study]\"\n",
    "        beta = ci_l = ci_u = pval = np.nan\n",
    "        if key in fit.params.index:\n",
    "            beta = float(fit.params[key])\n",
    "            ci_l, ci_u = [float(x) for x in fit.conf_int().loc[key]]\n",
    "            pval = float(fit.pvalues[key])\n",
    "\n",
    "        pct = pct_l = pct_u = np.nan\n",
    "        if scl == \"log\" and np.isfinite(beta):\n",
    "            pct = 100 * (np.exp(beta) - 1)\n",
    "            pct_l = 100 * (np.exp(ci_l) - 1)\n",
    "            pct_u = 100 * (np.exp(ci_u) - 1)\n",
    "\n",
    "        outcome = f\"{region} · {scale}\" + (\"\" if scl == \"raw\" else \" (%/yr)\")\n",
    "        rows.append({\n",
    "            \"Outcome\": outcome,\n",
    "            \"β\": beta,\n",
    "            \"95% CI Low\": ci_l,\n",
    "            \"95% CI High\": ci_u,\n",
    "            \"p\": pval,\n",
    "            \"%/year\": pct,\n",
    "            \"%/year CI Low\": pct_l,\n",
    "            \"%/year CI High\": pct_u,\n",
    "            \"N Study\": int(d.loc[d.group == 'Study', 'Participant_ID'].nunique())\n",
    "                       if \"Participant_ID\" in d.columns else int((d.group == 'Study').sum()),\n",
    "            \"N Control\": int(d.loc[d.group == 'Control', 'Participant_ID'].nunique())\n",
    "                         if \"Participant_ID\" in d.columns else int((d.group == 'Control').sum()),\n",
    "        })\n",
    "\n",
    "    res = pd.DataFrame(rows)\n",
    "    if return_models:\n",
    "        return res, fits\n",
    "    return res\n",
    "\n",
    "def inc_dec_tests(df, use_ratio=True):\n",
    "    \"\"\"\n",
    "    Proportion of participants with increased vs decreased WMH (Study vs Control).\n",
    "    Returns counts, percentages, difference, CIs, and p-values.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for region, (f2, f3) in wmh_fields.items():\n",
    "        d = df.copy()\n",
    "        if use_ratio:\n",
    "            y, _, _ = annual_change_raw_ratio(d, f2, f3)\n",
    "        else:\n",
    "            y, _, _ = annual_change_raw_abs_hsn(d, f2, f3)\n",
    "        d = d.assign(Annual_Change=y).dropna(subset=[\"Annual_Change\", \"group\"])\n",
    "        d[\"Increased\"] = (d[\"Annual_Change\"] > 0).astype(int)\n",
    "\n",
    "        inc_s = int(d.loc[d.group == \"Study\", \"Increased\"].sum())\n",
    "        inc_c = int(d.loc[d.group == \"Control\", \"Increased\"].sum())\n",
    "        n_s = int((d.group == \"Study\").sum())\n",
    "        n_c = int((d.group == \"Control\").sum())\n",
    "        dec_s, dec_c = n_s - inc_s, n_c - inc_c\n",
    "\n",
    "        table = np.array([[inc_s, dec_s], [inc_c, dec_c]], dtype=int)\n",
    "\n",
    "        if (table < 5).any():\n",
    "            test = \"Fisher exact\"\n",
    "            _, p_primary = stats.fisher_exact(table)\n",
    "        else:\n",
    "            test = \"Chi-square (Yates)\"\n",
    "            _, p_primary, _, _ = stats.chi2_contingency(table, correction=True)\n",
    "\n",
    "        prop_s = inc_s / n_s if n_s > 0 else np.nan\n",
    "        prop_c = inc_c / n_c if n_c > 0 else np.nan\n",
    "        ci_l, ci_h = confint_proportions_2indep(\n",
    "            inc_s, n_s, inc_c, n_c, method=\"newcomb\"\n",
    "        )\n",
    "        _, p_prop = proportions_ztest([inc_s, inc_c], [n_s, n_c])\n",
    "\n",
    "        rows.append({\n",
    "            \"Outcome\": region,\n",
    "            \"Study Increase\": inc_s,\n",
    "            \"Study Decrease\": dec_s,\n",
    "            \"Study N\": n_s,\n",
    "            \"Control Increase\": inc_c,\n",
    "            \"Control Decrease\": dec_c,\n",
    "            \"Control N\": n_c,\n",
    "            \"Increase % (Study)\": 100 * prop_s if n_s > 0 else np.nan,\n",
    "            \"Increase % (Control)\": 100 * prop_c if n_c > 0 else np.nan,\n",
    "            \"Prop Diff (S-C)\": (prop_s - prop_c) if (n_s > 0 and n_c > 0) else np.nan,\n",
    "            \"95% CI Low\": ci_l,\n",
    "            \"95% CI High\": ci_h,\n",
    "            \"Test\": test,\n",
    "            \"p_primary\": p_primary,\n",
    "            \"p_prop_z\": float(p_prop),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------------- Formatting helpers ----------------\n",
    "def _format_pq(p, q=None):\n",
    "    def fnum(x, prefix):\n",
    "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "            return None\n",
    "        return f\"{prefix}<0.001\" if x < 0.001 else f\"{prefix}={x:.3f}\"\n",
    "    p_txt = fnum(p, \"p\")\n",
    "    q_txt = fnum(q, \"q\") if q is not None else None\n",
    "    if p_txt and q_txt:\n",
    "        return f\"{p_txt}, {q_txt}\"\n",
    "    if p_txt:\n",
    "        return p_txt\n",
    "    if q_txt:\n",
    "        return q_txt\n",
    "    return None\n",
    "\n",
    "def _stars_by_p(p):\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p < 0.001:\n",
    "        return \"***\"\n",
    "    if p < 0.01:\n",
    "        return \"**\"\n",
    "    if p < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "def _add_pq_bracket(ax, x_left, x_right, y_base,\n",
    "                    label_txt, barw=0.52, fontsize=8, dy=0.8):\n",
    "    if not label_txt:\n",
    "        return\n",
    "    dx = barw * 0.60\n",
    "    y_top = y_base + 1.0\n",
    "    ax.plot(\n",
    "        [x_left - dx, x_left - dx, x_right + dx, x_right + dx],\n",
    "        [y_base, y_top, y_top, y_base],\n",
    "        color=\"black\", lw=0.9, zorder=5,\n",
    "    )\n",
    "    ax.text(\n",
    "        (x_left + x_right) / 2.0, y_top + dy,\n",
    "        label_txt,\n",
    "        ha=\"center\", va=\"bottom\",\n",
    "        fontsize=fontsize, fontstyle=\"italic\",\n",
    "        color=\"black\", zorder=6,\n",
    "    )\n",
    "\n",
    "# ---------------- Baseline covariate helper ----------------\n",
    "def _baseline_column(df, change_kind, f2):\n",
    "    \"\"\"\n",
    "    Baseline covariate (Instance 2) matching change_kind.\n",
    "    \"\"\"\n",
    "    if change_kind in (\"raw_abs\", \"log_abs\"):\n",
    "        base = df[f2] * df[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"]\n",
    "        if change_kind == \"log_abs\":\n",
    "            base = np.log1p(base)\n",
    "        return pd.to_numeric(base, errors=\"coerce\")\n",
    "    else:\n",
    "        wm2 = df[\"Volume_of_white_matter_Instance_2\"].replace(0, np.nan)\n",
    "        base = pd.to_numeric(df[f2], errors=\"coerce\") / wm2\n",
    "        if change_kind == \"log_ratio\":\n",
    "            base = np.log1p(base)\n",
    "        return pd.to_numeric(base, errors=\"coerce\")\n",
    "\n",
    "# ---------------- Diagnostics plots ----------------\n",
    "def plot_model_diagnostics(fit, out_base):\n",
    "    \"\"\"\n",
    "    Save residual diagnostics:\n",
    "      - Residuals vs fitted\n",
    "      - Normal Q–Q\n",
    "      - Histogram of standardized residuals\n",
    "    \"\"\"\n",
    "    if fit is None:\n",
    "        return\n",
    "\n",
    "    # Residuals vs Fitted\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.0))\n",
    "    ax.scatter(fit.fittedvalues, fit.resid, s=14, alpha=0.75, edgecolors=\"none\")\n",
    "    ax.axhline(0, color=\"grey\", ls=\"--\", lw=1)\n",
    "    ax.set_xlabel(\"Fitted values\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    ax.set_title(\"Residuals vs Fitted\")\n",
    "    ax.grid(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_base + \"_resid_vs_fitted.png\", **PNG_KW)\n",
    "    plt.savefig(out_base + \"_resid_vs_fitted.pdf\", **PDF_KW)\n",
    "    plt.savefig(out_base + \"_resid_vs_fitted.svg\", **SVG_KW)\n",
    "    plt.savefig(out_base + \"_resid_vs_fitted.tiff\", **TIFF_KW)\n",
    "    plt.close()\n",
    "\n",
    "    # Q–Q plot\n",
    "    fig = plt.figure(figsize=(6.0, 6.0))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sm.ProbPlot(fit.resid).qqplot(line=\"s\", ax=ax)\n",
    "    ax.set_title(\"Normal Q–Q\")\n",
    "    ax.grid(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_base + \"_qq.png\", **PNG_KW)\n",
    "    plt.savefig(out_base + \"_qq.pdf\", **PDF_KW)\n",
    "    plt.savefig(out_base + \"_qq.svg\", **SVG_KW)\n",
    "    plt.savefig(out_base + \"_qq.tiff\", **TIFF_KW)\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram of standardized residuals\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.0))\n",
    "    try:\n",
    "        infl = fit.get_influence()\n",
    "        zresid = infl.resid_studentized_internal\n",
    "    except Exception:\n",
    "        zresid = (fit.resid - np.nanmean(fit.resid)) / np.nanstd(fit.resid)\n",
    "    ax.hist(zresid, bins=30, edgecolor=\"white\")\n",
    "    ax.set_xlabel(\"Standardized residuals\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Histogram of standardized residuals\")\n",
    "    ax.grid(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_base + \"_hist_stdresid.png\", **PNG_KW)\n",
    "    plt.savefig(out_base + \"_hist_stdresid.pdf\", **PDF_KW)\n",
    "    plt.savefig(out_base + \"_hist_stdresid.svg\", **SVG_KW)\n",
    "    plt.savefig(out_base + \"_hist_stdresid.tiff\", **TIFF_KW)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------- Methods record ----------------\n",
    "def write_methods_record(outdir):\n",
    "    txt = []\n",
    "    txt.append(\"Longitudinal WMH change (Instance 3 - Instance 2)\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"Outcomes:\")\n",
    "    txt.append(\"- HSN absolute change per year (raw and log1p back-transform to %/year).\")\n",
    "    txt.append(\"- WMH/WM ratio change per year (raw and log1p back-transform to %/year).\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"Primary model: OLS with Study vs Control indicator; covariates:\")\n",
    "    txt.append(\"- Age at Instance 2, Sex, BMI at Instance 0, Genetic ethnic grouping, Smoking (ever),\")\n",
    "    txt.append(\"  Alcohol intake frequency (ordinal), Townsend deprivation index at recruitment.\")\n",
    "    txt.append(\"SE: Cluster-robust by matched set (match_id) if available; otherwise HC3.\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"Sensitivity model: Adds baseline value at Instance 2 matching the outcome scale.\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"Multiple testing:\")\n",
    "    txt.append(\"- For HSN log1p models (Periventricular, Deep), BH-FDR applied; Total excluded (q=NaN).\")\n",
    "    txt.append(\"- Inc/Dec tests: only Periventricular and Deep included in FDR; Total excluded.\")\n",
    "    txt.append(\"\")\n",
    "    txt.append(\"Figures:\")\n",
    "    txt.append(\"- Forest plots match main analysis style; PNG/PDF/SVG and 600 dpi TIFF with LZW compression.\")\n",
    "    path = os.path.join(outdir, \"METHODS_Longitudinal.txt\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(txt))\n",
    "    return path\n",
    "\n",
    "# ---------------- Forest Plotting ----------------\n",
    "def forest(df_plot, ttl, outpath, logscale=False):\n",
    "    \"\"\"\n",
    "    Publication-styled forest plot:\n",
    "      - Deep-blue markers & CIs\n",
    "      - Outcome order: Total → Periventricular → Deep\n",
    "      - Y-axis labels: Total WMH / PWMH / DWMH\n",
    "      - logscale=True uses %/year; else raw β.\n",
    "    \"\"\"\n",
    "    if df_plot is None or df_plot.empty:\n",
    "        return\n",
    "\n",
    "    plt.rcParams.update(PUB_RC)\n",
    "\n",
    "    dfp = df_plot.copy()\n",
    "    region = dfp[\"Outcome\"].astype(str).str.split(\"·\", n=1, expand=True)[0].str.strip()\n",
    "    dfp[\"Region\"] = region\n",
    "\n",
    "    dfp = dfp[dfp[\"Region\"].isin(PLOT_ORDER)].copy()\n",
    "    if dfp.empty:\n",
    "        return\n",
    "\n",
    "    dfp[\"Region\"] = pd.Categorical(dfp[\"Region\"], categories=PLOT_ORDER, ordered=True)\n",
    "    dfp = dfp.sort_values(\"Region\").reset_index(drop=True)\n",
    "\n",
    "    if logscale:\n",
    "        x = pd.to_numeric(dfp[\"%/year\"], errors=\"coerce\").to_numpy()\n",
    "        xl = pd.to_numeric(dfp[\"%/year CI Low\"], errors=\"coerce\").to_numpy()\n",
    "        xu = pd.to_numeric(dfp[\"%/year CI High\"], errors=\"coerce\").to_numpy()\n",
    "        xlabel = \"Annualized % change (SA - Control)\"\n",
    "    else:\n",
    "        x = pd.to_numeric(dfp[\"β\"], errors=\"coerce\").to_numpy()\n",
    "        xl = pd.to_numeric(dfp[\"95% CI Low\"], errors=\"coerce\").to_numpy()\n",
    "        xu = pd.to_numeric(dfp[\"95% CI High\"], errors=\"coerce\").to_numpy()\n",
    "        xlabel = \"Annualized change (β ± 95% CI)\"\n",
    "\n",
    "    n = len(dfp)\n",
    "    y = np.arange(n)[::-1]\n",
    "    fig_h = max(3.6, 0.85 * n)\n",
    "\n",
    "    fig = plt.figure(figsize=(6.8, fig_h))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.errorbar(\n",
    "        x, y,\n",
    "        xerr=[x - xl, xu - x],\n",
    "        fmt=\"o\",\n",
    "        ms=MS,\n",
    "        mfc=COLOR_PRIMARY,\n",
    "        mec=COLOR_PRIMARY,\n",
    "        ecolor=COLOR_PRIMARY,\n",
    "        elinewidth=ELW,\n",
    "        capsize=CAPSIZE,\n",
    "        linestyle=\"none\",\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    ax.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    ax.grid(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ylab = [DISPLAY_LABEL.get(r, r) for r in dfp[\"Region\"].astype(str)]\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(ylab)\n",
    "\n",
    "    xmin = np.nanmin(xl) if np.isfinite(np.nanmin(xl)) else -1.0\n",
    "    xmax = np.nanmax(xu) if np.isfinite(np.nanmax(xu)) else 1.0\n",
    "    span = xmax - xmin if (xmax - xmin) > 0 else 1.0\n",
    "    pad_left, pad_right = 0.08 * span, 0.30 * span\n",
    "    ax.set_xlim(xmin - pad_left, xmax + pad_right)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_title(ttl)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
    "    base, _ = os.path.splitext(outpath)\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".svg\", **SVG_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------------- Single-region Inc/Dec plots ----------------\n",
    "def plot_inc_dec_paper(tests_table, ttl, outdir):\n",
    "    \"\"\"\n",
    "    Single-region stacked bar plots for Increase vs Decrease.\n",
    "    - q-values only shown for Periventricular & Deep (if available).\n",
    "    - Total WMH: p only (as primary); q suppressed.\n",
    "    \"\"\"\n",
    "    if tests_table.empty:\n",
    "        return\n",
    "\n",
    "    plt.rcParams.update(PUB_RC)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    OI_INCREASE = \"#D55E00\"\n",
    "    OI_DECREASE = \"#009E73\"\n",
    "\n",
    "    for region in tests_table[\"Outcome\"].unique():\n",
    "        t = tests_table[tests_table[\"Outcome\"] == region].iloc[0]\n",
    "        inc_c, dec_c, n_c = float(t[\"Control Increase\"]), float(t[\"Control Decrease\"]), float(t[\"Control N\"])\n",
    "        inc_s, dec_s, n_s = float(t[\"Study Increase\"]),   float(t[\"Study Decrease\"]),   float(t[\"Study N\"])\n",
    "\n",
    "        inc_pct_c = 100 * inc_c / n_c if n_c > 0 else np.nan\n",
    "        inc_pct_s = 100 * inc_s / n_s if n_s > 0 else np.nan\n",
    "        dec_pct_c = 100 - inc_pct_c if not np.isnan(inc_pct_c) else np.nan\n",
    "        dec_pct_s = 100 - inc_pct_s if not np.isnan(inc_pct_s) else np.nan\n",
    "\n",
    "        p_main = pd.to_numeric(t[\"p_primary\"], errors=\"coerce\")\n",
    "        p_main = float(p_main) if pd.notna(p_main) else np.nan\n",
    "\n",
    "        # q 仅 Periventricular/Deep 显示\n",
    "        q_main = None\n",
    "        if region in (\"Periventricular\", \"Deep\"):\n",
    "            if \"q_FDR_incdec_combined\" in t.index:\n",
    "                q_try = pd.to_numeric(t[\"q_FDR_incdec_combined\"], errors=\"coerce\")\n",
    "                q_main = float(q_try) if pd.notna(q_try) else None\n",
    "\n",
    "        pq_txt_base = _format_pq(p_main, q_main)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6.2, 4.3))\n",
    "        x = np.array([0, 1], dtype=float)\n",
    "        barw = 0.52\n",
    "\n",
    "        ax.bar(x, [inc_pct_c, inc_pct_s], width=barw,\n",
    "               color=OI_INCREASE, edgecolor=\"black\", linewidth=0.8,\n",
    "               label=\"Increase\", zorder=3)\n",
    "        ax.bar(x, [dec_pct_c, dec_pct_s], width=barw,\n",
    "               bottom=[inc_pct_c, inc_pct_s],\n",
    "               color=OI_DECREASE, edgecolor=\"black\", linewidth=0.8,\n",
    "               label=\"Decrease\", zorder=3)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([\"Control\", \"Study\"])\n",
    "        ax.set_ylabel(\"Participants (%)\")\n",
    "        ax.set_ylim(0, 104)\n",
    "        ax.set_yticks([0, 25, 50, 75, 100])\n",
    "        ax.set_title(f\"{region} {ttl} — Increase vs Decrease by Group\", pad=16)\n",
    "\n",
    "        ax.grid(axis=\"y\", linestyle=\":\", alpha=0.45)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.margins(x=0.18)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "        # % labels\n",
    "        for i, ip in enumerate([inc_pct_c, inc_pct_s]):\n",
    "            if not np.isnan(ip):\n",
    "                ax.text(x[i], ip * 0.50, f\"{ip:.1f}%\",\n",
    "                        ha=\"center\", va=\"center\", fontsize=9, color=\"white\")\n",
    "        for i, (ip, dp) in enumerate([(inc_pct_c, dec_pct_c),\n",
    "                                      (inc_pct_s, dec_pct_s)]):\n",
    "            if not np.isnan(dp):\n",
    "                ax.text(x[i], ip + dp * 0.50, f\"{dp:.1f}%\",\n",
    "                        ha=\"center\", va=\"center\", fontsize=9, color=\"white\")\n",
    "\n",
    "        # p / q label (Peri/Deep may show q)\n",
    "        pq_txt_base = _format_pq(p_main, q_main)\n",
    "        stars = _stars_by_p(p_main)\n",
    "        label_txt = f\"{stars} {pq_txt_base}\" if stars and pq_txt_base else (stars or pq_txt_base)\n",
    "\n",
    "        if label_txt:\n",
    "            _add_pq_bracket(\n",
    "                ax,\n",
    "                x_left=x[0],\n",
    "                x_right=x[1],\n",
    "                y_base=101.0,\n",
    "                label_txt=label_txt,\n",
    "                barw=barw,\n",
    "                fontsize=9,\n",
    "                dy=0.4,\n",
    "            )\n",
    "\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1.0), frameon=False)\n",
    "        plt.subplots_adjust(left=0.10, right=0.76, bottom=0.14, top=0.88)\n",
    "\n",
    "        fname = f\"{safe_name(region)}_IncDec_{safe_name(ttl)}_Paper\"\n",
    "        base = os.path.join(outdir, fname)\n",
    "        plt.savefig(base + \".png\", **PNG_KW)\n",
    "        plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "        plt.savefig(base + \".svg\", **SVG_KW)\n",
    "        plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "        plt.close()\n",
    "\n",
    "# ---------------- 2-panel Figure X (HSN) ----------------\n",
    "def plot_figureX_hsn(la_table: pd.DataFrame,\n",
    "                     incdec_hsn: pd.DataFrame,\n",
    "                     outdir: str,\n",
    "                     filename: str = \"FigureX_Longitudinal_WMH.png\"):\n",
    "    \"\"\"\n",
    "    Two-panel figure:\n",
    "      Panel A: HSN log1p annualized % change (SA - Control), Total/PWMH/DWMH.\n",
    "               Significance by FDR for PWMH/DWMH; p-only for Total.\n",
    "      Panel B: Inc/Dec stacked bars; q only for PWMH/DWMH.\n",
    "    \"\"\"\n",
    "    if la_table is None or la_table.empty or incdec_hsn is None or incdec_hsn.empty:\n",
    "        return\n",
    "\n",
    "    plt.rcParams.update(PUB_RC)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # --- Panel A prep ---\n",
    "    region_order_raw = [\"Total\", \"Periventricular\", \"Deep\"]\n",
    "    wanted = [f\"{lab} · HSN (%/yr)\" for lab in region_order_raw]\n",
    "\n",
    "    la = la_table.copy()\n",
    "    la = la[la[\"Outcome\"].isin(wanted)].copy()\n",
    "    if la.empty:\n",
    "        return\n",
    "    la[\"Outcome\"] = pd.Categorical(la[\"Outcome\"], categories=wanted, ordered=True)\n",
    "    la = la.sort_values(\"Outcome\").reset_index(drop=True)\n",
    "    la[\"Region\"] = la[\"Outcome\"].str.split(\"·\", n=1, expand=True)[0].str.strip()\n",
    "\n",
    "    region_map = {\"Total\": \"Total WMH\", \"Periventricular\": \"PWMH\", \"Deep\": \"DWMH\"}\n",
    "    y_labels_left = [region_map.get(r, r) for r in la[\"Region\"]]\n",
    "\n",
    "    x = pd.to_numeric(la[\"%/year\"], errors=\"coerce\").to_numpy()\n",
    "    xl = pd.to_numeric(la[\"%/year CI Low\"], errors=\"coerce\").to_numpy()\n",
    "    xu = pd.to_numeric(la[\"%/year CI High\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "    def _pick_q(row):\n",
    "        if \"q_FDR_primary\" in row and pd.notna(row[\"q_FDR_primary\"]):\n",
    "            return row[\"q_FDR_primary\"]\n",
    "        if \"q_FDR_sensitivity\" in row and pd.notna(row[\"q_FDR_sensitivity\"]):\n",
    "            return row[\"q_FDR_sensitivity\"]\n",
    "        return np.nan\n",
    "\n",
    "    qvals = pd.to_numeric(la.apply(_pick_q, axis=1), errors=\"coerce\").to_numpy()\n",
    "    pvals = pd.to_numeric(la.get(\"p\", np.nan), errors=\"coerce\").to_numpy()\n",
    "\n",
    "\n",
    "    use_q = np.isfinite(qvals)\n",
    "    sig_mask = np.where(use_q, (qvals <= 0.05), (pvals <= 0.05))\n",
    "\n",
    "\n",
    "    COLOR_NONSIG = \"#7f7f7f\"\n",
    "    point_colors = np.where(sig_mask, COLOR_PRIMARY, COLOR_NONSIG)\n",
    "\n",
    "    # --- Panel B prep (Inc/Dec HSN) ---\n",
    "    t = incdec_hsn.copy()\n",
    "    t = t[t[\"Outcome\"].isin(region_order_raw)].copy()\n",
    "\n",
    "    rows = []\n",
    "    for rg in region_order_raw:\n",
    "        r = t[t[\"Outcome\"] == rg]\n",
    "        if r.empty:\n",
    "            continue\n",
    "        r = r.iloc[0]\n",
    "        n_c = float(r[\"Control N\"])\n",
    "        n_s = float(r[\"Study N\"])\n",
    "        inc_c = 100.0 * float(r[\"Control Increase\"]) / n_c if n_c > 0 else np.nan\n",
    "        inc_s = 100.0 * float(r[\"Study Increase\"]) / n_s if n_s > 0 else np.nan\n",
    "        dec_c = 100.0 - inc_c if not np.isnan(inc_c) else np.nan\n",
    "        dec_s = 100.0 - inc_s if not np.isnan(inc_s) else np.nan\n",
    "\n",
    "        q_show = None\n",
    "        if (rg in (\"Periventricular\", \"Deep\") and\n",
    "            (\"q_FDR_incdec_combined\" in r.index) and\n",
    "            pd.notna(r[\"q_FDR_incdec_combined\"])):\n",
    "            q_show = float(r[\"q_FDR_incdec_combined\"])\n",
    "\n",
    "        rows.append({\n",
    "            \"Region\": region_map.get(rg, rg),\n",
    "            \"inc_c\": inc_c, \"dec_c\": dec_c,\n",
    "            \"inc_s\": inc_s, \"dec_s\": dec_s,\n",
    "            \"p\": float(r[\"p_primary\"]) if pd.notna(r[\"p_primary\"]) else np.nan,\n",
    "            \"q\": q_show,\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return\n",
    "    dfB = pd.DataFrame(rows)\n",
    "\n",
    "    # --- Figure layout ---\n",
    "    fig = plt.figure(figsize=(11.6, 4.0))\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[1.5, 1.45], wspace=0.30)\n",
    "\n",
    "    # Panel A\n",
    "    axA = fig.add_subplot(gs[0, 0])\n",
    "    y = np.linspace(0, len(la) - 1, len(la)) * 0.7\n",
    "    y = y[::-1]\n",
    "    ypad = 0.35\n",
    "    axA.set_ylim(-ypad, y.max() + ypad)\n",
    "\n",
    "    span_ref = np.nanmax(xu) - np.nanmin(xl) if np.isfinite(np.nanmax(xu) - np.nanmin(xl)) else 1.0\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if any(np.isnan([x[i], xl[i], xu[i]])):\n",
    "            continue\n",
    "        axA.errorbar(\n",
    "            x[i], y[i],\n",
    "            xerr=[[x[i] - xl[i]], [xu[i] - x[i]]],\n",
    "            fmt=\"o\", ms=MS,\n",
    "            mfc=point_colors[i], mec=point_colors[i],\n",
    "            ecolor=point_colors[i], elinewidth=ELW,\n",
    "            capsize=CAPSIZE, linestyle=\"none\", zorder=3,\n",
    "        )\n",
    "\n",
    "    axA.axvline(0, color=\"grey\", linestyle=\"--\", linewidth=1)\n",
    "    axA.grid(False)\n",
    "    axA.spines[\"top\"].set_visible(False)\n",
    "    axA.spines[\"right\"].set_visible(False)\n",
    "    axA.set_yticks(y)\n",
    "    axA.set_yticklabels(y_labels_left)\n",
    "\n",
    "    xmin = np.nanmin(xl) if np.isfinite(np.nanmin(xl)) else -1.0\n",
    "    xmax = np.nanmax(xu) if np.isfinite(np.nanmax(xu)) else 1.0\n",
    "    span = xmax - xmin if (xmax - xmin) > 0 else 1.0\n",
    "    axA.set_xlim(xmin - 0.08 * span, xmax + 0.15 * span)\n",
    "\n",
    "    axA.set_xlabel(\"Annualized % change (SA - Control)\")\n",
    "\n",
    "    # Panel B\n",
    "    axB = fig.add_subplot(gs[0, 1])\n",
    "    COL_INC = COLOR_PRIMARY\n",
    "    COL_DEC = \"#9c9c9c\"\n",
    "    edge_lw = 0.8\n",
    "\n",
    "    centers = np.arange(len(dfB)) * 2.0\n",
    "    barw = 0.48\n",
    "\n",
    "    for i, r in dfB.iterrows():\n",
    "        xc, xs = centers[i] - 0.36, centers[i] + 0.36\n",
    "        axB.bar(xc, r[\"inc_c\"], width=barw, color=COL_INC,\n",
    "                edgecolor=COL_INC, linewidth=edge_lw, zorder=3)\n",
    "        axB.bar(xc, r[\"dec_c\"], width=barw, bottom=r[\"inc_c\"],\n",
    "                color=COL_DEC, edgecolor=COL_DEC, linewidth=edge_lw, zorder=3)\n",
    "        axB.bar(xs, r[\"inc_s\"], width=barw, color=COL_INC,\n",
    "                edgecolor=COL_INC, linewidth=edge_lw, zorder=3)\n",
    "        axB.bar(xs, r[\"dec_s\"], width=barw, bottom=r[\"inc_s\"],\n",
    "                color=COL_DEC, edgecolor=COL_DEC, linewidth=edge_lw, zorder=3)\n",
    "\n",
    "        stars = _stars_by_p(r[\"p\"])\n",
    "        label_txt = stars\n",
    "        if label_txt:\n",
    "            from numpy import nansum\n",
    "            y_base = max(nansum([r[\"inc_c\"], r[\"dec_c\"]]),\n",
    "                         nansum([r[\"inc_s\"], r[\"dec_s\"]])) + 2.0\n",
    "            _add_pq_bracket(\n",
    "                axB,\n",
    "                x_left=xc,\n",
    "                x_right=xs,\n",
    "                y_base=y_base,\n",
    "                label_txt=label_txt,\n",
    "                barw=barw,\n",
    "                fontsize=11,\n",
    "                dy=0.6,\n",
    "            )\n",
    "\n",
    "    tick_positions = []\n",
    "    tick_labels = []\n",
    "    for i, region_name in enumerate(dfB[\"Region\"]):\n",
    "        xc, xs = centers[i] - 0.36, centers[i] + 0.36\n",
    "        tick_positions.extend([xc, xs])\n",
    "        tick_labels.extend([\"CO\", \"SA\"])\n",
    "        axB.text(centers[i], -10.5, region_name,\n",
    "                 ha=\"center\", va=\"top\", fontsize=11)\n",
    "\n",
    "    axB.set_xticks(tick_positions)\n",
    "    axB.set_xticklabels(tick_labels, fontsize=9)\n",
    "    plt.subplots_adjust(bottom=0.18)\n",
    "\n",
    "    axB.set_ylim(0, 115)\n",
    "    axB.set_ylabel(\"Participants (%)\")\n",
    "    axB.grid(axis=\"y\", linestyle=\":\", alpha=0.45)\n",
    "    axB.set_axisbelow(True)\n",
    "    axB.spines[\"top\"].set_visible(False)\n",
    "    axB.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    legend_items = [\n",
    "        mpl.patches.Patch(facecolor=COL_INC, edgecolor=COL_INC, label=\"Increase\"),\n",
    "        mpl.patches.Patch(facecolor=COL_DEC, edgecolor=COL_DEC, label=\"Decrease\"),\n",
    "    ]\n",
    "    axB.legend(handles=legend_items,\n",
    "               loc=\"center left\", bbox_to_anchor=(1.02, 0.5),\n",
    "               frameon=False, handlelength=1.4, ncol=1)\n",
    "\n",
    "    axB.set_xlim(min(tick_positions) - 1.0, max(tick_positions) + 1.0)\n",
    "\n",
    "    boxA = axA.get_position()\n",
    "    fig.text(boxA.x0 - 0.07, boxA.y1 + 0.005, \"A\",\n",
    "             fontsize=18, fontweight=\"bold\")\n",
    "    boxB = axB.get_position()\n",
    "    fig.text(boxB.x0 - 0.09, boxB.y1 + 0.005, \"B\",\n",
    "             fontsize=18, fontweight=\"bold\")\n",
    "\n",
    "    plt.subplots_adjust(left=0.18, right=0.80, bottom=0.12, top=0.94)\n",
    "\n",
    "    base = os.path.join(outdir, os.path.splitext(filename)[0])\n",
    "    plt.savefig(base + \".png\", **PNG_KW)\n",
    "    plt.savefig(base + \".pdf\", **PDF_KW)\n",
    "    plt.savefig(base + \".svg\", **SVG_KW)\n",
    "    plt.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---------------- Follow-up duration summary ----------------\n",
    "def _summarize_series_years(s: pd.Series):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return dict(N=0, Mean=np.nan, SD=np.nan,\n",
    "                    Median=np.nan, Q1=np.nan, Q3=np.nan,\n",
    "                    IQR=np.nan, Min=np.nan, Max=np.nan)\n",
    "    mean = float(s.mean())\n",
    "    sd = float(s.std(ddof=1))\n",
    "    q1 = float(s.quantile(0.25))\n",
    "    q3 = float(s.quantile(0.75))\n",
    "    return dict(\n",
    "        N=int(s.size),\n",
    "        Mean=mean,\n",
    "        SD=sd,\n",
    "        Median=float(s.median()),\n",
    "        Q1=q1,\n",
    "        Q3=q3,\n",
    "        IQR=q3 - q1,\n",
    "        Min=float(s.min()),\n",
    "        Max=float(s.max()),\n",
    "    )\n",
    "\n",
    "def write_followup_summary(df: pd.DataFrame, label: str, subdir: str):\n",
    "    \"\"\"Summarize follow-up duration (Instance 2 → Instance 3) by group and print summary.\"\"\"\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    rows = []\n",
    "    parts = [\n",
    "        (\"Overall\", df),\n",
    "        (\"Control\", df[df[\"group\"] == \"Control\"]),\n",
    "        (\"Study\", df[df[\"group\"] == \"Study\"]),\n",
    "    ]\n",
    "    for gname, gdf in parts:\n",
    "        stats_row = _summarize_series_years(gdf[\"Delta_years\"])\n",
    "        rows.append({\"Cohort\": label, \"Group\": gname, **stats_row})\n",
    "    tab = pd.DataFrame(rows)\n",
    "\n",
    "    # Save summary table\n",
    "    out_csv = os.path.join(subdir, \"FollowUp_Duration.csv\")\n",
    "    tab.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    def fmt(x):\n",
    "        return \"NA\" if (x is None or (isinstance(x, float) and np.isnan(x))) else f\"{x:.2f}\"\n",
    "\n",
    "    def sentence(row):\n",
    "        n = row[\"N\"]\n",
    "        mean, sd = fmt(row[\"Mean\"]), fmt(row[\"SD\"])\n",
    "        med, q1, q3 = fmt(row[\"Median\"]), fmt(row[\"Q1\"]), fmt(row[\"Q3\"])\n",
    "        mn, mx = fmt(row[\"Min\"]), fmt(row[\"Max\"])\n",
    "        grp = row[\"Group\"]\n",
    "        return (f\"{label} — {grp}: follow-up between Instance 2 and 3 \"\n",
    "                f\"was median {med} years (IQR {q1}–{q3}), mean {mean}±{sd} years; \"\n",
    "                f\"range {mn}–{mx}; N={n}.\")\n",
    "\n",
    "    lines = [sentence(r) for _, r in tab.iterrows()]\n",
    "    out_txt = os.path.join(subdir, \"FollowUp_Duration_narrative.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    # Print concise console summary\n",
    "    print(f\"[{label}] Follow-up duration summary (years):\")\n",
    "    for _, r in tab.iterrows():\n",
    "        med, q1, q3 = fmt(r[\"Median\"]), fmt(r[\"Q1\"]), fmt(r[\"Q3\"])\n",
    "        print(f\"  {r['Group']}: median {med} (IQR {q1}–{q3}); N={r['N']}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Pipeline for one cohort ----------------\n",
    "def run_for_one_cohort(label, filepath):\n",
    "    \"\"\"\n",
    "    Full longitudinal pipeline (I3–I2) for one cohort.\n",
    "    Uses core covariates only (no CMC, no e4, no education).\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== {label} =====\")\n",
    "    subdir = os.path.join(OUTDIR, label)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "    # Input & preprocessing\n",
    "    df = clean_cols(pd.read_csv(filepath))\n",
    "\n",
    "    df[\"Delta_years\"] = (\n",
    "        pd.to_datetime(df[\"Date_of_attending_assessment_centre_Instance_3\"])\n",
    "        - pd.to_datetime(df[\"Date_of_attending_assessment_centre_Instance_2\"])\n",
    "    ).dt.days / 365.25\n",
    "\n",
    "    df = df[(df[\"Delta_years\"] > 0) & (df[\"Delta_years\"] < 8)].copy()\n",
    "\n",
    "    needed = [c for p in wmh_fields.values() for c in p] + [\n",
    "        \"Volume_of_white_matter_Instance_2\",\n",
    "        \"Volume_of_white_matter_Instance_3\",\n",
    "        \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\",\n",
    "        \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_3\",\n",
    "        \"Participant_ID\",\n",
    "        \"match_id\",\n",
    "        \"group\",\n",
    "        \"Delta_years\",\n",
    "    ]\n",
    "    df = df.dropna(subset=[c for c in needed if c in df.columns]).copy()\n",
    "    df[\"group\"] = pd.Categorical(df[\"group\"], [\"Control\", \"Study\"])\n",
    "\n",
    "    # Covariates (no CMC, no e4, no education)\n",
    "    covars = [\n",
    "        \"Age_at_Instance_2\",\n",
    "        \"Sex\",\n",
    "        \"Body_mass_index_BMI_Instance_0\",\n",
    "        \"Genetic_ethnic_grouping\",\n",
    "        \"Smoking_Ever\",\n",
    "        \"Alcohol_intake_frequency_ordinal\",\n",
    "        \"Townsend_deprivation_index_at_recruitment\",\n",
    "    ]\n",
    "    categorical_all = {\"Sex\", \"Genetic_ethnic_grouping\", \"Smoking_Ever\"}\n",
    "\n",
    "    covars_in = [v for v in covars if v in df.columns]\n",
    "    categorical_in = [v for v in covars_in if v in categorical_all]\n",
    "\n",
    "    # Simple imputation\n",
    "    for v in covars_in:\n",
    "        if pd.api.types.is_numeric_dtype(df[v]):\n",
    "            df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "            df[v] = df[v].fillna(df[v].median())\n",
    "        else:\n",
    "            m = df[v].mode(dropna=True)\n",
    "            df[v] = df[v].fillna(m.iloc[0] if not m.empty else df[v])\n",
    "\n",
    "    # Diagnostics: raw vs log1p\n",
    "    diag = []\n",
    "    for region, (f2, f3) in wmh_fields.items():\n",
    "        # HSN\n",
    "        d_abs = df.copy()\n",
    "        y1, _, _ = annual_change_raw_abs_hsn(d_abs, f2, f3)\n",
    "        y2, _, _ = annual_change_log1p_abs_hsn(d_abs, f2, f3)\n",
    "        d_abs[\"Y_raw_abs\"] = y1\n",
    "        d_abs[\"Y_log_abs\"] = y2\n",
    "        r = diagnostic_compare(d_abs, \"Y_raw_abs\", \"Y_log_abs\",\n",
    "                               covars_in, categorical_in)\n",
    "        if r:\n",
    "            diag.append({\"Outcome\": f\"{region} · HSN\", **r})\n",
    "        # Ratio\n",
    "        d_rat = df.copy()\n",
    "        y1, _, _ = annual_change_raw_ratio(d_rat, f2, f3)\n",
    "        y2, _, _ = annual_change_log1p_ratio(d_rat, f2, f3)\n",
    "        d_rat[\"Y_raw_ratio\"] = y1\n",
    "        d_rat[\"Y_log_ratio\"] = y2\n",
    "        r = diagnostic_compare(d_rat, \"Y_raw_ratio\", \"Y_log_ratio\",\n",
    "                               covars_in, categorical_in)\n",
    "        if r:\n",
    "            diag.append({\"Outcome\": f\"{region} · WMH/WM\", **r})\n",
    "\n",
    "    if diag:\n",
    "        pd.DataFrame(diag).to_csv(\n",
    "            os.path.join(subdir, \"Diagnostics_raw_vs_log1p.csv\"),\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\",\n",
    "        )\n",
    "\n",
    "    # Primary models\n",
    "    rr = run_models(df, covars_in, categorical_in, \"raw_ratio\")\n",
    "    ra = run_models(df, covars_in, categorical_in, \"raw_abs\")\n",
    "    lr = run_models(df, covars_in, categorical_in, \"log_ratio\")\n",
    "    la = run_models(df, covars_in, categorical_in, \"log_abs\")  # HSN log1p (%/yr)\n",
    "\n",
    "    # FDR for rr/ra/lr (simple, all regions)\n",
    "    for tab in [rr, ra, lr]:\n",
    "        if not tab.empty:\n",
    "            tab[\"FDR q\"] = multipletests(tab[\"p\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "    # FDR for la: only Periventricular & Deep within cohort; Total excluded\n",
    "    la = la.copy()\n",
    "    la[\"q_FDR_primary\"] = np.nan\n",
    "    la[\"q_FDR_sensitivity\"] = np.nan\n",
    "    if not la.empty:\n",
    "        # 仅对 PWMH / DWMH 做 FDR；若只有 1 个有效 p，则 q = p（确保每个区域都有 q）\n",
    "        region_name = la[\"Outcome\"].astype(str).str.split(\"·\", n=1, expand=True)[0].str.strip()\n",
    "        mask_pd = region_name.isin([\"Periventricular\", \"Deep\"])\n",
    "        idx = la.index[mask_pd & pd.to_numeric(la[\"p\"], errors=\"coerce\").notna()]\n",
    "\n",
    "        if len(idx) == 1:\n",
    "            qvals = pd.to_numeric(la.loc[idx, \"p\"], errors=\"coerce\").values  # q = p\n",
    "        elif len(idx) >= 2:\n",
    "            p_clean = pd.to_numeric(la.loc[idx, \"p\"], errors=\"coerce\").values\n",
    "            qvals = multipletests(p_clean, method=\"fdr_bh\", alpha=0.05)[1]\n",
    "        else:\n",
    "            qvals = None\n",
    "\n",
    "        if qvals is not None:\n",
    "            if label == \"Primary\":\n",
    "                la.loc[idx, \"q_FDR_primary\"] = qvals\n",
    "            elif label == \"Sensitivity\":\n",
    "                la.loc[idx, \"q_FDR_sensitivity\"] = qvals\n",
    "\n",
    "\n",
    "\n",
    "    # Baseline-adjusted HSN log1p models\n",
    "    def run_models_log_abs_with_baseline(df_in, covars_in, categorical_in):\n",
    "        rows = []\n",
    "        for region, (f2, f3) in wmh_fields.items():\n",
    "            d = df_in.copy()\n",
    "            y, _, _ = annual_change_log1p_abs_hsn(d, f2, f3)\n",
    "            v2 = d[f2] * d[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"]\n",
    "            d = d.assign(\n",
    "                Annual_Change=y,\n",
    "                Baseline_log1p=np.log1p(v2),\n",
    "            ).dropna(subset=[\"Annual_Change\", \"Baseline_log1p\",\n",
    "                             \"match_id\", \"group\"])\n",
    "            rhs_terms = [f\"C({c})\" if c in categorical_in else c\n",
    "                         for c in covars_in] + [\"Baseline_log1p\"]\n",
    "            rhs = \" + \".join(rhs_terms)\n",
    "            formula = \"Annual_Change ~ C(group)\" + (f\" + {rhs}\" if rhs else \"\")\n",
    "            fit = fit_cluster(formula, d, \"match_id\")\n",
    "            key = \"C(group)[T.Study]\"\n",
    "            beta = ci_l = ci_u = pval = np.nan\n",
    "            if key in fit.params.index:\n",
    "                beta = float(fit.params[key])\n",
    "                ci_l, ci_u = [float(x) for x in fit.conf_int().loc[key]]\n",
    "                pval = float(fit.pvalues[key])\n",
    "            pct = pct_l = pct_u = np.nan\n",
    "            if np.isfinite(beta):\n",
    "                pct = 100 * (np.exp(beta) - 1)\n",
    "                pct_l = 100 * (np.exp(ci_l) - 1)\n",
    "                pct_u = 100 * (np.exp(ci_u) - 1)\n",
    "            outcome = f\"{region} · HSN (%/yr) — baseline-adjusted\"\n",
    "            rows.append({\n",
    "                \"Outcome\": outcome,\n",
    "                \"β\": beta,\n",
    "                \"95% CI Low\": ci_l,\n",
    "                \"95% CI High\": ci_u,\n",
    "                \"p\": pval,\n",
    "                \"%/year\": pct,\n",
    "                \"%/year CI Low\": pct_l,\n",
    "                \"%/year CI High\": pct_u,\n",
    "                \"N Study\": int(d.loc[d.group == 'Study', 'Participant_ID'].nunique())\n",
    "                           if \"Participant_ID\" in d.columns else int((d.group == 'Study').sum()),\n",
    "                \"N Control\": int(d.loc[d.group == 'Control', 'Participant_ID'].nunique())\n",
    "                             if \"Participant_ID\" in d.columns else int((d.group == 'Control').sum()),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    la_baseline = run_models_log_abs_with_baseline(df, covars_in, categorical_in)\n",
    "\n",
    "    # --- FDR: cohort-wide across PWMH + DWMH (Total excluded) BEFORE saving CSV ---\n",
    "    def _apply_fdr_pwmh_dwmh_in_cohort(tbl: pd.DataFrame, cohort_label: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply BH-FDR across all Periventricular + Deep rows within a cohort.\n",
    "        - Only numeric p-values are included.\n",
    "        - If only one valid test exists, set q = p.\n",
    "        - Writes q into q_FDR_primary or q_FDR_sensitivity according to the cohort.\n",
    "        \"\"\"\n",
    "        if tbl is None or tbl.empty:\n",
    "            return tbl\n",
    "\n",
    "        out = tbl.copy()\n",
    "        # Ensure q columns exist\n",
    "        if \"q_FDR_primary\" not in out.columns:\n",
    "            out[\"q_FDR_primary\"] = np.nan\n",
    "        if \"q_FDR_sensitivity\" not in out.columns:\n",
    "            out[\"q_FDR_sensitivity\"] = np.nan\n",
    "\n",
    "        # Identify PWMH/DWMH rows by region (left of \"·\")\n",
    "        region = out[\"Outcome\"].astype(str).str.split(\"·\", n=1, expand=True)[0].str.strip()\n",
    "        mask_pd = region.isin([\"Periventricular\", \"Deep\"])\n",
    "\n",
    "        # Select valid indices with numeric p\n",
    "        idx = out.index[mask_pd & pd.to_numeric(out[\"p\"], errors=\"coerce\").notna()]\n",
    "        if len(idx) == 0:\n",
    "            return out\n",
    "\n",
    "        # Compute q: one test -> q=p; otherwise BH-FDR\n",
    "        if len(idx) == 1:\n",
    "            qvals = pd.to_numeric(out.loc[idx, \"p\"], errors=\"coerce\").values\n",
    "        else:\n",
    "            pvals = pd.to_numeric(out.loc[idx, \"p\"], errors=\"coerce\").values\n",
    "            qvals = multipletests(pvals, method=\"fdr_bh\", alpha=0.05)[1]\n",
    "\n",
    "        if cohort_label == \"Primary\":\n",
    "            out.loc[idx, \"q_FDR_primary\"] = qvals\n",
    "        elif cohort_label == \"Sensitivity\":\n",
    "            out.loc[idx, \"q_FDR_sensitivity\"] = qvals\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Apply FDR to HSN log1p (unadjusted) and baseline-adjusted tables\n",
    "    la = _apply_fdr_pwmh_dwmh_in_cohort(la, label)\n",
    "    la_baseline = _apply_fdr_pwmh_dwmh_in_cohort(la_baseline, label)\n",
    "\n",
    "    # --- Save regression tables AFTER FDR so q columns are in the CSVs ---\n",
    "    rr.to_csv(os.path.join(subdir, \"WMH_WM_ratio_raw_change.csv\"),\n",
    "              index=False, encoding=\"utf-8-sig\")\n",
    "    ra.to_csv(os.path.join(subdir, \"WMH_HSN_abs_raw_change.csv\"),\n",
    "              index=False, encoding=\"utf-8-sig\")\n",
    "    lr.to_csv(os.path.join(subdir, \"WMH_WM_ratio_log1p_pct_change.csv\"),\n",
    "              index=False, encoding=\"utf-8-sig\")\n",
    "    la.to_csv(os.path.join(subdir, \"WMH_HSN_abs_log1p_pct_change.csv\"),\n",
    "              index=False, encoding=\"utf-8-sig\")\n",
    "    if la_baseline is not None and not la_baseline.empty:\n",
    "        la_baseline.to_csv(\n",
    "            os.path.join(subdir, \"WMH_HSN_abs_log1p_pct_change_BaselineAdjusted.csv\"),\n",
    "            index=False, encoding=\"utf-8-sig\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # Forest plots\n",
    "    forest(rr, \"WMH/WM ratio (raw annualized change)\",\n",
    "           os.path.join(subdir, \"Forest_Ratio_Raw.png\"), logscale=False)\n",
    "    forest(ra, \"HSN-WMH (raw annualized change)\",\n",
    "           os.path.join(subdir, \"Forest_HSN_Raw.png\"), logscale=False)\n",
    "    forest(lr, \"WMH/WM ratio (annualized % change)\",\n",
    "           os.path.join(subdir, \"Forest_Ratio_LogPct.png\"), logscale=True)\n",
    "    forest(la, \"HSN-WMH (annualized % change, log1p)\",\n",
    "           os.path.join(subdir, \"Forest_HSN_LogPct.png\"), logscale=True)\n",
    "    if not la_baseline.empty:\n",
    "        forest(la_baseline,\n",
    "               \"HSN-WMH (annualized % change, log1p) — baseline-adjusted\",\n",
    "               os.path.join(subdir, \"Forest_HSN_LogPct_BaselineAdj.png\"),\n",
    "               logscale=True)\n",
    "\n",
    "    # Increase/Decrease tests\n",
    "    tr = inc_dec_tests(df, use_ratio=True)\n",
    "    ta = inc_dec_tests(df, use_ratio=False)\n",
    "\n",
    "\n",
    "    # FDR only for Peri & Deep (Total excluded) — Ratio (tr)\n",
    "    if not tr.empty:\n",
    "        tr[\"FDR q (primary)\"] = np.nan\n",
    "        m = tr[\"Outcome\"].isin([\"Periventricular\", \"Deep\"])\n",
    "        idx = tr.index[m & pd.to_numeric(tr[\"p_primary\"], errors=\"coerce\").notna()]\n",
    "        if len(idx) >= 1:\n",
    "            p_clean = pd.to_numeric(tr.loc[idx, \"p_primary\"], errors=\"coerce\").values\n",
    "            tr.loc[idx, \"FDR q (primary)\"] = multipletests(p_clean, method=\"fdr_bh\")[1]\n",
    "\n",
    "    if not ta.empty:\n",
    "        ta[\"FDR q (primary)\"] = np.nan\n",
    "        m = ta[\"Outcome\"].isin([\"Periventricular\", \"Deep\"])\n",
    "        idx = ta.index[m & pd.to_numeric(ta[\"p_primary\"], errors=\"coerce\").notna()]\n",
    "        if len(idx) >= 1:\n",
    "            p_clean = pd.to_numeric(ta.loc[idx, \"p_primary\"], errors=\"coerce\").values\n",
    "            ta.loc[idx, \"FDR q (primary)\"] = multipletests(p_clean, method=\"fdr_bh\")[1]\n",
    "\n",
    "    # Combined FDR across HSN+Ratio (Peri/Deep only), drop NaN and allow size=1\n",
    "    comb = []\n",
    "    for df_src, typ in [(tr, \"Ratio\"), (ta, \"HSN\")]:\n",
    "        if not df_src.empty:\n",
    "            sub = df_src[df_src[\"Outcome\"].isin([\"Periventricular\", \"Deep\"])].copy()\n",
    "            sub[\"p_primary\"] = pd.to_numeric(sub[\"p_primary\"], errors=\"coerce\")\n",
    "            for _, r in sub.dropna(subset=[\"p_primary\"]).iterrows():\n",
    "                comb.append((typ, r[\"Outcome\"], float(r[\"p_primary\"])))\n",
    "\n",
    "    if comb:\n",
    "        pvals = np.array([x[2] for x in comb], dtype=float)\n",
    "        qvals = multipletests(pvals, method=\"fdr_bh\")[1]\n",
    "        if not tr.empty:\n",
    "            tr[\"q_FDR_incdec_combined\"] = np.nan\n",
    "        if not ta.empty:\n",
    "            ta[\"q_FDR_incdec_combined\"] = np.nan\n",
    "        for (typ, region, _), qv in zip(comb, qvals):\n",
    "            if typ == \"Ratio\" and not tr.empty:\n",
    "                tr.loc[tr[\"Outcome\"] == region, \"q_FDR_incdec_combined\"] = qv\n",
    "            if typ == \"HSN\" and not ta.empty:\n",
    "                ta.loc[ta[\"Outcome\"] == region, \"q_FDR_incdec_combined\"] = qv\n",
    "\n",
    "\n",
    "    # Export Inc/Dec tables\n",
    "    tr.to_csv(os.path.join(subdir, \"IncreaseDecrease_Ratio.csv\"),\n",
    "              index=False, encoding=\"utf-8-sig\")\n",
    "    ta.to_csv(os.path.join(subdir, \"IncreaseDecrease_HSN.csv\"),\n",
    "              index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Plots\n",
    "    plot_inc_dec_paper(tr, \"WMH/WM\", subdir)\n",
    "    plot_inc_dec_paper(ta, \"HSN\", subdir)\n",
    "\n",
    "    # Within-group Wilcoxon (I2 vs I3 HSN volumes)\n",
    "    rows = []\n",
    "    for region, (f2, f3) in wmh_fields.items():\n",
    "        for grp in [\"Control\", \"Study\"]:\n",
    "            dgrp = df[df[\"group\"] == grp].copy()\n",
    "            y2 = dgrp[f2] * dgrp[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"]\n",
    "            y3 = dgrp[f3] * dgrp[\"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_3\"]\n",
    "            dgrp = dgrp.assign(y2=y2, y3=y3).dropna(subset=[\"y2\", \"y3\"])\n",
    "            if len(dgrp) == 0:\n",
    "                stat, p = np.nan, np.nan\n",
    "            else:\n",
    "                try:\n",
    "                    stat, p = stats.wilcoxon(dgrp[\"y2\"], dgrp[\"y3\"])\n",
    "                except ValueError:\n",
    "                    stat, p = np.nan, np.nan\n",
    "            rows.append({\n",
    "                \"Outcome\": region,\n",
    "                \"Group\": grp,\n",
    "                \"N\": len(dgrp),\n",
    "                \"I2 mean\": np.mean(dgrp[\"y2\"]) if len(dgrp) > 0 else np.nan,\n",
    "                \"I3 mean\": np.mean(dgrp[\"y3\"]) if len(dgrp) > 0 else np.nan,\n",
    "                \"Mean change\": np.mean(dgrp[\"y3\"] - dgrp[\"y2\"]) if len(dgrp) > 0 else np.nan,\n",
    "                \"Median change\": np.median(dgrp[\"y3\"] - dgrp[\"y2\"]) if len(dgrp) > 0 else np.nan,\n",
    "                \"Wilcoxon p\": p,\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        os.path.join(subdir, \"WithinGroup_Wilcoxon.csv\"),\n",
    "        index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    # Follow-up summary\n",
    "    write_followup_summary(df, label, subdir)\n",
    "\n",
    "    # Two-panel combined figure\n",
    "    plot_figureX_hsn(la, ta, subdir,\n",
    "                     filename=\"FigureX_Longitudinal_WMH.png\")\n",
    "\n",
    "    # Methods record\n",
    "    write_methods_record(subdir)\n",
    "\n",
    "# ---------------- Run pipeline for both cohorts ----------------\n",
    "for lab, fp in COHORTS.items():\n",
    "    run_for_one_cohort(lab, fp)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {os.path.abspath(OUTDIR)}\")\n",
    "\n",
    "# =====================================================================\n",
    "# Combine BOTH cohorts & ALL FOUR longitudinal analyses into ONE eTable\n",
    "# =====================================================================\n",
    "\n",
    "import os as _os, re as _re\n",
    "from pathlib import Path\n",
    "\n",
    "OUTDIR = \"Annual_Change\"\n",
    "\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    from docx.oxml import OxmlElement\n",
    "    from docx.oxml.ns import qn\n",
    "    from docx.shared import Pt, Inches\n",
    "except Exception:\n",
    "    Document = None\n",
    "\n",
    "def _safe(s: str) -> str:\n",
    "    return _re.sub(r\"[^0-9A-Za-z._-]+\", \"_\", str(s))\n",
    "\n",
    "def _set_border(cell, **kwargs):\n",
    "    tc = cell._tc\n",
    "    tcPr = tc.get_or_add_tcPr()\n",
    "    tcBorders = tcPr.find(qn('w:tcBorders'))\n",
    "    if tcBorders is None:\n",
    "        tcBorders = OxmlElement('w:tcBorders')\n",
    "        tcPr.append(tcBorders)\n",
    "    for edge in ('left', 'right', 'top', 'bottom', 'insideH', 'insideV'):\n",
    "        if edge in kwargs:\n",
    "            edge_data = kwargs.get(edge)\n",
    "            tag = OxmlElement(f\"w:{edge}\")\n",
    "            for key in (\"val\", \"sz\", \"color\", \"space\"):\n",
    "                if key in edge_data:\n",
    "                    tag.set(qn(f\"w:{key}\"), str(edge_data[key]))\n",
    "            tcBorders.append(tag)\n",
    "\n",
    "def _three_line(table, header_row_idx=0):\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            _set_border(cell,\n",
    "                        left={\"val\": \"nil\"},\n",
    "                        right={\"val\": \"nil\"},\n",
    "                        top={\"val\": \"nil\"},\n",
    "                        bottom={\"val\": \"nil\"})\n",
    "    for cell in table.rows[header_row_idx].cells:\n",
    "        _set_border(cell,\n",
    "                    top={\"val\": \"single\", \"sz\": 8, \"color\": \"000000\"},\n",
    "                    bottom={\"val\": \"single\", \"sz\": 8, \"color\": \"000000\"})\n",
    "    for cell in table.rows[-1].cells:\n",
    "        _set_border(cell,\n",
    "                    bottom={\"val\": \"single\", \"sz\": 8, \"color\": \"000000\"})\n",
    "\n",
    "def _right_align(table, idxs):\n",
    "    for row in table.rows[1:]:\n",
    "        for j in idxs:\n",
    "            if j < len(row.cells):\n",
    "                for p in row.cells[j].paragraphs:\n",
    "                    p.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
    "\n",
    "def _fmt_num(x, nd=1):\n",
    "    return \"\" if pd.isna(x) else f\"{float(x):.{nd}f}\"\n",
    "\n",
    "def _fmt_p(x):\n",
    "    return \"—\" if pd.isna(x) else (\"<0.001\" if float(x) < 1e-3 else f\"{float(x):.3f}\")\n",
    "\n",
    "def _fmt_q(x):\n",
    "    return \"—\" if pd.isna(x) else (\"<0.001\" if float(x) < 1e-3 else f\"{float(x):.3f}\")\n",
    "\n",
    "ANL = [\n",
    "    dict(key=\"HSN_log1p\",\n",
    "         file=\"WMH_HSN_abs_log1p_pct_change.csv\",\n",
    "         metric=\"HSN (log1p %/yr)\",\n",
    "         eff=\"%/year\", lo=\"%/year CI Low\", hi=\"%/year CI High\",\n",
    "         pct=True, q_mode=\"by_cohort\"),\n",
    "    dict(key=\"WMHWM_log1p\",\n",
    "         file=\"WMH_WM_ratio_log1p_pct_change.csv\",\n",
    "         metric=\"WMH/WM (log1p %/yr)\",\n",
    "         eff=\"%/year\", lo=\"%/year CI Low\", hi=\"%/year CI High\",\n",
    "         pct=True, q_mode=None),\n",
    "    dict(key=\"HSN_raw\",\n",
    "         file=\"WMH_HSN_abs_raw_change.csv\",\n",
    "         metric=\"HSN (β/yr)\",\n",
    "         eff=\"β\", lo=\"95% CI Low\", hi=\"95% CI High\",\n",
    "         pct=False, q_mode=None),\n",
    "    dict(key=\"WMHWM_raw\",\n",
    "         file=\"WMH_WM_ratio_raw_change.csv\",\n",
    "         metric=\"WMH/WM (β/yr)\",\n",
    "         eff=\"β\", lo=\"95% CI Low\", hi=\"95% CI High\",\n",
    "         pct=False, q_mode=None),\n",
    "]\n",
    "\n",
    "REGION_MAP = {\"Total\": \"Total WMH\", \"Periventricular\": \"PWMH\", \"Deep\": \"DWMH\"}\n",
    "\n",
    "def _pick_q_row(row, cohort_label: str):\n",
    "    c = cohort_label.lower()\n",
    "    if c.startswith(\"primary\") and (\"q_FDR_primary\" in row.index):\n",
    "        return row.get(\"q_FDR_primary\", np.nan)\n",
    "    if c.startswith(\"sensitivity\") and (\"q_FDR_sensitivity\" in row.index):\n",
    "        return row.get(\"q_FDR_sensitivity\", np.nan)\n",
    "    return np.nan\n",
    "\n",
    "def _gather_all():\n",
    "    rows = []\n",
    "    order_region = [\"Total\", \"Periventricular\", \"Deep\"]\n",
    "    for cohort_label in COHORTS.keys():\n",
    "        subdir = _os.path.join(OUTDIR, cohort_label)\n",
    "        for cfg in ANL:\n",
    "            p = _os.path.join(subdir, cfg[\"file\"])\n",
    "            if not _os.path.exists(p):\n",
    "                print(f\"[Warn] Missing: {p}\")\n",
    "                continue\n",
    "            df = pd.read_csv(p)\n",
    "            region = df[\"Outcome\"].astype(str).str.split(\"·\", n=1, expand=True)[0].str.strip()\n",
    "            df[\"Region\"] = region\n",
    "            df = df[df[\"Region\"].isin(order_region)].copy()\n",
    "            for _, r in df.iterrows():\n",
    "                eff = pd.to_numeric(r.get(cfg[\"eff\"]), errors=\"coerce\")\n",
    "                lo = pd.to_numeric(r.get(cfg[\"lo\"]), errors=\"coerce\")\n",
    "                hi = pd.to_numeric(r.get(cfg[\"hi\"]), errors=\"coerce\")\n",
    "                pval = pd.to_numeric(r.get(\"p\"), errors=\"coerce\")\n",
    "                qval = _pick_q_row(r, cohort_label) if cfg[\"q_mode\"] == \"by_cohort\" else np.nan\n",
    "                rows.append({\n",
    "                    \"Cohort\": cohort_label,\n",
    "                    \"Metric\": cfg[\"metric\"],\n",
    "                    \"Outcome\": REGION_MAP.get(r[\"Region\"], r[\"Region\"]),\n",
    "                    \"Effect\": eff,\n",
    "                    \"CI Low\": lo,\n",
    "                    \"CI High\": hi,\n",
    "                    \"p\": pval,\n",
    "                    \"q\": qval,\n",
    "                    \"is_pct\": cfg[\"pct\"],\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def export_single_eTable(all_df: pd.DataFrame,\n",
    "                         title=\"eTable 7. Longitudinal WMH changes across analytic cohorts and metrics\",\n",
    "                         legend=(\n",
    "                             \"Adjusted annualized between-group differences are shown for Primary and \"\n",
    "                             \"Sensitivity cohorts. Metrics: HSN (log1p %/yr), WMH/WM (log1p %/yr), \"\n",
    "                             \"HSN (β/yr), WMH/WM (β/yr). For HSN (log1p %/yr), BH-FDR was applied \"\n",
    "                             \"within cohort to PWMH and DWMH only; Total WMH excluded. Models adjust \"\n",
    "                             \"for age at Instance 2, sex, BMI at Instance 0, genetic ethnic grouping, \"\n",
    "                             \"smoking (ever), alcohol-intake frequency, Townsend index; cluster-robust \"\n",
    "                             \"SEs by match_id or HC3 where clustering not feasible. Two-sided α=0.05.\"\n",
    "                         )):\n",
    "    if all_df is None or all_df.empty:\n",
    "        print(\"[Info] No results to export.\")\n",
    "        return None\n",
    "\n",
    "    all_df = all_df.copy()\n",
    "    all_df[\"Analytic cohort\"] = all_df[\"Cohort\"].replace({\n",
    "        \"Primary\": \"Primary PSM cohort\",\n",
    "        \"Sensitivity\": \"PSM–Sensitivity cohort\",\n",
    "    })\n",
    "    all_df = all_df.drop(columns=[\"Cohort\"])\n",
    "\n",
    "    order_metric = [a[\"metric\"] for a in ANL]\n",
    "    order_region = [\"Total WMH\", \"PWMH\", \"DWMH\"]\n",
    "\n",
    "    all_df[\"Analytic cohort\"] = pd.Categorical(\n",
    "        all_df[\"Analytic cohort\"],\n",
    "        categories=[\"Primary PSM cohort\", \"PSM–Sensitivity cohort\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "    all_df[\"Metric\"] = pd.Categorical(\n",
    "        all_df[\"Metric\"], categories=order_metric, ordered=True\n",
    "    )\n",
    "    all_df[\"Outcome\"] = pd.Categorical(\n",
    "        all_df[\"Outcome\"], categories=order_region, ordered=True\n",
    "    )\n",
    "\n",
    "    df = all_df.sort_values(\n",
    "        [\"Analytic cohort\", \"Metric\", \"Outcome\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    csv_path = _os.path.join(OUTDIR,\n",
    "                             \"Longitudinal_AllAnalyses_AllCohorts.csv\")\n",
    "    df_out = df[[\n",
    "        \"Analytic cohort\", \"Metric\", \"Outcome\",\n",
    "        \"Effect\", \"CI Low\", \"CI High\", \"p\", \"q\"\n",
    "    ]].copy()\n",
    "    df_out.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Combined CSV saved: {csv_path}\")\n",
    "\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: Word eTable skipped.\")\n",
    "        return None\n",
    "\n",
    "    doc = Document()\n",
    "    try:\n",
    "        doc.styles[\"Normal\"].font.name = \"Times New Roman\"\n",
    "        doc.styles[\"Normal\"].font.size = Pt(10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    p = doc.add_paragraph()\n",
    "    r = p.add_run(title)\n",
    "    r.bold = True\n",
    "\n",
    "    headers = [\n",
    "        \"Analytic cohort\", \"Metric\", \"Outcome\",\n",
    "        \"Effect\", \"95% CI (low, high)\", \"p\", \"q\",\n",
    "    ]\n",
    "    t = doc.add_table(rows=1, cols=len(headers))\n",
    "    for j, h in enumerate(headers):\n",
    "        t.rows[0].cells[j].text = h\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        ci_txt = \"\" if any(pd.isna([r[\"CI Low\"], r[\"CI High\"]])) else \\\n",
    "                 f\"({_fmt_num(r['CI Low'], 1 if r['is_pct'] else 3)}, {_fmt_num(r['CI High'], 1 if r['is_pct'] else 3)})\"\n",
    "        vals = [\n",
    "            str(r[\"Analytic cohort\"]),\n",
    "            str(r[\"Metric\"]),\n",
    "            str(r[\"Outcome\"]),\n",
    "            _fmt_num(r[\"Effect\"], 1 if r[\"is_pct\"] else 3),\n",
    "            ci_txt,\n",
    "            _fmt_p(r[\"p\"]),\n",
    "            _fmt_q(r[\"q\"]),\n",
    "        ]\n",
    "        for j, v in enumerate(vals):\n",
    "            cells[j].text = v\n",
    "\n",
    "    _right_align(t, idxs=[3, 4, 5, 6])\n",
    "    _three_line(t, header_row_idx=0)\n",
    "\n",
    "    doc.add_paragraph().add_run(legend).italic = True\n",
    "\n",
    "    docx_path = _os.path.join(\n",
    "        OUTDIR, \"Longitudinal_AllAnalyses_AllCohorts.docx\"\n",
    "    )\n",
    "    doc.save(docx_path)\n",
    "    print(f\"Word eTable saved: {docx_path}\")\n",
    "    return docx_path\n",
    "\n",
    "_all = _gather_all()\n",
    "export_single_eTable(_all)\n",
    "\n",
    "# =====================================================================\n",
    "# eTable: Increase vs Decrease (between-group)\n",
    "# =====================================================================\n",
    "\n",
    "def export_incdec_eTable(cohorts=COHORTS,\n",
    "                         title=\"eTable 8. Increase vs Decrease tests across analytic cohorts and WMH regions\",\n",
    "                         legend=(\n",
    "                             \"Proportion of participants with increased WMH per year shown as n (%). \"\n",
    "                             \"Tests: Fisher's exact or Chi-square. BH-FDR applied only to PWMH and DWMH; \"\n",
    "                             \"Total WMH excluded.\"\n",
    "                         )):\n",
    "    rows = []\n",
    "    for cohort_label in cohorts.keys():\n",
    "        if cohort_label == \"Primary\":\n",
    "            cohort_name = \"Primary PSM cohort\"\n",
    "        elif cohort_label == \"Sensitivity\":\n",
    "            cohort_name = \"PSM–Sensitivity cohort\"\n",
    "        else:\n",
    "            cohort_name = cohort_label\n",
    "\n",
    "        subdir = _os.path.join(OUTDIR, cohort_label)\n",
    "        for typ, fname in [(\"HSN\", \"IncreaseDecrease_HSN.csv\"),\n",
    "                           (\"WMH/WM\", \"IncreaseDecrease_Ratio.csv\")]:\n",
    "            fpath = _os.path.join(subdir, fname)\n",
    "            if not _os.path.exists(fpath):\n",
    "                print(f\"[Warn] Missing: {fpath}\")\n",
    "                continue\n",
    "            df = pd.read_csv(fpath)\n",
    "\n",
    "            for _, r in df.iterrows():\n",
    "                n_s = int(r.get(\"Study N\", np.nan)) if not pd.isna(r.get(\"Study N\", np.nan)) else 0\n",
    "                n_c = int(r.get(\"Control N\", np.nan)) if not pd.isna(r.get(\"Control N\", np.nan)) else 0\n",
    "                inc_s = int(r.get(\"Study Increase\", np.nan)) if not pd.isna(r.get(\"Study Increase\", np.nan)) else 0\n",
    "                inc_c = int(r.get(\"Control Increase\", np.nan)) if not pd.isna(r.get(\"Control Increase\", np.nan)) else 0\n",
    "                pct_s = (100 * inc_s / n_s) if n_s > 0 else np.nan\n",
    "                pct_c = (100 * inc_c / n_c) if n_c > 0 else np.nan\n",
    "\n",
    "                study_str = f\"{inc_s} ({pct_s:.1f}%)\" if n_s > 0 else \"—\"\n",
    "                ctrl_str = f\"{inc_c} ({pct_c:.1f}%)\" if n_c > 0 else \"—\"\n",
    "                n_str = f\"{n_s} / {n_c}\" if (n_s > 0 and n_c > 0) else \"—\"\n",
    "\n",
    "                rows.append({\n",
    "                    \"Analytic cohort\": cohort_name,\n",
    "                    \"N (SA/Control)\": n_str,\n",
    "                    \"Metric\": typ,\n",
    "                    \"Outcome\": REGION_MAP.get(r[\"Outcome\"], r[\"Outcome\"]),\n",
    "                    \"Study Increase n (%)\": study_str,\n",
    "                    \"Control Increase n (%)\": ctrl_str,\n",
    "                    \"Prop Diff (S-C)\": _fmt_num(r.get(\"Prop Diff (S-C)\"), 3),\n",
    "                    \"95% CI\": (\"\" if any(pd.isna([r.get(\"95% CI Low\"),\n",
    "                                                  r.get(\"95% CI High\")]))\n",
    "                               else f\"({_fmt_num(r['95% CI Low'], 3)}, \"\n",
    "                                    f\"{_fmt_num(r['95% CI High'], 3)})\"),\n",
    "                    \"p\": _fmt_p(r.get(\"p_primary\")),\n",
    "                    \"q\": _fmt_q(r.get(\"q_FDR_incdec_combined\", np.nan)),\n",
    "                })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[Info] No Increase/Decrease results found.\")\n",
    "        return None\n",
    "\n",
    "    df_all = pd.DataFrame(rows)\n",
    "\n",
    "    metric_order = [\"HSN\", \"WMH/WM\"]\n",
    "    outcome_order = [\"Total WMH\", \"PWMH\", \"DWMH\"]\n",
    "\n",
    "    df_all[\"Metric\"] = pd.Categorical(\n",
    "        df_all[\"Metric\"], categories=metric_order, ordered=True\n",
    "    )\n",
    "    df_all[\"Outcome\"] = pd.Categorical(\n",
    "        df_all[\"Outcome\"], categories=outcome_order, ordered=True\n",
    "    )\n",
    "    df_all[\"Analytic cohort\"] = pd.Categorical(\n",
    "        df_all[\"Analytic cohort\"],\n",
    "        categories=[\"Primary PSM cohort\", \"PSM–Sensitivity cohort\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    df_all = df_all.sort_values(\n",
    "        [\"Analytic cohort\", \"Metric\", \"Outcome\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    csv_path = _os.path.join(OUTDIR, \"IncreaseDecrease_AllCohorts.csv\")\n",
    "    df_all.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"CSV saved: {csv_path}\")\n",
    "\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: Word eTable skipped.\")\n",
    "        return None\n",
    "\n",
    "    doc = Document()\n",
    "    try:\n",
    "        doc.styles[\"Normal\"].font.name = \"Times New Roman\"\n",
    "        doc.styles[\"Normal\"].font.size = Pt(10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    p = doc.add_paragraph()\n",
    "    r = p.add_run(title)\n",
    "    r.bold = True\n",
    "\n",
    "    headers = [\n",
    "        \"Analytic cohort\", \"N (SA/Control)\", \"Metric\", \"Outcome\",\n",
    "        \"Study Increase n (%)\", \"Control Increase n (%)\",\n",
    "        \"Prop Diff (S-C)\", \"95% CI\", \"p\", \"q\",\n",
    "    ]\n",
    "    t = doc.add_table(rows=1, cols=len(headers))\n",
    "    for j, h in enumerate(headers):\n",
    "        t.rows[0].cells[j].text = h\n",
    "\n",
    "    for _, r in df_all.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        vals = [str(r[h]) for h in headers]\n",
    "        for j, v in enumerate(vals):\n",
    "            cells[j].text = v\n",
    "\n",
    "    _right_align(t, idxs=[1, 4, 5, 6, 7, 8, 9])\n",
    "    _three_line(t, header_row_idx=0)\n",
    "\n",
    "    doc.add_paragraph().add_run(legend).italic = True\n",
    "\n",
    "    docx_path = _os.path.join(OUTDIR, \"IncreaseDecrease_AllCohorts.docx\")\n",
    "    doc.save(docx_path)\n",
    "    print(f\"Word eTable saved: {docx_path}\")\n",
    "    return docx_path\n",
    "\n",
    "export_incdec_eTable()\n",
    "\n",
    "# =====================================================================\n",
    "# Within-group Increase vs Decrease (binomial vs 50%)\n",
    "# =====================================================================\n",
    "\n",
    "def export_within_group_incdec(cohorts=COHORTS,\n",
    "                               title=\"eTable 11. Within-group Increase vs Decrease tests (SA and Control, by cohort)\",\n",
    "                               legend=(\n",
    "                                   \"Within-group exact binomial tests (two-sided, p0=0.5) assess whether the \"\n",
    "                                   \"proportion with WMH increase vs decrease deviates from 50%, separately in \"\n",
    "                                   \"SA and Control. BH-FDR applied only to PWMH and DWMH; Total WMH excluded.\"\n",
    "                               )):\n",
    "    rows = []\n",
    "\n",
    "    for cohort_label in cohorts.keys():\n",
    "        cohort_name = (\"Primary PSM cohort\" if cohort_label == \"Primary\"\n",
    "                       else \"PSM–Sensitivity cohort\" if cohort_label == \"Sensitivity\"\n",
    "                       else cohort_label)\n",
    "        subdir = _os.path.join(OUTDIR, cohort_label)\n",
    "\n",
    "        for typ, fname in [(\"HSN\", \"IncreaseDecrease_HSN.csv\"),\n",
    "                           (\"WMH/WM\", \"IncreaseDecrease_Ratio.csv\")]:\n",
    "            fpath = _os.path.join(subdir, fname)\n",
    "            if not _os.path.exists(fpath):\n",
    "                print(f\"[Warn] Missing: {fpath}\")\n",
    "                continue\n",
    "            df = pd.read_csv(fpath)\n",
    "            df = df.rename(columns=lambda x: x.replace(\"Study\", \"SA\") if isinstance(x, str) else x)\n",
    "\n",
    "            for _, r in df.iterrows():\n",
    "                outcome = REGION_MAP.get(r[\"Outcome\"], r[\"Outcome\"])\n",
    "                for grp in [\"SA\", \"Control\"]:\n",
    "                    n = int(r.get(f\"{grp} N\", np.nan)) if not pd.isna(r.get(f\"{grp} N\", np.nan)) else 0\n",
    "                    inc = int(r.get(f\"{grp} Increase\", np.nan)) if not pd.isna(r.get(f\"{grp} Increase\", np.nan)) else 0\n",
    "                    dec = int(r.get(f\"{grp} Decrease\", np.nan)) if not pd.isna(r.get(f\"{grp} Decrease\", np.nan)) else 0\n",
    "                    nn = inc + dec\n",
    "                    if nn <= 0:\n",
    "                        continue\n",
    "\n",
    "                    bt = stats.binomtest(k=inc, n=nn, p=0.5, alternative=\"two-sided\")\n",
    "                    pval = float(bt.pvalue)\n",
    "\n",
    "                    rows.append({\n",
    "                        \"Analytic cohort\": cohort_name,\n",
    "                        \"Metric\": typ,\n",
    "                        \"Outcome\": outcome,\n",
    "                        \"Group\": grp,\n",
    "                        \"N\": nn,\n",
    "                        \"Increase n\": inc,\n",
    "                        \"Decrease n\": dec,\n",
    "                        \"Increase %\": f\"{100 * inc / nn:.1f}%\",\n",
    "                        \"Decrease %\": f\"{100 * dec / nn:.1f}%\",\n",
    "                        \"Test\": \"Exact binomial (p0=0.5)\",\n",
    "                        \"p\": pval,\n",
    "                        \"q\": np.nan,\n",
    "                    })\n",
    "\n",
    "    if not rows:\n",
    "        print(\"[Info] No within-group results found.\")\n",
    "        return None\n",
    "\n",
    "    df_all = pd.DataFrame(rows)\n",
    "\n",
    "    df_all[\"Analytic cohort\"] = pd.Categorical(\n",
    "        df_all[\"Analytic cohort\"],\n",
    "        categories=[\"Primary PSM cohort\", \"PSM–Sensitivity cohort\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    outcome_order = {\n",
    "        \"Total WMH\": 0,\n",
    "        \"PWMH\": 1, \"Periventricular WMH\": 1, \"Periventricular\": 1,\n",
    "        \"DWMH\": 2, \"Deep WMH\": 2, \"Deep\": 2,\n",
    "    }\n",
    "    df_all[\"Outcome_order\"] = df_all[\"Outcome\"].map(outcome_order).fillna(99)\n",
    "\n",
    "    df_all = df_all.sort_values(\n",
    "        [\"Analytic cohort\", \"Metric\", \"Group\", \"Outcome_order\"]\n",
    "    ).reset_index(drop=True)\n",
    "    df_all = df_all.drop(columns=[\"Outcome_order\"])\n",
    "\n",
    "    # FDR within (cohort × metric × group) for PWMH/DWMH only\n",
    "    df_all[\"q\"] = np.nan\n",
    "    for (cohort_name, typ, grp), sub in df_all.groupby([\"Analytic cohort\", \"Metric\", \"Group\"]):\n",
    "        mask = sub[\"Outcome\"].isin([\"PWMH\", \"Periventricular WMH\", \"Periventricular\",\n",
    "                                    \"DWMH\", \"Deep WMH\", \"Deep\"])\n",
    "        idx = sub.index[mask & pd.to_numeric(sub[\"p\"], errors=\"coerce\").notna()]\n",
    "        if len(idx) >= 1:\n",
    "            p_clean = pd.to_numeric(sub.loc[idx, \"p\"], errors=\"coerce\").values\n",
    "            qvals = multipletests(p_clean, method=\"fdr_bh\")[1]\n",
    "            df_all.loc[idx, \"q\"] = qvals\n",
    "\n",
    "\n",
    "    csv_path = _os.path.join(OUTDIR, \"WithinGroup_IncDec_AllCohorts.csv\")\n",
    "    df_out = df_all.copy()\n",
    "    df_out[\"p\"] = df_out[\"p\"].apply(_fmt_p)\n",
    "    df_out[\"q\"] = df_out[\"q\"].apply(_fmt_q)\n",
    "    df_out.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Within-group Inc/Dec CSV saved: {csv_path}\")\n",
    "\n",
    "    if Document is None:\n",
    "        print(\"python-docx not available: Word eTable skipped.\")\n",
    "        return None\n",
    "\n",
    "    doc = Document()\n",
    "    try:\n",
    "        doc.styles[\"Normal\"].font.name = \"Times New Roman\"\n",
    "        doc.styles[\"Normal\"].font.size = Pt(10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    p = doc.add_paragraph()\n",
    "    r = p.add_run(title)\n",
    "    r.bold = True\n",
    "\n",
    "    headers = [\n",
    "        \"Analytic cohort\", \"Metric\", \"Outcome\", \"Group\", \"N\",\n",
    "        \"Increase n\", \"Decrease n\",\n",
    "        \"Increase %\", \"Decrease %\",\n",
    "        \"p\", \"q\",\n",
    "    ]\n",
    "    t = doc.add_table(rows=1, cols=len(headers))\n",
    "    for j, h in enumerate(headers):\n",
    "        t.rows[0].cells[j].text = h\n",
    "\n",
    "    for _, r in df_out.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        vals = [str(r[h]) for h in headers]\n",
    "        for j, v in enumerate(vals):\n",
    "            cells[j].text = v\n",
    "\n",
    "    _right_align(t, idxs=[4, 5, 6, 7, 8, 9, 10])\n",
    "    _three_line(t, header_row_idx=0)\n",
    "\n",
    "    doc.add_paragraph().add_run(legend).italic = True\n",
    "\n",
    "    docx_path = _os.path.join(OUTDIR, \"WithinGroup_IncDec_AllCohorts.docx\")\n",
    "    doc.save(docx_path)\n",
    "    print(f\"Word eTable saved: {docx_path}\")\n",
    "    return docx_path\n",
    "\n",
    "export_within_group_incdec()\n",
    "\n",
    "# =====================================================================\n",
    "# SA I2 - Dx distribution (final; explicit Dx column)\n",
    "# =====================================================================\n",
    "\n",
    "SAVE_DIR = os.path.join(OUTDIR, \"SA_Dx_Timing\")\n",
    "\n",
    "def _plot_sa_i2_minus_dx_for_cohort(cohort_label, csv_path, outdir=SAVE_DIR):\n",
    "    \"\"\"Plot SA (Study) distribution of (Instance 2 date – SA diagnosis date) in years, per cohort.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[Skip] {cohort_label}: file not found -> {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # Load and normalize column names\n",
    "    df = clean_cols(pd.read_csv(csv_path))\n",
    "\n",
    "    # Required columns (diagnosis column corrected)\n",
    "    req = [\n",
    "        \"group\",\n",
    "        \"Date_of_attending_assessment_centre_Instance_2\",\n",
    "        \"Date_G47_first_reported_sleep_disorders\",\n",
    "    ]\n",
    "    missing = [c for c in req if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[Skip] {cohort_label}: missing columns -> {missing}\")\n",
    "        return\n",
    "\n",
    "    # SA only\n",
    "    d = df[df[\"group\"].astype(str).str.strip().str.capitalize() == \"Study\"].copy()\n",
    "    if d.empty:\n",
    "        print(f\"[Info] {cohort_label}: Study group is empty.\")\n",
    "        return\n",
    "\n",
    "    # Compute (I2 – Dx) in years\n",
    "    i2 = pd.to_datetime(d[\"Date_of_attending_assessment_centre_Instance_2\"], errors=\"coerce\")\n",
    "    dx = pd.to_datetime(d[\"Date_G47_first_reported_sleep_disorders\"], errors=\"coerce\")\n",
    "    mask = i2.notna() & dx.notna()\n",
    "    if not mask.any():\n",
    "        print(f\"[Info] {cohort_label}: no valid I2 and Dx pairs.\")\n",
    "        return\n",
    "\n",
    "    diff_years = (i2[mask] - dx[mask]).dt.days / 365.25\n",
    "    diff_years = diff_years.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if diff_years.empty:\n",
    "        print(f\"[Info] {cohort_label}: I2–Dx data empty after cleaning.\")\n",
    "        return\n",
    "\n",
    "    # Descriptives\n",
    "    n = diff_years.size\n",
    "    mu = diff_years.mean()\n",
    "    sd = diff_years.std(ddof=1)\n",
    "    q1, med, q3 = diff_years.quantile([0.25, 0.50, 0.75])\n",
    "    mn, mx = diff_years.min(), diff_years.max()\n",
    "    n_pos = int((diff_years > 0).sum())\n",
    "    n_neg = int((diff_years < 0).sum())\n",
    "\n",
    "    print(\n",
    "        f\"[{cohort_label}] SA I2–Dx (years): N={n} | mean={mu:.2f}, SD={sd:.2f} | \"\n",
    "        f\"median={med:.2f}, IQR={q1:.2f}–{q3:.2f} | min={mn:.2f}, max={mx:.2f} | \"\n",
    "        f\"positive={n_pos}, negative={n_neg}\"\n",
    "    )\n",
    "\n",
    "    # Histogram\n",
    "    lo, hi = int(np.floor(mn)), int(np.ceil(mx))\n",
    "    bins = np.arange(lo, hi + 1, 1) if hi > lo else 15\n",
    "\n",
    "    plt.rcParams.update(PUB_RC)\n",
    "    fig, ax = plt.subplots(figsize=(6.8, 4.2))\n",
    "    ax.hist(diff_years, bins=bins, edgecolor=\"white\", label=\"SA (Study)\")\n",
    "    ax.axvline(0, color=\"grey\", ls=\"--\", lw=1)\n",
    "    ax.set_title(f\"{cohort_label} — SA: Timing of I2 relative to diagnosis (I2 – Dx)\")\n",
    "    ax.set_xlabel(\"Years (I2 – Dx) [positive = Dx before I2; negative = Dx after I2]\")\n",
    "    ax.set_ylabel(\"Participants\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    txt = (\n",
    "        f\"N={n}\\nMean={mu:.2f}, SD={sd:.2f}\\n\"\n",
    "        f\"Median={med:.2f} (IQR {q1:.2f}–{q3:.2f})\\n\"\n",
    "        f\"Min={mn:.2f}, Max={mx:.2f}\"\n",
    "    )\n",
    "    ax.text(\n",
    "        0.98, 0.98, txt,\n",
    "        transform=ax.transAxes,\n",
    "        ha=\"right\", va=\"top\",\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"#cccccc\"),\n",
    "    )\n",
    "\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    out_subdir = os.path.join(outdir, cohort_label)\n",
    "    os.makedirs(out_subdir, exist_ok=True)\n",
    "    base = os.path.join(out_subdir, \"SA_I2_minus_Dx_Distribution\")\n",
    "    fig.savefig(base + \".png\", **PNG_KW)\n",
    "    fig.savefig(base + \".pdf\", **PDF_KW)\n",
    "    fig.savefig(base + \".svg\", **SVG_KW)\n",
    "    fig.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "for _lab, _fp in COHORTS.items():\n",
    "    _plot_sa_i2_minus_dx_for_cohort(_lab, _fp, outdir=SAVE_DIR)\n",
    "\n",
    "# =====================================================================\n",
    "# SA-only — Dx duration (I2 - Dx) by Increase vs Decrease (Total HSN)\n",
    "# =====================================================================\n",
    "\n",
    "from scipy import stats as _stats\n",
    "\n",
    "def _summarize_series_noclip(s: pd.Series):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return dict(N=0, Mean=np.nan, SD=np.nan,\n",
    "                    Median=np.nan, Q1=np.nan, Q3=np.nan,\n",
    "                    IQR=np.nan, Min=np.nan, Max=np.nan)\n",
    "    mean = float(s.mean())\n",
    "    sd = float(s.std(ddof=1))\n",
    "    q1 = float(s.quantile(0.25))\n",
    "    q3 = float(s.quantile(0.75))\n",
    "    return dict(\n",
    "        N=int(s.size),\n",
    "        Mean=mean,\n",
    "        SD=sd,\n",
    "        Median=float(s.median()),\n",
    "        Q1=q1,\n",
    "        Q3=q3,\n",
    "        IQR=q3 - q1,\n",
    "        Min=float(s.min()),\n",
    "        Max=float(s.max()),\n",
    "    )\n",
    "\n",
    "def _analyze_sa_dx_duration_by_incdec(cohort_label, filepath, out_root):\n",
    "    \"\"\"\n",
    "    SA-only, split by Total WMH HSN absolute change (Increase vs Decrease),\n",
    "    outcome = (I2 - Dx) in years (no clipping). Mann–Whitney U test.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"[Skip] {cohort_label}: file not found -> {filepath}\")\n",
    "        return\n",
    "\n",
    "    df = clean_cols(pd.read_csv(filepath))\n",
    "\n",
    "    col_I2 = \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "    col_T2 = \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\"\n",
    "    col_T3 = \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_3\"\n",
    "    col_S2 = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "    col_S3 = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_3\"\n",
    "    col_DX = \"Date_G47_first_reported_sleep_disorders\"\n",
    "\n",
    "    needed = [col_I2, col_T2, col_T3, col_S2, col_S3, col_DX, \"group\"]\n",
    "    miss = [c for c in needed if c not in df.columns]\n",
    "    if miss:\n",
    "        print(f\"[Skip] {cohort_label}: missing columns -> {miss}\")\n",
    "        return\n",
    "\n",
    "    d = df[df[\"group\"].astype(str).str.strip().str.capitalize() == \"Study\"].copy()\n",
    "    if d.empty:\n",
    "        print(f\"[Info] {cohort_label}: SA group empty.\")\n",
    "        return\n",
    "\n",
    "    d[col_I2] = pd.to_datetime(d[col_I2], errors=\"coerce\")\n",
    "    d[col_DX] = pd.to_datetime(d[col_DX], errors=\"coerce\")\n",
    "    d[\"Dx_years_I2_minus_Dx\"] = (\n",
    "        d[col_I2] - d[col_DX]\n",
    "    ).dt.days / 365.25\n",
    "\n",
    "    d = d.dropna(subset=[\n",
    "        \"Dx_years_I2_minus_Dx\",\n",
    "        col_T2, col_T3, col_S2, col_S3\n",
    "    ]).copy()\n",
    "    if d.empty:\n",
    "        print(f\"[Info] {cohort_label}: no valid I2-Dx values in SA group.\")\n",
    "        return\n",
    "\n",
    "    v2 = pd.to_numeric(d[col_T2], errors=\"coerce\") * pd.to_numeric(d[col_S2], errors=\"coerce\")\n",
    "    v3 = pd.to_numeric(d[col_T3], errors=\"coerce\") * pd.to_numeric(d[col_S3], errors=\"coerce\")\n",
    "    d[\"IncDec\"] = np.where(v3 > v2, \"Increase\", \"Decrease\")\n",
    "\n",
    "    inc = d.loc[d[\"IncDec\"] == \"Increase\", \"Dx_years_I2_minus_Dx\"]\n",
    "    dec = d.loc[d[\"IncDec\"] == \"Decrease\", \"Dx_years_I2_minus_Dx\"]\n",
    "    all_sa = d[\"Dx_years_I2_minus_Dx\"]\n",
    "\n",
    "    sum_inc = _summarize_series_noclip(inc)\n",
    "    sum_dec = _summarize_series_noclip(dec)\n",
    "    sum_all = _summarize_series_noclip(all_sa)\n",
    "\n",
    "    if sum_inc[\"N\"] > 0 and sum_dec[\"N\"] > 0:\n",
    "        try:\n",
    "            U, p_mwu = _stats.mannwhitneyu(inc, dec, alternative=\"two-sided\")\n",
    "        except Exception:\n",
    "            U, p_mwu = np.nan, np.nan\n",
    "    else:\n",
    "        U, p_mwu = np.nan, np.nan\n",
    "\n",
    "    outdir = os.path.join(out_root, \"SA_DxDuration_ByIncDec\", cohort_label)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    def _fmt(v, nd=2):\n",
    "        return \"NA\" if (v is None or (isinstance(v, float) and np.isnan(v))) else f\"{float(v):.{nd}f}\"\n",
    "\n",
    "    tbl = pd.DataFrame([\n",
    "        {\"Cohort\": cohort_label, \"Subgroup\": \"Increase\", **sum_inc},\n",
    "        {\"Cohort\": cohort_label, \"Subgroup\": \"Decrease\", **sum_dec},\n",
    "        {\"Cohort\": cohort_label, \"Subgroup\": \"Overall SA\", **sum_all},\n",
    "    ])\n",
    "    tbl[\"Mean±SD (years)\"] = tbl.apply(\n",
    "        lambda r: f\"{_fmt(r['Mean'])}±{_fmt(r['SD'])}\", axis=1\n",
    "    )\n",
    "    tbl[\"Median [IQR] (years)\"] = tbl.apply(\n",
    "        lambda r: f\"{_fmt(r['Median'])} [{_fmt(r['Q1'])}, {_fmt(r['Q3'])}]\", axis=1\n",
    "    )\n",
    "    tbl[\"Min–Max (years)\"] = tbl.apply(\n",
    "        lambda r: f\"{_fmt(r['Min'])}–{_fmt(r['Max'])}\", axis=1\n",
    "    )\n",
    "    keep = [\n",
    "        \"Cohort\", \"Subgroup\", \"N\",\n",
    "        \"Mean±SD (years)\",\n",
    "        \"Median [IQR] (years)\",\n",
    "        \"Min–Max (years)\",\n",
    "    ]\n",
    "\n",
    "    if pd.notna(p_mwu):\n",
    "        p_txt = \"<0.001\" if p_mwu < 0.001 else f\"{p_mwu:.3f}\"\n",
    "    else:\n",
    "        p_txt = \"NA\"\n",
    "\n",
    "    out_rows = tbl[keep].copy()\n",
    "    out_rows.loc[len(out_rows)] = [\n",
    "        cohort_label,\n",
    "        \"Mann–Whitney U (two-sided) p\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        p_txt,\n",
    "    ]\n",
    "\n",
    "    csv_path = os.path.join(outdir, f\"{cohort_label}_SA_DxDuration_IncDec.csv\")\n",
    "    out_rows.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] SA Dx-duration (I2-Dx) table saved: {csv_path}\")\n",
    "\n",
    "    # Figure\n",
    "    plt.rcParams.update(PUB_RC)\n",
    "    fig, ax = plt.subplots(figsize=(6.8, 4.6))\n",
    "    d[\"IncDec\"] = pd.Categorical(\n",
    "        d[\"IncDec\"], categories=[\"Increase\", \"Decrease\"], ordered=True\n",
    "    )\n",
    "    plot_data = [\n",
    "        d.loc[d[\"IncDec\"] == \"Increase\", \"Dx_years_I2_minus_Dx\"],\n",
    "        d.loc[d[\"IncDec\"] == \"Decrease\", \"Dx_years_I2_minus_Dx\"],\n",
    "    ]\n",
    "\n",
    "    bp = ax.boxplot(\n",
    "        plot_data,\n",
    "        positions=[0, 1],\n",
    "        widths=0.55,\n",
    "        vert=True,\n",
    "        showfliers=True,\n",
    "        patch_artist=True,\n",
    "        medianprops=dict(linewidth=1.4),\n",
    "        boxprops=dict(linewidth=1.0),\n",
    "        whiskerprops=dict(linewidth=1.0),\n",
    "        capprops=dict(linewidth=1.0),\n",
    "    )\n",
    "\n",
    "    col_inc = COLOR_PRIMARY\n",
    "    col_dec = \"#9c9c9c\"\n",
    "    bp[\"boxes\"][0].set_facecolor(col_inc)\n",
    "    bp[\"boxes\"][1].set_facecolor(col_dec)\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    for i, yv in enumerate(plot_data):\n",
    "        x = np.full(len(yv), i, dtype=float) + rng.uniform(-0.10, 0.10, size=len(yv))\n",
    "        ax.scatter(\n",
    "            x,\n",
    "            yv,\n",
    "            s=18,\n",
    "            alpha=0.70,\n",
    "            edgecolors=\"white\",\n",
    "            linewidths=0.4,\n",
    "            c=(col_inc if i == 0 else col_dec),\n",
    "            label=(\"Increase\" if i == 0 else \"Decrease\"),\n",
    "            zorder=3,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"Increase\", \"Decrease\"])\n",
    "    ax.set_ylabel(\"Diagnosis duration (years, I2 - Dx)\")\n",
    "    ax.set_title(\n",
    "        f\"{cohort_label} — SA: I2 - Dx duration by subgroup (Increase vs Decrease)\",\n",
    "        pad=12,\n",
    "    )\n",
    "\n",
    "    if pd.notna(p_mwu):\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            0.98,\n",
    "            f\"Mann–Whitney U: p={p_txt}\",\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    ax.grid(axis=\"y\", linestyle=\":\", alpha=0.45)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax.legend(\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=False,\n",
    "    )\n",
    "    plt.subplots_adjust(right=0.76, left=0.12, bottom=0.15, top=0.88)\n",
    "\n",
    "    base = os.path.join(outdir, f\"{cohort_label}_SA_DxDuration_IncDec\")\n",
    "    fig.savefig(base + \".png\", **PNG_KW)\n",
    "    fig.savefig(base + \".pdf\", **PDF_KW)\n",
    "    fig.savefig(base + \".svg\", **SVG_KW)\n",
    "    fig.savefig(base + \".tiff\", **TIFF_KW)\n",
    "    plt.close(fig)\n",
    "\n",
    "for _lab, _fp in COHORTS.items():\n",
    "    _analyze_sa_dx_duration_by_incdec(_lab, _fp, OUTDIR)\n",
    "\n",
    "# ========================= End of longitudinal script =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9e954a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_Primary_Total.csv\n",
      "[RCS] Primary – Total – Percent: F=0.704, p=0.495, df_diff=2\n",
      "[RCS] Primary – Total – Log (Δ log1p): F=0.568, p=0.567, df_diff=2\n",
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_Primary_Periventricular.csv\n",
      "[RCS] Primary – Periventricular – Percent: F=0.623, p=0.536, df_diff=2\n",
      "[RCS] Primary – Periventricular – Log (Δ log1p): F=0.473, p=0.623, df_diff=2\n",
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_Primary_Deep.csv\n",
      "[RCS] Primary – Deep – Percent: F=0.850, p=0.428, df_diff=2\n",
      "[RCS] Primary – Deep – Log (Δ log1p): F=0.745, p=0.475, df_diff=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\639122857.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for seg, sub in dat.groupby(\"Segment\", dropna=True):\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\639122857.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for seg, sub in dat.groupby(\"Segment\", dropna=True):\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\639122857.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for seg, sub in dat.groupby(\"Segment\", dropna=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_Sensitivity_Total.csv\n",
      "[RCS] Sensitivity – Total – Percent: F=0.256, p=0.774, df_diff=2\n",
      "[RCS] Sensitivity – Total – Log (Δ log1p): F=0.256, p=0.774, df_diff=2\n",
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_Sensitivity_Periventricular.csv\n",
      "[RCS] Sensitivity – Periventricular – Percent: F=0.625, p=0.536, df_diff=2\n",
      "[RCS] Sensitivity – Periventricular – Log (Δ log1p): F=0.218, p=0.804, df_diff=2\n",
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_Sensitivity_Deep.csv\n",
      "[RCS] Sensitivity – Deep – Percent: F=0.167, p=0.846, df_diff=2\n",
      "[RCS] Sensitivity – Deep – Log (Δ log1p): F=0.954, p=0.386, df_diff=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\639122857.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for seg, sub in dat.groupby(\"Segment\", dropna=True):\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\639122857.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for seg, sub in dat.groupby(\"Segment\", dropna=True):\n",
      "C:\\Users\\M328449\\AppData\\Local\\Temp\\ipykernel_19788\\639122857.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for seg, sub in dat.groupby(\"Segment\", dropna=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Publishable segment slope table saved: Duration_I2_Individual\\Tables\\SegmentSlopes_5y_Publishable.docx\n",
      "[OK] CSV saved: Duration_I2_Individual\\Tables\\SegmentSlopes_5y_Publishable.csv\n",
      "[OK] Combined per-point CSV saved: Duration_I2_Individual\\Tables\\IndivDiff_AllCohorts_AllRegions.csv\n",
      "[OK] RCS omnibus tests saved: Duration_I2_Individual\\Tables\\RCS_Omnibus_Nonlinearity.csv\n",
      "[OK] Publishable RCS omnibus eTable saved: Duration_I2_Individual\\Tables\\eTable_RCS_Omnibus_Nonlinearity.docx\n",
      "\n",
      "All cohorts processed. Plots saved; publishable tables exported.\n",
      "Counts for duration filter (use Total WMH to avoid triple-counting):\n",
      "- Primary: N_total=852, N_<=30y=835, N_>30y=17\n",
      "- Sensitivity: N_total=877, N_<=30y=860, N_>30y=17\n",
      "Overall N_>30y removed: 34\n",
      "[OK] Saved: Duration_I2_Individual\\Tables\\Counts_Duration_gt30_excluded.txt\n"
     ]
    }
   ],
   "source": [
    "# Individual scatter + LOESS analysis: SA diagnosis duration vs WMH differences\n",
    "# -----------------------------------------------------------------------------\n",
    "# Plots: unchanged (publication-grade, PNG/PDF/SVG/TIFF)\n",
    "# Tables:\n",
    "#   1) Publishable Word three-line table of piecewise slopes (every 5 years)\n",
    "#   2) Publishable Word three-line eTable: RCS (restricted cubic splines)\n",
    "#      omnibus nonlinearity tests across cohorts/regions, on Percent & Log scales\n",
    "#      (FDR only for PWMH & DWMH, per main analysis)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os, re, warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# statsmodels\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# RCS splines\n",
    "from patsy import cr\n",
    "\n",
    "# Word export\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "COHORTS = {\n",
    "    \"Primary\":     \"primary_cohort.csv\",\n",
    "    \"Sensitivity\": \"sensitivity_cohort.csv\",\n",
    "}\n",
    "OUTDIR = \"Duration_I2_Individual\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "SUBDIR = {\n",
    "    \"plots_log\": os.path.join(OUTDIR, \"Plots_log\"),\n",
    "    \"plots_log_sens\": os.path.join(OUTDIR, \"Plots_log_Sensitivity\"),\n",
    "    \"plots_pct\": os.path.join(OUTDIR, \"Plots_pct\"),\n",
    "    \"plots_pct_sens\": os.path.join(OUTDIR, \"Plots_pct_Sensitivity\"),\n",
    "    \"plots_pct_sens_panel\": os.path.join(OUTDIR, \"Plots_pct_Sens_Panel\"),\n",
    "    \"tables\": os.path.join(OUTDIR, \"Tables\"),\n",
    "}\n",
    "for _p in SUBDIR.values():\n",
    "    os.makedirs(_p, exist_ok=True)\n",
    "\n",
    "# Collector for all per-point rows across cohorts/regions\n",
    "POINT_ROWS_ALL = []\n",
    "\n",
    "# Column names (Instance 2 date, first SA diagnosis date)\n",
    "COL_I2_DATE = \"Date_of_attending_assessment_centre_Instance_2\"\n",
    "COL_DX_DATE = \"Date_G47_first_reported_sleep_disorders\"\n",
    "\n",
    "# WMH columns and scaling (Instance 2)\n",
    "WMH_I2_COLS = {\n",
    "    \"Total\":          \"Total_volume_of_white_matter_hyperintensities_from_T1_and_T2_FLAIR_images_Instance_2\",\n",
    "    \"Periventricular\":\"Total_volume_of_peri_ventricular_white_matter_hyperintensities_Instance_2\",\n",
    "    \"Deep\":           \"Total_volume_of_deep_white_matter_hyperintensities_Instance_2\",\n",
    "}\n",
    "SCALE_I2 = \"Volumetric_scaling_from_T1_head_image_to_standard_space_Instance_2\"\n",
    "\n",
    "# Display labels and plotting order\n",
    "DISPLAY_LABEL = {\"Total\":\"Total WMH\",\"Periventricular\":\"PWMH\",\"Deep\":\"DWMH\"}\n",
    "PLOT_ORDER = [\"Total\",\"Periventricular\",\"Deep\"]\n",
    "FDR_REGIONS = {\"PWMH\", \"DWMH\"}  # only these two get FDR q-values\n",
    "\n",
    "# Plot style\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"Arial\",\"figure.dpi\": 120,\"savefig.dpi\": 600,\n",
    "    \"font.size\": 12,\"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 11,\"ytick.labelsize\": 11,\n",
    "    \"figure.autolayout\": True\n",
    "})\n",
    "EXPORT_KW = {\n",
    "    \"png\": {\"dpi\":600,\"bbox_inches\":\"tight\",\"facecolor\":\"white\"},\n",
    "    \"pdf\": {\"bbox_inches\":\"tight\"},\n",
    "    \"svg\": {\"bbox_inches\":\"tight\"},\n",
    "    \"tiff\": {\"dpi\":600,\"bbox_inches\":\"tight\"},\n",
    "}\n",
    "\n",
    "# ---------------- UTILITIES ----------------\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns=lambda x: re.sub(r\"[^0-9a-zA-Z]+\",\"_\",str(x)))\n",
    "\n",
    "def ensure_datetime(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def ensure_numeric(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    return re.sub(r\"[^0-9A-Za-z._-]+\",\"_\",str(s)).strip(\"_\")\n",
    "\n",
    "def prepare_dataframe(fp: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute diagnosis duration and log1p head-size–normalized WMH at Instance 2.\"\"\"\n",
    "    df = clean_cols(pd.read_csv(fp, low_memory=False))\n",
    "\n",
    "    d_i2 = ensure_datetime(df.get(COL_I2_DATE))\n",
    "    d_dx = ensure_datetime(df.get(COL_DX_DATE))\n",
    "    df[\"Diagnosis_duration_years\"] = (d_i2 - d_dx).dt.days / 365.25\n",
    "    df.loc[df[\"Diagnosis_duration_years\"] < 0, \"Diagnosis_duration_years\"] = np.nan\n",
    "    df.loc[df[\"Diagnosis_duration_years\"] > 60, \"Diagnosis_duration_years\"] = np.nan\n",
    "\n",
    "    scale = ensure_numeric(df.get(SCALE_I2)).fillna(1.0)\n",
    "    for reg, col in WMH_I2_COLS.items():\n",
    "        v = ensure_numeric(df.get(col)) * scale\n",
    "        df[f\"Log_HSN_{reg}_I2\"] = np.log1p(v)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------- INDIVIDUAL DIFFERENCES ----------------\n",
    "def compute_individual_diff(df: pd.DataFrame, region: str, label: str) -> pd.DataFrame:\n",
    "    \"\"\"Compute SA–Control differences within matched sets (Δ log1p and %).\"\"\"\n",
    "    y_col = f\"Log_HSN_{region}_I2\"\n",
    "\n",
    "    if \"group\" not in df.columns:\n",
    "        if \"treatment_var\" in df.columns:\n",
    "            df[\"group\"] = df[\"treatment_var\"].map({0:\"Control\", 1:\"Study\"})\n",
    "        elif \"SA\" in df.columns:\n",
    "            df[\"group\"] = df[\"SA\"].map({0:\"Control\", 1:\"Study\"})\n",
    "        else:\n",
    "            raise ValueError(f\"{label}: 'group'/'treatment_var'/'SA' column not found.\")\n",
    "\n",
    "    sa   = df[df[\"group\"] == \"Study\"].copy()\n",
    "    ctrl = df[df[\"group\"] == \"Control\"].copy()\n",
    "\n",
    "    ctrl_mean = ctrl.groupby(\"match_id\")[y_col].mean().rename(\"ctrl_mean\")\n",
    "    sa = sa.merge(ctrl_mean, on=\"match_id\", how=\"left\")\n",
    "\n",
    "    sa[\"Diff_log\"] = sa[y_col] - sa[\"ctrl_mean\"]        # Δ log1p\n",
    "    sa[\"Diff_pct\"] = np.expm1(sa[\"Diff_log\"]) * 100.0   # % difference\n",
    "\n",
    "    return sa[[\"match_id\",\"Diagnosis_duration_years\",\"Diff_log\",\"Diff_pct\"]]\n",
    "\n",
    "# ---------------- PLOTTING (unchanged) ----------------\n",
    "def plot_scatter_loess_log(dat: pd.DataFrame, region: str, label: str):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_log\",\n",
    "                    alpha=0.5, s=28, color=\"#1f3b4d\")\n",
    "    sns.regplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_log\",\n",
    "                lowess=True, scatter=False, color=\"red\", line_kws={\"lw\":1.8})\n",
    "    plt.axhline(0, color=\"grey\", ls=\"--\")\n",
    "    plt.xlabel(\"Diagnosis duration (years)\")\n",
    "    plt.ylabel(\"Δ log1p WMH\")\n",
    "    plt.title(f\"{label} – {DISPLAY_LABEL.get(region,region)} (log1p)\")\n",
    "    base = os.path.join(SUBDIR[\"plots_log\"], f\"IndivDiffLog_{safe_name(label)}_{region}\")\n",
    "    for ext, kw in EXPORT_KW.items():\n",
    "        plt.savefig(base+f\".{ext}\", **kw)\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter_loess_pct(dat: pd.DataFrame, region: str, label: str):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                    alpha=0.5, s=28, color=\"#1f3b4d\")\n",
    "    sns.regplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                lowess=True, scatter=False, color=\"red\", line_kws={\"lw\":1.8})\n",
    "    plt.axhline(0, color=\"grey\", ls=\"--\")\n",
    "    plt.xlabel(\"Diagnosis duration (years)\")\n",
    "    plt.ylabel(\"Δ WMH vs Controls (%)\")\n",
    "    plt.title(f\"{label} – {DISPLAY_LABEL.get(region,region)} (%)\")\n",
    "    base = os.path.join(SUBDIR[\"plots_pct\"], f\"IndivDiffPct_{safe_name(label)}_{region}\")\n",
    "    for ext, kw in EXPORT_KW.items():\n",
    "        plt.savefig(base+f\".{ext}\", **kw)\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter_loess_log_sensitivity(dat: pd.DataFrame, region: str, label: str,\n",
    "                                       max_years: int = 30, right_limit: float = 0.4):\n",
    "    dat = dat[dat[\"Diagnosis_duration_years\"] <= max_years].copy()\n",
    "    if dat.empty: return\n",
    "    fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "    sns.scatterplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_log\",\n",
    "                    alpha=0.4, s=28, color=\"#1f3b4d\", ax=ax1)\n",
    "    ax1.axhline(0, color=\"grey\", ls=\"--\")\n",
    "    ax1.set_xlabel(\"Diagnosis duration (years)\")\n",
    "    ax1.set_ylabel(\"Δ log1p WMH – raw (all points)\", color=\"#1f3b4d\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"#1f3b4d\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(-right_limit, right_limit)\n",
    "    ax2.set_ylabel(f\"Δ log1p WMH – LOESS (±{right_limit})\", color=\"red\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "    smoothed = lowess(dat[\"Diff_log\"], dat[\"Diagnosis_duration_years\"], frac=0.3, return_sorted=True)\n",
    "    ax2.plot(smoothed[:,0], smoothed[:,1], color=\"red\", lw=1.8)\n",
    "\n",
    "    plt.title(f\"{label} – {DISPLAY_LABEL.get(region,region)} (log1p, ≤{max_years}y)\")\n",
    "    base = os.path.join(SUBDIR[\"plots_log_sens\"],\n",
    "                        f\"IndivDiffLogDualRight_{safe_name(label)}_{region}_le{max_years}y_pm{right_limit}\")\n",
    "    for ext, kw in EXPORT_KW.items():\n",
    "        plt.savefig(base+f\".{ext}\", **kw)\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter_loess_pct_sensitivity(dat: pd.DataFrame, region: str, label: str,\n",
    "                                       max_years: int = 30, ylim: tuple = (-100, 100)):\n",
    "    dat = dat[dat[\"Diagnosis_duration_years\"] <= max_years].copy()\n",
    "    if dat.empty: return\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                    alpha=0.5, s=28, color=\"#1f3b4d\")\n",
    "    sns.regplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                lowess=True, scatter=False, color=\"red\", line_kws={\"lw\":1.8})\n",
    "    plt.axhline(0, color=\"grey\", ls=\"--\")\n",
    "    plt.xlabel(\"Diagnosis duration (years)\")\n",
    "    plt.ylabel(\"Δ WMH vs Controls (%)\")\n",
    "    plt.title(f\"{label} – {DISPLAY_LABEL.get(region,region)} (%, ≤{max_years}y)\")\n",
    "    plt.ylim(*ylim)\n",
    "    base = os.path.join(SUBDIR[\"plots_pct_sens\"],\n",
    "                        f\"IndivDiffPct_{safe_name(label)}_{region}_le{max_years}y\")\n",
    "    for ext, kw in EXPORT_KW.items():\n",
    "        plt.savefig(base+f\".{ext}\", **kw)\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter_loess_pct_dualaxis(dat: pd.DataFrame, region: str, label: str,\n",
    "                                    max_years: int = 30, right_ylim: tuple = (0, 40)):\n",
    "    dat = dat[dat[\"Diagnosis_duration_years\"] <= max_years].copy()\n",
    "    if dat.empty: return\n",
    "\n",
    "    left_min = float(np.nanmin(dat[\"Diff_pct\"]))\n",
    "    left_max = float(np.nanmax(dat[\"Diff_pct\"]))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "    sns.scatterplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                    alpha=0.4, s=28, color=\"#1f3b4d\", ax=ax1)\n",
    "    ax1.axhline(0, color=\"grey\", ls=\"--\")\n",
    "    ax1.set_xlabel(\"Diagnosis duration (years)\")\n",
    "    ax1.set_ylabel(\"Δ WMH vs Controls (%) – raw (all points)\", color=\"#1f3b4d\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"#1f3b4d\")\n",
    "    ax1.set_ylim(left_min, left_max)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.regplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                lowess=True, scatter=False, color=\"red\", line_kws={\"lw\":1.8}, ax=ax2)\n",
    "\n",
    "    left_span = left_max - left_min\n",
    "    rel0 = (0.0 - left_min) / left_span if left_span > 0 else 0.5\n",
    "    right_span = right_ylim[1] - right_ylim[0]\n",
    "    right_min_aligned = right_ylim[0] - rel0 * right_span\n",
    "    right_max_aligned = right_min_aligned + right_span\n",
    "    ax2.set_ylim(right_min_aligned, right_max_aligned)\n",
    "\n",
    "    ax2.set_ylabel(\"Δ WMH vs Controls (%) – LOESS (0–50%)\", color=\"red\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "    plt.title(f\"{label} – {DISPLAY_LABEL.get(region,region)} (%, ≤{max_years}y)\")\n",
    "    base = os.path.join(SUBDIR[\"plots_pct_sens\"],\n",
    "                        f\"IndivDiffPctDualFixed_{safe_name(label)}_{region}_le{max_years}y\")\n",
    "    for ext, kw in EXPORT_KW.items():\n",
    "        plt.savefig(base+f\".{ext}\", **kw)\n",
    "    plt.close()\n",
    "\n",
    "def plot_pct_threepanel_dualaxis(results_by_region: dict, label: str,\n",
    "                                 max_years: int = 30, right_ylim: tuple = (0, 40)):\n",
    "    outdir = os.path.join(SUBDIR[\"plots_pct_sens_panel\"], safe_name(label))\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 3.5), sharex=True)\n",
    "    panel_name = {\"Total\": \"Total WMH\", \"Periventricular\": \"PWMH\", \"Deep\": \"DWMH\"}\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "    for i, region in enumerate(PLOT_ORDER):\n",
    "        dat = results_by_region.get(region)\n",
    "        if dat is None or dat.empty: continue\n",
    "        dat = dat[dat[\"Diagnosis_duration_years\"] <= max_years].copy()\n",
    "        if dat.empty: continue\n",
    "\n",
    "        ax1 = axes[i]\n",
    "        data_min = float(np.nanmin(dat[\"Diff_pct\"]))\n",
    "        left_min = min(0.0, data_min)\n",
    "        left_max = 2000.0 if region in (\"Total\", \"Periventricular\") else 7000.0\n",
    "\n",
    "        sns.scatterplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                        alpha=0.3, s=20, color=\"#4a6c8c\", ax=ax1, zorder=1)\n",
    "        ax1.axhline(0, color=\"grey\", ls=\"--\", lw=1, zorder=2)\n",
    "        ax1.set_xlabel(\"Diagnosis duration (years)\")\n",
    "        ax1.set_ylabel(\"% change (SA – Control)\" if i == 0 else \"\", color=\"black\")\n",
    "        ax1.tick_params(axis=\"y\", labelcolor=\"black\")\n",
    "        ax1.set_ylim(left_min, left_max)\n",
    "        ax1.yaxis.set_major_locator(MaxNLocator(nbins=8, prune='upper' if region==\"Deep\" else None))\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        sns.regplot(data=dat, x=\"Diagnosis_duration_years\", y=\"Diff_pct\",\n",
    "                    lowess=True, scatter=False, color=\"#e76f51\",\n",
    "                    line_kws={\"lw\": 2.0, \"zorder\": 3}, ax=ax2)\n",
    "\n",
    "        left_span = left_max - left_min\n",
    "        rel0 = (0.0 - left_min) / left_span if left_span > 0 else 0.5\n",
    "        right_span = right_ylim[1] - right_ylim[0]\n",
    "        right_min_aligned = right_ylim[0] - rel0 * right_span\n",
    "        right_max_aligned = right_min_aligned + right_span\n",
    "        ax2.set_ylim(right_min_aligned, right_max_aligned)\n",
    "        if i == 2:\n",
    "            ax2.set_ylabel(\"LOESS %\", color=\"#e76f51\")\n",
    "        else:\n",
    "            ax2.set_ylabel(\"\")\n",
    "\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=\"#e76f51\")\n",
    "        panel_tag = chr(65 + i)\n",
    "        ax1.text(-0.10, 1.11, panel_tag, transform=ax1.transAxes,\n",
    "                 fontsize=18, fontweight=\"bold\", va=\"top\", ha=\"right\")\n",
    "        ax1.text(0.50, 1.095, panel_name.get(region, region),\n",
    "                 transform=ax1.transAxes, ha=\"center\", va=\"top\",\n",
    "                 fontsize=14, fontweight=\"normal\", color=\"black\", clip_on=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    base = os.path.join(outdir, f\"Figure_Percent_LT{max_years}\")\n",
    "    for ext, kw in EXPORT_KW.items():\n",
    "        plt.savefig(base + f\".{ext}\", **kw)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------- SEGMENT SLOPES: every 5 years ----------------\n",
    "def make_5y_bins(max_years: int = 30):\n",
    "    \"\"\"Return bins and labels for [0,5], (5,10], ..., (25,30] years.\"\"\"\n",
    "    edges = list(range(0, max_years + 5, 5))  # 0,5,10,...,30\n",
    "    labels = [f\"{edges[i]}–{edges[i+1]}y\" for i in range(len(edges)-1)]\n",
    "    return edges, labels\n",
    "\n",
    "def compute_segment_slopes_5y(dat: pd.DataFrame, region: str, label: str,\n",
    "                              max_years: int = 30) -> pd.DataFrame | None:\n",
    "    \"\"\"Piecewise linear slopes for Δ% within 5-year segments up to max_years (HC3 SE).\"\"\"\n",
    "    dat = dat[dat[\"Diagnosis_duration_years\"] <= max_years].copy()\n",
    "    if dat.empty:\n",
    "        return None\n",
    "\n",
    "    edges, labels = make_5y_bins(max_years)\n",
    "    dat[\"Segment\"] = pd.cut(\n",
    "        dat[\"Diagnosis_duration_years\"], bins=edges, labels=labels, include_lowest=True, right=True\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for seg, sub in dat.groupby(\"Segment\", dropna=True):\n",
    "        if len(sub) < 10:  # small-segment guard\n",
    "            continue\n",
    "        m = smf.ols(\"Diff_pct ~ Diagnosis_duration_years\", data=sub).fit(cov_type=\"HC3\")\n",
    "        slope = m.params[\"Diagnosis_duration_years\"]\n",
    "        ci_low, ci_high = m.conf_int().loc[\"Diagnosis_duration_years\"]\n",
    "        pval = m.pvalues[\"Diagnosis_duration_years\"]\n",
    "        rows.append({\n",
    "            \"Analytic cohort\": label,\n",
    "            \"WMH region\": DISPLAY_LABEL.get(region, region),\n",
    "            \"Duration segment (years)\": seg,\n",
    "            \"N\": int(len(sub)),\n",
    "            \"β per year\": float(slope),\n",
    "            \"CI low\": float(ci_low),\n",
    "            \"CI high\": float(ci_high),\n",
    "            \"p\": float(pval),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # NOTE: FDR will be applied later across PWMH & DWMH only (per main analysis)\n",
    "    return df\n",
    "\n",
    "# ---------------- WORD EXPORT: helpers ----------------\n",
    "def _set_doc_normal_tnr10(doc: Document):\n",
    "    \"\"\"Force Normal style to Times New Roman 10pt for full document.\"\"\"\n",
    "    style = doc.styles[\"Normal\"]\n",
    "    style.font.name = \"Times New Roman\"\n",
    "    style._element.rPr.rFonts.set(qn(\"w:eastAsia\"), \"Times New Roman\")\n",
    "    style.font.size = Pt(10)\n",
    "\n",
    "def _apply_table_tnr10(table):\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            for p in cell.paragraphs:\n",
    "                for run in p.runs:\n",
    "                    run.font.name = \"Times New Roman\"\n",
    "                    run.font.size = Pt(10)\n",
    "\n",
    "# ---------------- WORD EXPORT: publishable tables ----------------\n",
    "PRETTY_COLS = [\n",
    "    \"Analytic cohort\", \"WMH region\", \"Duration segment (years)\", \"N\",\n",
    "    \"β per year\", \"CI low\", \"CI high\", \"p\", \"q (FDR)\"\n",
    "]\n",
    "\n",
    "def save_publishable_segment_table(df: pd.DataFrame, outpath: str,\n",
    "                                   title: str, legend_text: str):\n",
    "    \"\"\"Save a publishable Word three-line table (Times New Roman 10pt) with Title+Legend.\"\"\"\n",
    "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
    "    df = df[PRETTY_COLS].copy()\n",
    "\n",
    "    # Sort: cohort, region (Total→PWMH→DWMH), then segment in chronological order\n",
    "    def _seg_start(s):\n",
    "        try:\n",
    "            return int(str(s).split(\"–\")[0])\n",
    "        except Exception:\n",
    "            return 9999\n",
    "    region_order = {\"Total WMH\": 0, \"PWMH\": 1, \"DWMH\": 2}\n",
    "    df[\"_seg_order\"] = df[\"Duration segment (years)\"].map(_seg_start)\n",
    "    df[\"_reg_order\"] = df[\"WMH region\"].map(lambda x: region_order.get(str(x), 9))\n",
    "    df = (\n",
    "        df.sort_values([\"Analytic cohort\", \"_reg_order\", \"_seg_order\"])\n",
    "          .drop(columns=[\"_seg_order\", \"_reg_order\"])\n",
    "    )\n",
    "\n",
    "    # Format numbers\n",
    "    df_fmt = df.copy()\n",
    "    for c in [\"β per year\",\"CI low\",\"CI high\"]:\n",
    "        df_fmt[c] = pd.to_numeric(df_fmt[c], errors=\"coerce\").map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"\")\n",
    "    for c in [\"p\",\"q (FDR)\"]:\n",
    "        df_fmt[c] = pd.to_numeric(df_fmt[c], errors=\"coerce\").map(lambda x: f\"{x:.3g}\" if pd.notna(x) else \"\")\n",
    "\n",
    "    doc = Document()\n",
    "    _set_doc_normal_tnr10(doc)\n",
    "\n",
    "    # Title\n",
    "    p = doc.add_paragraph()\n",
    "    run = p.add_run(title)\n",
    "    run.bold = True\n",
    "    run.font.name = \"Times New Roman\"\n",
    "    run.font.size = Pt(10)\n",
    "\n",
    "    # Table\n",
    "    t = doc.add_table(rows=1, cols=len(df_fmt.columns))\n",
    "    hdr = t.rows[0].cells\n",
    "    for j, c in enumerate(df_fmt.columns):\n",
    "        hdr[j].text = str(c)\n",
    "\n",
    "    for _, row in df_fmt.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        for j, c in enumerate(df_fmt.columns):\n",
    "            cells[j].text = \"\" if pd.isna(row[c]) else str(row[c])\n",
    "\n",
    "    # Three-line borders\n",
    "    tbl = t._tbl\n",
    "    for i, tr in enumerate(tbl.tr_lst):\n",
    "        for tc in tr.tc_lst:\n",
    "            tcPr = tc.get_or_add_tcPr()\n",
    "            tcBorders = OxmlElement('w:tcBorders')\n",
    "            if i == 0:\n",
    "                top = OxmlElement(\"w:top\"); top.set(qn(\"w:val\"), \"single\"); top.set(qn(\"w:sz\"), \"16\")\n",
    "                bot = OxmlElement(\"w:bottom\"); bot.set(qn(\"w:val\"), \"single\"); bot.set(qn(\"w:sz\"), \"16\")\n",
    "                tcBorders.append(top); tcBorders.append(bot)\n",
    "            elif i == len(tbl.tr_lst) - 1:\n",
    "                bot = OxmlElement(\"w:bottom\"); bot.set(qn(\"w:val\"), \"single\"); bot.set(qn(\"w:sz\"), \"16\")\n",
    "                tcBorders.append(bot)\n",
    "            tcPr.append(tcBorders)\n",
    "\n",
    "    # Font unify\n",
    "    _apply_table_tnr10(t)\n",
    "\n",
    "    # Legend\n",
    "    p_leg = doc.add_paragraph()\n",
    "    r_leg = p_leg.add_run(legend_text)\n",
    "    r_leg.italic = True\n",
    "    r_leg.font.name = \"Times New Roman\"\n",
    "    r_leg.font.size = Pt(10)\n",
    "\n",
    "    doc.save(outpath)\n",
    "    print(f\"[OK] Publishable segment slope table saved: {outpath}\")\n",
    "\n",
    "# ===== RCS omnibus nonlinearity test & eTable (supplement) =====================\n",
    "RCS_ROWS = []  # collect per cohort × region × scale\n",
    "\n",
    "def rcs_global_test(dat: pd.DataFrame, region: str, label: str,\n",
    "                    x_col: str = \"Diagnosis_duration_years\",\n",
    "                    y_col: str = \"Diff_pct\",\n",
    "                    df_spline: int = 4,\n",
    "                    max_years: int = 30):\n",
    "    \"\"\"\n",
    "    Omnibus nonlinearity test: linear vs restricted cubic spline (RCS).\n",
    "    Full model:   y ~ cr(x, df=df_spline)\n",
    "    Restricted:   y ~ x\n",
    "    Test:         nested F-test via results.compare_f_test()\n",
    "    \"\"\"\n",
    "    d = dat[[x_col, y_col]].dropna().copy()\n",
    "    d = d[d[x_col] <= max_years]\n",
    "    if len(d) < (df_spline + 2):\n",
    "        return None  # insufficient data\n",
    "\n",
    "    # Restricted linear model\n",
    "    m_lin = smf.ols(f\"{y_col} ~ {x_col}\", data=d).fit()\n",
    "\n",
    "    # Full RCS model (natural cubic spline)\n",
    "    m_rcs = smf.ols(f\"{y_col} ~ cr({x_col}, df={df_spline})\", data=d).fit()\n",
    "\n",
    "    F, p, df_diff = m_rcs.compare_f_test(m_lin)  # omnibus nonlinearity\n",
    "    row = {\n",
    "        \"Analytic cohort\": label,\n",
    "        \"WMH region\": DISPLAY_LABEL.get(region, region),\n",
    "        \"Scale\": \"Percent\" if y_col == \"Diff_pct\" else \"Log (Δ log1p)\",\n",
    "        \"N\": int(len(d)),\n",
    "        \"df_spline\": int(df_spline),\n",
    "        \"F (nonlinearity)\": float(F) if F is not None else np.nan,\n",
    "        \"p (nonlinearity)\": float(p) if p is not None else np.nan,\n",
    "        \"df_diff\": int(df_diff) if df_diff is not None else np.nan,\n",
    "        # context: overall model fit\n",
    "        \"F (full model)\": float(m_rcs.fvalue) if m_rcs.fvalue is not None else np.nan,\n",
    "        \"p (full model)\": float(m_rcs.f_pvalue) if m_rcs.f_pvalue is not None else np.nan,\n",
    "    }\n",
    "    RCS_ROWS.append(row)\n",
    "    print(f\"[RCS] {label} – {region} – {row['Scale']}: \"\n",
    "          f\"F={row['F (nonlinearity)']:.3f}, p={row['p (nonlinearity)']:.3g}, \"\n",
    "          f\"df_diff={row['df_diff']}\")\n",
    "    return row\n",
    "\n",
    "RCS_PRETTY_COLS = [\n",
    "    \"Analytic cohort\", \"WMH region\", \"Scale\", \"N\", \"df_spline\",\n",
    "    \"F (nonlinearity)\", \"p (nonlinearity)\", \"q (FDR, nonlinearity)\",\n",
    "    \"df_diff\", \"F (full model)\", \"p (full model)\"\n",
    "]\n",
    "\n",
    "def save_publishable_rcs_table(df: pd.DataFrame, outpath: str,\n",
    "                               title: str, legend_text: str):\n",
    "    \"\"\"Save a publishable Word three-line eTable for RCS omnibus tests.\"\"\"\n",
    "    os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
    "\n",
    "    # Order by cohort, region (Total→PWMH→DWMH), then Scale (Log→Percent)\n",
    "    region_order = {\"Total WMH\":0, \"PWMH\":1, \"DWMH\":2}\n",
    "    scale_order  = {\"Log (Δ log1p)\":0, \"Percent\":1}\n",
    "    df[\"_rord\"] = df[\"WMH region\"].map(lambda x: region_order.get(x, 9))\n",
    "    df[\"_sord\"] = df[\"Scale\"].map(lambda x: scale_order.get(x, 9))\n",
    "    df = df.sort_values([\"Analytic cohort\", \"_rord\", \"_sord\"]).drop(columns=[\"_rord\",\"_sord\"])\n",
    "\n",
    "    # Format numbers\n",
    "    df_fmt = df.copy()\n",
    "    for c in [\"F (nonlinearity)\", \"F (full model)\"]:\n",
    "        df_fmt[c] = pd.to_numeric(df_fmt[c], errors=\"coerce\").map(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"\")\n",
    "    for c in [\"p (nonlinearity)\", \"q (FDR, nonlinearity)\", \"p (full model)\"]:\n",
    "        df_fmt[c] = pd.to_numeric(df_fmt[c], errors=\"coerce\").map(lambda x: f\"{x:.3g}\" if pd.notna(x) else \"\")\n",
    "\n",
    "    df_fmt = df_fmt[RCS_PRETTY_COLS]\n",
    "\n",
    "    doc = Document()\n",
    "    _set_doc_normal_tnr10(doc)\n",
    "\n",
    "    p = doc.add_paragraph()\n",
    "    run = p.add_run(title)\n",
    "    run.bold = True\n",
    "    run.font.name = \"Times New Roman\"\n",
    "    run.font.size = Pt(10)\n",
    "\n",
    "    t = doc.add_table(rows=1, cols=len(df_fmt.columns))\n",
    "    hdr = t.rows[0].cells\n",
    "    for j, c in enumerate(df_fmt.columns):\n",
    "        hdr[j].text = str(c)\n",
    "\n",
    "    for _, row in df_fmt.iterrows():\n",
    "        cells = t.add_row().cells\n",
    "        for j, c in enumerate(df_fmt.columns):\n",
    "            cells[j].text = \"\" if pd.isna(row[c]) else str(row[c])\n",
    "\n",
    "    # Three-line borders\n",
    "    tbl = t._tbl\n",
    "    for i, tr in enumerate(tbl.tr_lst):\n",
    "        for tc in tr.tc_lst:\n",
    "            tcPr = tc.get_or_add_tcPr()\n",
    "            tcBorders = OxmlElement('w:tcBorders')\n",
    "            if i == 0:\n",
    "                top = OxmlElement(\"w:top\"); top.set(qn(\"w:val\"), \"single\"); top.set(qn(\"w:sz\"), \"16\")\n",
    "                bot = OxmlElement(\"w:bottom\"); bot.set(qn(\"w:val\"), \"single\"); bot.set(qn(\"w:sz\"), \"16\")\n",
    "                tcBorders.append(top); tcBorders.append(bot)\n",
    "            elif i == len(tbl.tr_lst) - 1:\n",
    "                bot = OxmlElement(\"w:bottom\"); bot.set(qn(\"w:val\"), \"single\"); bot.set(qn(\"w:sz\"), \"16\")\n",
    "                tcBorders.append(bot)\n",
    "            tcPr.append(tcBorders)\n",
    "\n",
    "    _apply_table_tnr10(t)\n",
    "\n",
    "    p_leg = doc.add_paragraph()\n",
    "    r_leg = p_leg.add_run(legend_text)\n",
    "    r_leg.italic = True\n",
    "    r_leg.font.name = \"Times New Roman\"\n",
    "    r_leg.font.size = Pt(10)\n",
    "\n",
    "    doc.save(outpath)\n",
    "    print(f\"[OK] Publishable RCS omnibus eTable saved: {outpath}\")\n",
    "# ============================================================================\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "def run_one_cohort(label: str, fp: str) -> dict:\n",
    "    \"\"\"Compute diffs (for plots), make all plots; return per-region data.\"\"\"\n",
    "    df = prepare_dataframe(fp)\n",
    "    if \"group\" not in df.columns:\n",
    "        if \"treatment_var\" in df.columns:\n",
    "            df[\"group\"] = df[\"treatment_var\"].map({0:\"Control\", 1:\"Study\"})\n",
    "        elif \"SA\" in df.columns:\n",
    "            df[\"group\"] = df[\"SA\"].map({0:\"Control\", 1:\"Study\"})\n",
    "        else:\n",
    "            raise ValueError(f\"{label}: no 'group'/'treatment_var'/'SA' column.\")\n",
    "\n",
    "    results_by_region = {}\n",
    "    for reg in PLOT_ORDER:\n",
    "        dat = compute_individual_diff(df, reg, label)\n",
    "        if dat is None or dat.empty:\n",
    "            continue\n",
    "        results_by_region[reg] = dat\n",
    "\n",
    "        # --- Save per-cohort×region per-point CSV ---\n",
    "        per_csv = os.path.join(SUBDIR[\"tables\"], f\"IndivDiff_{safe_name(label)}_{reg}.csv\")\n",
    "        dat.to_csv(per_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK] CSV saved: {per_csv}\")\n",
    "\n",
    "        # --- Append rows to the global collector for the combined CSV ---\n",
    "        dat2 = dat.copy()\n",
    "        dat2.insert(0, \"WMH region\", DISPLAY_LABEL.get(reg, reg))\n",
    "        dat2.insert(0, \"Analytic cohort\", label)\n",
    "        POINT_ROWS_ALL.append(dat2)\n",
    "\n",
    "        # Plots (unchanged)\n",
    "        plot_scatter_loess_log(dat, reg, label)\n",
    "        plot_scatter_loess_pct(dat, reg, label)\n",
    "        plot_scatter_loess_log_sensitivity(dat, reg, label, max_years=30)\n",
    "        plot_scatter_loess_pct_sensitivity(dat, reg, label, max_years=30)\n",
    "        plot_scatter_loess_pct_dualaxis(dat, reg, label, max_years=30)\n",
    "\n",
    "        # --- RCS omnibus tests on Percent and Log scales ---\n",
    "        rcs_global_test(dat, reg, label, y_col=\"Diff_pct\", df_spline=4, max_years=30)\n",
    "        rcs_global_test(dat, reg, label, y_col=\"Diff_log\", df_spline=4, max_years=30)\n",
    "\n",
    "    # Three-panel figure using accumulated regions\n",
    "    if results_by_region:\n",
    "        plot_pct_threepanel_dualaxis(results_by_region, label, max_years=30)\n",
    "\n",
    "    return results_by_region\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Collect all 5-year segment slopes across cohorts/regions\n",
    "    seg_results_all = []\n",
    "\n",
    "    for lab, fp in COHORTS.items():\n",
    "        if not os.path.exists(fp):\n",
    "            print(f\"[Skip] {lab}: file not found → {fp}\")\n",
    "            continue\n",
    "\n",
    "        res = run_one_cohort(lab, fp)\n",
    "        for reg, dat in res.items():\n",
    "            seg_tbl = compute_segment_slopes_5y(dat, reg, lab, max_years=30)\n",
    "            if seg_tbl is not None and not seg_tbl.empty:\n",
    "                seg_results_all.append(seg_tbl)\n",
    "\n",
    "    # Merge and export the single publishable 5y-segment table\n",
    "    if seg_results_all:\n",
    "        df_all = pd.concat(seg_results_all, ignore_index=True)\n",
    "\n",
    "        # === FDR only for PWMH & DWMH (not for Total WMH) ===\n",
    "        df_all[\"q (FDR)\"] = np.nan\n",
    "        m = df_all[\"WMH region\"].isin(FDR_REGIONS) & df_all[\"p\"].notna()\n",
    "        if m.any():\n",
    "            df_all.loc[m, \"q (FDR)\"] = multipletests(df_all.loc[m, \"p\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "        title = \"eTable Y. Piecewise 5-year slopes of SA–control differences in white matter hyperintensities by diagnosis duration\"\n",
    "        legend = (\n",
    "            \"Legend. Slopes (β per year) are estimated within 5-year segments using OLS with HC3 standard errors. \"\n",
    "            \"Outcome is Δ WMH vs controls (%) derived from expm1 of log-differences. \"\n",
    "            \"FDR is applied to PWMH and DWMH only, consistent with the main analysis. \"\n",
    "            \"Segments with N<10 are omitted.\"\n",
    "        )\n",
    "\n",
    "        docx_path = os.path.join(SUBDIR[\"tables\"], \"SegmentSlopes_5y_Publishable.docx\")\n",
    "        save_publishable_segment_table(df_all, docx_path, title, legend)\n",
    "\n",
    "        # Also export CSV for reproducibility\n",
    "        csv_path = os.path.join(SUBDIR[\"tables\"], \"SegmentSlopes_5y_Publishable.csv\")\n",
    "        df_all.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK] CSV saved: {csv_path}\")\n",
    "\n",
    "    # === Export combined per-point CSV ===\n",
    "    if POINT_ROWS_ALL:\n",
    "        df_points_all = pd.concat(POINT_ROWS_ALL, ignore_index=True)\n",
    "        ordered_cols = [\n",
    "            \"Analytic cohort\", \"WMH region\",\n",
    "            \"match_id\", \"Diagnosis_duration_years\",\n",
    "            \"Diff_log\", \"Diff_pct\"\n",
    "        ]\n",
    "        df_points_all = df_points_all[ordered_cols]\n",
    "        out_points_csv = os.path.join(SUBDIR[\"tables\"], \"IndivDiff_AllCohorts_AllRegions.csv\")\n",
    "        df_points_all.to_csv(out_points_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK] Combined per-point CSV saved: {out_points_csv}\")\n",
    "\n",
    "    # === RCS omnibus eTable (Supplement; FDR only for PWMH & DWMH) ============\n",
    "    if RCS_ROWS:\n",
    "        df_rcs = pd.DataFrame(RCS_ROWS)\n",
    "\n",
    "        # FDR for nonlinearity p-values — PWMH & DWMH only\n",
    "        df_rcs[\"q (FDR, nonlinearity)\"] = np.nan\n",
    "        m = df_rcs[\"WMH region\"].isin(FDR_REGIONS) & df_rcs[\"p (nonlinearity)\"].notna()\n",
    "        if m.any():\n",
    "            df_rcs.loc[m, \"q (FDR, nonlinearity)\"] = multipletests(\n",
    "                df_rcs.loc[m, \"p (nonlinearity)\"], method=\"fdr_bh\"\n",
    "            )[1]\n",
    "\n",
    "        # Save CSV\n",
    "        out_rcs_csv = os.path.join(SUBDIR[\"tables\"], \"RCS_Omnibus_Nonlinearity.csv\")\n",
    "        df_rcs.to_csv(out_rcs_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK] RCS omnibus tests saved: {out_rcs_csv}\")\n",
    "\n",
    "        # Save Word eTable\n",
    "        rcs_title = \"eTable X. Omnibus restricted cubic spline tests for nonlinearity of SA–control WMH differences by diagnosis duration\"\n",
    "        rcs_legend = (\n",
    "            \"Legend. Omnibus tests compare a restricted cubic spline model (df=4) with a linear model for diagnosis duration \"\n",
    "            \"using a nested F-test. Outcome is Δ WMH vs controls (%) and Δ log1p WMH (log scale). \"\n",
    "            \"FDR is applied to PWMH and DWMH only, consistent with the main analysis. \"\n",
    "            \"N denotes observations within ≤30 years.\"\n",
    "        )\n",
    "        rcs_docx = os.path.join(SUBDIR[\"tables\"], \"eTable_RCS_Omnibus_Nonlinearity.docx\")\n",
    "        save_publishable_rcs_table(df_rcs, rcs_docx, rcs_title, rcs_legend)\n",
    "\n",
    "    print(\"\\nAll cohorts processed. Plots saved; publishable tables exported.\")\n",
    "\n",
    "    # === How many had duration >30y (count once using Total WMH) ===\n",
    "try:\n",
    "    df_pts = pd.read_csv(os.path.join(SUBDIR[\"tables\"], \"IndivDiff_AllCohorts_AllRegions.csv\"))\n",
    "    d = df_pts[df_pts[\"WMH region\"] == \"Total WMH\"]\n",
    "\n",
    "    by_cohort = (\n",
    "        d.groupby(\"Analytic cohort\")[\"Diagnosis_duration_years\"]\n",
    "         .agg(N_total=lambda s: int(s.notna().sum()),\n",
    "              N_le30=lambda s: int((s <= 30).sum()),\n",
    "              N_gt30=lambda s: int((s > 30).sum()))\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "    overall_gt30 = int((d[\"Diagnosis_duration_years\"] > 30).sum())\n",
    "\n",
    "    txt = [\"Counts for duration filter (use Total WMH to avoid triple-counting):\"]\n",
    "    for _, r in by_cohort.iterrows():\n",
    "        txt.append(f\"- {r['Analytic cohort']}: N_total={r['N_total']}, N_<=30y={r['N_le30']}, N_>30y={r['N_gt30']}\")\n",
    "    txt.append(f\"Overall N_>30y removed: {overall_gt30}\")\n",
    "\n",
    "    out_txt = os.path.join(SUBDIR[\"tables\"], \"Counts_Duration_gt30_excluded.txt\")\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(txt))\n",
    "    print(\"\\n\".join(txt))\n",
    "    print(f\"[OK] Saved: {out_txt}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not compute >30y counts:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "00aaa712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "pandas: 2.2.3\n",
      "numpy: 2.2.4\n",
      "scipy: 1.15.2\n",
      "statsmodels: 0.14.5\n",
      "matplotlib: 3.10.1\n",
      "seaborn: 0.13.2\n",
      "sklearn: 1.7.1\n",
      "lifelines: 0.30.0\n",
      "gseapy: 1.1.9\n",
      "networkx: 3.5\n",
      "graphviz: 0.21\n",
      "openpyxl: 3.1.5\n",
      "docx: 1.2.0\n",
      "tqdm: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "#Print Versions of all packages used in the analysis for reproducibility purposes\n",
    "import importlib\n",
    "\n",
    "packages = [\n",
    "    \"sys\", \"pandas\", \"numpy\", \"scipy\", \"statsmodels\",\n",
    "    \"matplotlib\", \"seaborn\", \"sklearn\",\n",
    "    \"lifelines\", \"gseapy\", \"networkx\", \"graphviz\",\n",
    "    \"openpyxl\", \"docx\", \"tqdm\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        if pkg == \"sys\":\n",
    "            import sys\n",
    "            print(f\"Python: {sys.version}\")\n",
    "        else:\n",
    "            module = importlib.import_module(pkg)\n",
    "            print(f\"{pkg}: {module.__version__}\")\n",
    "    except ImportError:\n",
    "        print(f\"{pkg}: not installed\")\n",
    "    except AttributeError:\n",
    "        print(f\"{pkg}: version attribute not found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
